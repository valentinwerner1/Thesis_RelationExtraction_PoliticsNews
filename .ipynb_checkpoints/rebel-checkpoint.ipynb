{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\svawe\\anaconda3\\lib\\site-packages\\torchvision\\io\\image.py:13: UserWarning: Failed to load image Python extension: [WinError 127] Die angegebene Prozedur wurde nicht gefunden\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Union, List, Optional\n",
    "import omegaconf\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "import datasets\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read = pd.read_json(\"data_src/raw/out1.json\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>article_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>_input_hash</th>\n",
       "      <th>_task_hash</th>\n",
       "      <th>_is_binary</th>\n",
       "      <th>spans</th>\n",
       "      <th>tokens</th>\n",
       "      <th>_view_id</th>\n",
       "      <th>relations</th>\n",
       "      <th>answer</th>\n",
       "      <th>_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Still, the restrictions imposed on the project...</td>\n",
       "      <td>183</td>\n",
       "      <td>18315</td>\n",
       "      <td>-1595512685</td>\n",
       "      <td>351917106</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'start': 35, 'end': 46, 'token_start': 6, 't...</td>\n",
       "      <td>[{'text': 'Still', 'start': 0, 'end': 5, 'id':...</td>\n",
       "      <td>relations</td>\n",
       "      <td>[{'head': 18, 'child': 14, 'head_span': {'star...</td>\n",
       "      <td>accept</td>\n",
       "      <td>1666082270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Days later, a released fighter and others, mos...</td>\n",
       "      <td>13</td>\n",
       "      <td>1317</td>\n",
       "      <td>399796307</td>\n",
       "      <td>1377411962</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'start': 12, 'end': 30, 'token_start': 3, 't...</td>\n",
       "      <td>[{'text': 'Days', 'start': 0, 'end': 4, 'id': ...</td>\n",
       "      <td>relations</td>\n",
       "      <td>[{'head': 20, 'child': 5, 'head_span': {'start...</td>\n",
       "      <td>accept</td>\n",
       "      <td>1666082881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Are countries allowed to turn away asylum seek...</td>\n",
       "      <td>11</td>\n",
       "      <td>1111</td>\n",
       "      <td>-1470052778</td>\n",
       "      <td>-160075520</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'start': 4, 'end': 13, 'token_start': 1, 'to...</td>\n",
       "      <td>[{'text': 'Are', 'start': 0, 'end': 3, 'id': 0...</td>\n",
       "      <td>relations</td>\n",
       "      <td>[{'head': 1, 'child': 7, 'head_span': {'start'...</td>\n",
       "      <td>accept</td>\n",
       "      <td>1666082907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One man, 28, spoke to The Washington Post on t...</td>\n",
       "      <td>167</td>\n",
       "      <td>1673</td>\n",
       "      <td>-61112669</td>\n",
       "      <td>-1028177687</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'start': 0, 'end': 11, 'token_start': 0, 'to...</td>\n",
       "      <td>[{'text': 'One', 'start': 0, 'end': 3, 'id': 0...</td>\n",
       "      <td>relations</td>\n",
       "      <td>[{'head': 3, 'child': 9, 'head_span': {'start'...</td>\n",
       "      <td>accept</td>\n",
       "      <td>1666082929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A U.S. defense official said Monday that two N...</td>\n",
       "      <td>174</td>\n",
       "      <td>1749</td>\n",
       "      <td>-2118794475</td>\n",
       "      <td>-1006559555</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'start': 0, 'end': 23, 'token_start': 0, 'to...</td>\n",
       "      <td>[{'text': 'A', 'start': 0, 'end': 1, 'id': 0, ...</td>\n",
       "      <td>relations</td>\n",
       "      <td>[]</td>\n",
       "      <td>accept</td>\n",
       "      <td>1666082966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  article_id  sentence_id  \\\n",
       "0  Still, the restrictions imposed on the project...         183        18315   \n",
       "1  Days later, a released fighter and others, mos...          13         1317   \n",
       "2  Are countries allowed to turn away asylum seek...          11         1111   \n",
       "3  One man, 28, spoke to The Washington Post on t...         167         1673   \n",
       "4  A U.S. defense official said Monday that two N...         174         1749   \n",
       "\n",
       "   _input_hash  _task_hash  _is_binary  \\\n",
       "0  -1595512685   351917106       False   \n",
       "1    399796307  1377411962       False   \n",
       "2  -1470052778  -160075520       False   \n",
       "3    -61112669 -1028177687       False   \n",
       "4  -2118794475 -1006559555       False   \n",
       "\n",
       "                                               spans  \\\n",
       "0  [{'start': 35, 'end': 46, 'token_start': 6, 't...   \n",
       "1  [{'start': 12, 'end': 30, 'token_start': 3, 't...   \n",
       "2  [{'start': 4, 'end': 13, 'token_start': 1, 'to...   \n",
       "3  [{'start': 0, 'end': 11, 'token_start': 0, 'to...   \n",
       "4  [{'start': 0, 'end': 23, 'token_start': 0, 'to...   \n",
       "\n",
       "                                              tokens   _view_id  \\\n",
       "0  [{'text': 'Still', 'start': 0, 'end': 5, 'id':...  relations   \n",
       "1  [{'text': 'Days', 'start': 0, 'end': 4, 'id': ...  relations   \n",
       "2  [{'text': 'Are', 'start': 0, 'end': 3, 'id': 0...  relations   \n",
       "3  [{'text': 'One', 'start': 0, 'end': 3, 'id': 0...  relations   \n",
       "4  [{'text': 'A', 'start': 0, 'end': 1, 'id': 0, ...  relations   \n",
       "\n",
       "                                           relations  answer  _timestamp  \n",
       "0  [{'head': 18, 'child': 14, 'head_span': {'star...  accept  1666082270  \n",
       "1  [{'head': 20, 'child': 5, 'head_span': {'start...  accept  1666082881  \n",
       "2  [{'head': 1, 'child': 7, 'head_span': {'start'...  accept  1666082907  \n",
       "3  [{'head': 3, 'child': 9, 'head_span': {'start'...  accept  1666082929  \n",
       "4                                                 []  accept  1666082966  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read[read.answer == \"accept\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix sentence_id mistake\n",
    "df = df.astype({\"sentence_id\":str,\"article_id\":str}) \n",
    "\n",
    "def fix_id(row):\n",
    "    return row.sentence_id[:len(row.article_id)] + \"_\" + row.sentence_id[len(row.article_id):]\n",
    "\n",
    "df[\"sentence_id\"] = df.apply(fix_id, axis = 1)\n",
    "\n",
    "#add annotator column in case more annotators will be used\n",
    "df[\"annotator\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for EDA\n",
    "df[\"relations_count\"] = df.relations.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>article_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>_input_hash</th>\n",
       "      <th>_task_hash</th>\n",
       "      <th>_is_binary</th>\n",
       "      <th>spans</th>\n",
       "      <th>tokens</th>\n",
       "      <th>_view_id</th>\n",
       "      <th>relations</th>\n",
       "      <th>answer</th>\n",
       "      <th>_timestamp</th>\n",
       "      <th>annotator</th>\n",
       "      <th>relations_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Still, the restrictions imposed on the project...</td>\n",
       "      <td>183</td>\n",
       "      <td>183_15</td>\n",
       "      <td>-1595512685</td>\n",
       "      <td>351917106</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'start': 35, 'end': 46, 'token_start': 6, 't...</td>\n",
       "      <td>[{'text': 'Still', 'start': 0, 'end': 5, 'id':...</td>\n",
       "      <td>relations</td>\n",
       "      <td>[{'head': 18, 'child': 14, 'head_span': {'star...</td>\n",
       "      <td>accept</td>\n",
       "      <td>1666082270</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Days later, a released fighter and others, mos...</td>\n",
       "      <td>13</td>\n",
       "      <td>13_17</td>\n",
       "      <td>399796307</td>\n",
       "      <td>1377411962</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'start': 12, 'end': 30, 'token_start': 3, 't...</td>\n",
       "      <td>[{'text': 'Days', 'start': 0, 'end': 4, 'id': ...</td>\n",
       "      <td>relations</td>\n",
       "      <td>[{'head': 20, 'child': 5, 'head_span': {'start...</td>\n",
       "      <td>accept</td>\n",
       "      <td>1666082881</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Are countries allowed to turn away asylum seek...</td>\n",
       "      <td>11</td>\n",
       "      <td>11_11</td>\n",
       "      <td>-1470052778</td>\n",
       "      <td>-160075520</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'start': 4, 'end': 13, 'token_start': 1, 'to...</td>\n",
       "      <td>[{'text': 'Are', 'start': 0, 'end': 3, 'id': 0...</td>\n",
       "      <td>relations</td>\n",
       "      <td>[{'head': 1, 'child': 7, 'head_span': {'start'...</td>\n",
       "      <td>accept</td>\n",
       "      <td>1666082907</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One man, 28, spoke to The Washington Post on t...</td>\n",
       "      <td>167</td>\n",
       "      <td>167_3</td>\n",
       "      <td>-61112669</td>\n",
       "      <td>-1028177687</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'start': 0, 'end': 11, 'token_start': 0, 'to...</td>\n",
       "      <td>[{'text': 'One', 'start': 0, 'end': 3, 'id': 0...</td>\n",
       "      <td>relations</td>\n",
       "      <td>[{'head': 3, 'child': 9, 'head_span': {'start'...</td>\n",
       "      <td>accept</td>\n",
       "      <td>1666082929</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A U.S. defense official said Monday that two N...</td>\n",
       "      <td>174</td>\n",
       "      <td>174_9</td>\n",
       "      <td>-2118794475</td>\n",
       "      <td>-1006559555</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'start': 0, 'end': 23, 'token_start': 0, 'to...</td>\n",
       "      <td>[{'text': 'A', 'start': 0, 'end': 1, 'id': 0, ...</td>\n",
       "      <td>relations</td>\n",
       "      <td>[]</td>\n",
       "      <td>accept</td>\n",
       "      <td>1666082966</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text article_id sentence_id  \\\n",
       "0  Still, the restrictions imposed on the project...        183      183_15   \n",
       "1  Days later, a released fighter and others, mos...         13       13_17   \n",
       "2  Are countries allowed to turn away asylum seek...         11       11_11   \n",
       "3  One man, 28, spoke to The Washington Post on t...        167       167_3   \n",
       "4  A U.S. defense official said Monday that two N...        174       174_9   \n",
       "\n",
       "   _input_hash  _task_hash  _is_binary  \\\n",
       "0  -1595512685   351917106       False   \n",
       "1    399796307  1377411962       False   \n",
       "2  -1470052778  -160075520       False   \n",
       "3    -61112669 -1028177687       False   \n",
       "4  -2118794475 -1006559555       False   \n",
       "\n",
       "                                               spans  \\\n",
       "0  [{'start': 35, 'end': 46, 'token_start': 6, 't...   \n",
       "1  [{'start': 12, 'end': 30, 'token_start': 3, 't...   \n",
       "2  [{'start': 4, 'end': 13, 'token_start': 1, 'to...   \n",
       "3  [{'start': 0, 'end': 11, 'token_start': 0, 'to...   \n",
       "4  [{'start': 0, 'end': 23, 'token_start': 0, 'to...   \n",
       "\n",
       "                                              tokens   _view_id  \\\n",
       "0  [{'text': 'Still', 'start': 0, 'end': 5, 'id':...  relations   \n",
       "1  [{'text': 'Days', 'start': 0, 'end': 4, 'id': ...  relations   \n",
       "2  [{'text': 'Are', 'start': 0, 'end': 3, 'id': 0...  relations   \n",
       "3  [{'text': 'One', 'start': 0, 'end': 3, 'id': 0...  relations   \n",
       "4  [{'text': 'A', 'start': 0, 'end': 1, 'id': 0, ...  relations   \n",
       "\n",
       "                                           relations  answer  _timestamp  \\\n",
       "0  [{'head': 18, 'child': 14, 'head_span': {'star...  accept  1666082270   \n",
       "1  [{'head': 20, 'child': 5, 'head_span': {'start...  accept  1666082881   \n",
       "2  [{'head': 1, 'child': 7, 'head_span': {'start'...  accept  1666082907   \n",
       "3  [{'head': 3, 'child': 9, 'head_span': {'start'...  accept  1666082929   \n",
       "4                                                 []  accept  1666082966   \n",
       "\n",
       "   annotator  relations_count  \n",
       "0          0                1  \n",
       "1          0                2  \n",
       "2          0                1  \n",
       "3          0                1  \n",
       "4          0                0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prune df\n",
    "df = df.drop(columns = [\"_input_hash\",\"_task_hash\",\"_is_binary\",\"tokens\",\"_view_id\",\"answer\",\"_timestamp\"])\n",
    "df = df.reset_index().drop(columns = [\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### check if it performs better with empty sentences or without\n",
    "#df = df[df.relations_count > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fight',\n",
       " 'ExpressIntentToCooperate',\n",
       " 'ReduceRelations',\n",
       " 'Coerce',\n",
       " 'EngageInDiplomaticCooperation',\n",
       " 'MakePublicStatement',\n",
       " 'ExhibitMilitaryPosture',\n",
       " 'Assault',\n",
       " 'Reject',\n",
       " 'Disapprove',\n",
       " 'ProvideAid',\n",
       " 'Investigate',\n",
       " 'Appeal',\n",
       " 'Demand',\n",
       " 'Yield',\n",
       " 'Consult',\n",
       " 'Protest',\n",
       " 'EngageInMaterialCooperation']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get data into needed format and create new DataFrame\n",
    "relations = []\n",
    "for index,row in df.iterrows():\n",
    "    for rel in row.relations:\n",
    "        relations.append(rel['label'])\n",
    "\n",
    "relations = list(set(relations))\n",
    "relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data into needed format and create new DataFrame\n",
    "data = []\n",
    "for index,row in df.iterrows():\n",
    "    rel_list = []\n",
    "    for rel in row.relations:\n",
    "        subj = row.text[rel[\"head_span\"][\"start\"]:rel[\"head_span\"][\"end\"]]\n",
    "        obj = row.text[rel[\"child_span\"][\"start\"]:rel[\"child_span\"][\"end\"]]\n",
    "        rel_list.append(f\"<triplet> {subj} <subj> {obj} <obj> {rel['label']}\")\n",
    "    data.append({\"doc_id\":row.sentence_id, \"text\": row.text, \"triplets\": \" \".join(rel_list)})\n",
    "\n",
    "data = pd.DataFrame(data, columns = [\"doc_id\",\"text\",\"triplets\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'triplets'],\n",
       "        num_rows: 131\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'triplets'],\n",
       "        num_rows: 23\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'triplets'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = datasets.Dataset.from_pandas(data.drop(columns=[\"doc_id\"]))\n",
    "\n",
    "#### just random splits for testing, change later to proper splitting\n",
    "###### and then change later again for CV\n",
    "split = ds.train_test_split(test_size = 0.2)\n",
    "split_val = split[\"test\"].train_test_split(test_size = 0.3)\n",
    "ds = datasets.DatasetDict({\"train\":split[\"train\"],\"val\":split_val[\"train\"], \"test\":split_val[\"test\"]})\n",
    "\n",
    "#ds = ds.with_format(\"torch\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conf:\n",
    "    #general\n",
    "    seed = 0\n",
    "    gpus = 0\n",
    "    \n",
    "    #input\n",
    "    batch_size = 32\n",
    "    max_length = 128\n",
    "    ignore_pad_token_for_loss = True\n",
    "    use_fast_tokenizer = True\n",
    "    gradient_acc_steps = 1\n",
    "    gradient_clip_value = 10.0\n",
    "\n",
    "    #optimizer\n",
    "    lr = 0.00005\n",
    "    weight_decay = 0.01\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 0.00000001\n",
    "    warmup_steps = 1000\n",
    "\n",
    "    #training\n",
    "    max_steps = 1200000\n",
    "    samples_interval = 1000\n",
    "\n",
    "    monitor_var  = \"val_loss\"\n",
    "    monitor_var_mode = \"min\"\n",
    "    val_check_interval = 0.5\n",
    "    val_percent_check = 0.1\n",
    "\n",
    "    model_name = \"model1.pth\"\n",
    "    checkpoint_path = f\"models/{model_name}\"\n",
    "    save_top_k = 1\n",
    "\n",
    "    early_stopping = False\n",
    "    patience = \"5\"\n",
    "\n",
    "    length_penalty = 0\n",
    "    no_repeat_ngram_size = 0\n",
    "    num_beans = 3\n",
    "    precision = 16\n",
    "    amp_level = None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetData(pl.LightningDataModule):\n",
    "    def __init__(self, conf: conf, tokenizer: AutoTokenizer, model: AutoModelForSeq2SeqLM):\n",
    "        \"\"\"init params from config\"\"\"\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.datasets = ds\n",
    "        self.max_length = conf.max_length\n",
    "        self.batch_size = conf.batch_size\n",
    "        self.ignore_pad_token_for_loss = conf.ignore_pad_token_for_loss\n",
    "        #self.padding = \n",
    "        \n",
    "        # Data collator\n",
    "        label_pad_token_id = -100                       \n",
    "        self.data_collator = DataCollatorForSeq2Seq(self.tokenizer, self.model, label_pad_token_id=label_pad_token_id)\n",
    "\n",
    "    def preprocess_function(self, data):\n",
    "        \"\"\"tokenize, pad, truncate\"\"\"\n",
    "        #split into input and labels\n",
    "        inputs = data[\"text\"]       \n",
    "        outputs = data[\"triplets\"]\n",
    "\n",
    "        #process input\n",
    "        model_inputs = self.tokenizer(inputs, max_length = self.max_length, padding = True, truncation = True)\n",
    "\n",
    "        #process labels\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            labels = self.tokenizer(outputs, max_length = self.max_length, padding = True, truncation = True)\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "        \n",
    "    #apply the preprocessing and load data\n",
    "    def train_dataloader(self, *args, **kwargs): \n",
    "        self.train_dataset = self.datasets[\"train\"]\n",
    "        self.train_dataset = self.train_dataset.map(self.preprocess_function, batched = True)\n",
    "        return DataLoader(self.train_dataset, batch_size = self.batch_size, collate_fn = self.data_collator, shuffle = True)\n",
    "\n",
    "    def val_dataloader(self, *args, **kwargs): \n",
    "        self.eval_dataset = self.datasets[\"val\"]\n",
    "        self.eval_dataset = self.eval_dataset.map(self.preprocess_function, batched = True)\n",
    "        return DataLoader(self.eval_dataset, batch_size = self.batch_size, collate_fn = self.data_collator)\n",
    "\n",
    "    def test_dataloader(self, *args, **kwargs): \n",
    "        self.test_dataset = self.datasets[\"test\"]\n",
    "        self.test_dataset = self.test_dataset.map(self.preprocess_function, batched = True)\n",
    "        return DataLoader(self.test_dataset, batch_size = self.batch_size, collate_fn = self.data_collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #from REBEL\n",
    "# def label_smoothed_nll_loss(lprobs, target, epsilon, ignore_index=-100):\n",
    "#     \"\"\"From fairseq\"\"\"\n",
    "#     if target.dim() == lprobs.dim() - 1:\n",
    "#         target = target.unsqueeze(-1)\n",
    "#     nll_loss = -lprobs.gather(dim=-1, index=target)\n",
    "#     smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
    "#     if ignore_index is not None:\n",
    "#         pad_mask = target.eq(ignore_index)\n",
    "#         nll_loss.masked_fill_(pad_mask, 0.0)\n",
    "#         smooth_loss.masked_fill_(pad_mask, 0.0)\n",
    "#     else:\n",
    "#         nll_loss = nll_loss.squeeze(-1)\n",
    "#         smooth_loss = smooth_loss.squeeze(-1)\n",
    "\n",
    "#     nll_loss = nll_loss.sum()  # mean()? Scared to break other math.\n",
    "#     smooth_loss = smooth_loss.sum()\n",
    "#     eps_i = epsilon / lprobs.size(-1)\n",
    "#     loss = (1.0 - epsilon) * nll_loss + eps_i * smooth_loss\n",
    "#     return loss, nll_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from REBEL\n",
    "from typing import Sequence\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning import Callback, LightningModule, Trainer\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import wandb\n",
    "\n",
    "class GenerateTextSamplesCallback(Callback):\n",
    "    \"\"\"\n",
    "    PL Callback to generate triplets along training\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, logging_batch_interval):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logging_batch_interval: How frequently to inspect/potentially plot something\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.logging_batch_interval = logging_batch_interval\n",
    "\n",
    "    def on_train_batch_end(self,trainer: Trainer,pl_module: LightningModule, outputs: Sequence, batch: Sequence, batch_idx: int, dataloader_idx: int,) -> None:\n",
    "        wandb_table = wandb.Table(columns=[\"Source\", \"Pred\", \"Gold\"])\n",
    "        # pl_module.logger.info(\"Executing translation callback\")\n",
    "        if (trainer.batch_idx + 1) % self.logging_batch_interval != 0:  # type: ignore[attr-defined]\n",
    "            return\n",
    "        labels = batch.pop(\"labels\")\n",
    "        gen_kwargs = {\n",
    "            \"max_length\": conf.max_length,\n",
    "            \"early_stopping\": False,\n",
    "            \"no_repeat_ngram_size\": 0,\n",
    "            \"num_beams\": conf.num_beans\n",
    "        }\n",
    "        pl_module.eval()\n",
    "\n",
    "        decoder_inputs = torch.roll(labels, 1, 1)[:,0:2]\n",
    "        decoder_inputs[:, 0] = 0\n",
    "        generated_tokens = pl_module.model.generate(\n",
    "            batch[\"input_ids\"].to(pl_module.model.device),\n",
    "            attention_mask=batch[\"attention_mask\"].to(pl_module.model.device),\n",
    "            decoder_input_ids=decoder_inputs.to(pl_module.model.device),\n",
    "            **gen_kwargs,\n",
    "        )\n",
    "        # in case the batch is shorter than max length, the output should be padded\n",
    "        if generated_tokens.shape[-1] < gen_kwargs[\"max_length\"]:\n",
    "            generated_tokens = pl_module._pad_tensors_to_max_len(generated_tokens, gen_kwargs[\"max_length\"])\n",
    "        pl_module.train()\n",
    "        decoded_preds = pl_module.tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
    "\n",
    "        if pl_module.hparams.ignore_pad_token_for_loss:\n",
    "            # Replace -100 in the labels as we can't decode them.\n",
    "            labels = torch.where(labels != -100, labels, pl_module.tokenizer.pad_token_id)\n",
    "\n",
    "        decoded_labels = pl_module.tokenizer.batch_decode(labels, skip_special_tokens=False)\n",
    "        decoded_inputs = pl_module.tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=False)\n",
    "\n",
    "        # pl_module.logger.experiment.log_text('generated samples', '\\n'.join(decoded_preds).replace('<pad>', ''))\n",
    "        # pl_module.logger.experiment.log_text('original samples', '\\n'.join(decoded_labels).replace('<pad>', ''))\n",
    "        for source, translation, gold_output in zip(decoded_inputs, decoded_preds, decoded_labels):\n",
    "            wandb_table.add_data(\n",
    "                source.replace('<pad>', ''), translation.replace('<pad>', ''), gold_output.replace('<pad>', '')\n",
    "            )\n",
    "        pl_module.logger.experiment.log({\"Triplets\": wandb_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from REBEL\n",
    "def shift_tokens_left(input_ids: torch.Tensor, pad_token_id: int):\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right.\n",
    "    \"\"\"\n",
    "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "    shifted_input_ids[:, :-1] = input_ids[:, 1:].clone()\n",
    "    shifted_input_ids[:, -1] = pad_token_id\n",
    "\n",
    "    assert pad_token_id is not None, \"self.model.config.pad_token_id has to be defined.\"\n",
    "\n",
    "    return shifted_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from REBEL\n",
    "\n",
    "##### can maybe be rewritten more effective\n",
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, conf, config: AutoConfig, tokenizer: AutoTokenizer, model: AutoModelForSeq2SeqLM):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        #self.loss_fn = label_smoothed_nll_loss\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "    def forward(self, inputs, labels):\n",
    "        ##### Check later if smooth labeled loss is better \n",
    "        outputs = self.model(**inputs, labels = labels, use_cache = False, return_dict = True, output_hidden_states = True)\n",
    "        output_dict = {'loss': outputs['loss'], 'logits': outputs['logits']}\n",
    "        return output_dict\n",
    "\n",
    "    def training_step(self, batch: dict, batch_idx: int):\n",
    "        ##### check later if labels = batch[\"labels\"] also works\n",
    "        labels = batch.pop(\"labels\")\n",
    "        labels_original = labels.clone()\n",
    "        batch[\"decoder_input_ids\"] = torch.where(labels != -100, labels, self.config.pad_token_id)\n",
    "        labels = shift_tokens_left(labels, -100)\n",
    "\n",
    "        forward_output = self.forward(batch, labels)\n",
    "        self.log('loss', forward_output['loss'])\n",
    "\n",
    "        batch[\"labels\"] = labels_original\n",
    "\n",
    "        #### ig i dont have this\n",
    "        if 'loss_aux' in forward_output:\n",
    "            self.log('loss_classifier', forward_output['loss_aux'])\n",
    "            return forward_output['loss'] + forward_output['loss_aux']\n",
    "\n",
    "        return forward_output['loss']\n",
    "\n",
    "    def validation_step(self, batch: dict, batch_idx):\n",
    "        #### pop maybe not needed?\n",
    "        labels = batch.pop(\"labels\")\n",
    "        batch[\"decoder_input_ids\"] = torch.where(labels != -100, labels, self.config.pad_token_id)\n",
    "        labels = shift_tokens_left(labels, -100)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # compute loss on predict data\n",
    "            forward_output = self.forward(batch, labels)\n",
    "\n",
    "        forward_output['loss'] = forward_output['loss'].mean().detach()\n",
    "\n",
    "        #### probably not needed ? why would i want only pred loss\n",
    "        # if self.hparams.prediction_loss_only:\n",
    "        #     self.log('val_loss', forward_output['loss'])\n",
    "        #     return\n",
    "\n",
    "        forward_output['logits'] = forward_output['logits'].detach()\n",
    "\n",
    "        if labels.shape[-1] < conf.max_length:\n",
    "            padded_tensor = self.config.pad_token_id * torch.ones((labels.shape[0], conf.max_length), dtype=tensor.dtype, device=tensor.device)\n",
    "            padded_tensor[:, : labels.shape[-1]] = labels\n",
    "            forward_output[\"labels\"] = padded_tensor\n",
    "        else:\n",
    "            forward_output['labels'] = labels\n",
    "\n",
    "        metrics = {}\n",
    "        metrics['val_loss'] = forward_output['loss']\n",
    "\n",
    "        #### only 1? so why loop lmao\n",
    "        for key in sorted(metrics.keys()):\n",
    "            self.log(key, metrics[key])\n",
    "\n",
    "        outputs = {}\n",
    "        outputs['predictions'], outputs['labels'] = self.generate_triples(batch, labels)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        #### popping again\n",
    "        labels = batch.pop(\"labels\")\n",
    "        batch[\"decoder_input_ids\"] = torch.where(labels != -100, labels, self.config.pad_token_id)\n",
    "        labels = shift_tokens_left(labels, -100)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # compute loss on predict data\n",
    "            forward_output = self.forward(batch, labels)\n",
    "\n",
    "        forward_output['loss'] = forward_output['loss'].mean().detach()\n",
    "\n",
    "        ##### probably not needed? would would i only want pred loss\n",
    "        # if self.hparams.prediction_loss_only:\n",
    "        #     self.log('test_loss', forward_output['loss'])\n",
    "        #     return\n",
    "\n",
    "        forward_output['logits'] = generated_tokens.detach() if self.hparams.predict_with_generate else forward_output['logits'].detach()\n",
    "\n",
    "        if labels.shape[-1] < gen_kwargs[\"max_length\"]:\n",
    "            padded_tensor = self.config.pad_token_id * torch.ones((labels.shape[0], conf.max_length), dtype=tensor.dtype, device=tensor.device)\n",
    "            padded_tensor[:, : labels.shape[-1]] = labels\n",
    "            forward_output[\"labels\"] = padded_tensor\n",
    "        else:\n",
    "            forward_output['labels'] = labels\n",
    "\n",
    "\n",
    "        metrics = {}\n",
    "        metrics['test_loss'] = forward_output['loss']\n",
    "\n",
    "        #### dont i only have one metric anyways?\n",
    "        for key in sorted(metrics.keys()):\n",
    "            self.log(key, metrics[key], prog_bar=True)\n",
    "\n",
    "        #### what does this actually do? how does this change everything\n",
    "        # if self.hparams.finetune:\n",
    "        #     return {'predictions': self.forward_samples(batch, labels)}\n",
    "        # else:\n",
    "\n",
    "        outputs = {}\n",
    "        outputs['predictions'], outputs['labels'] = self.generate_triples(batch, labels)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "    def validation_epoch_end():\n",
    "        \n",
    "        relations = [\"MakePublicStatement\",\"Appeal\",\"ExpressIntentToCooperate\",\"Consult\",\"EngageInDiplomaticCooperation\",\"EngageInMaterialCooperation\",\"ProvideAid\",\"Yield\",\"Investigate\",\"Demand\",\"Disapprove\",\"Reject\",\"Threaten\",\"ExhibitMilitaryPosture\",\"Protest\",\"ReduceRelations\",\"Coerce\",\"Assault\",\"Fight\",\"EngageInUnconventialMassViolence\"]\n",
    "        scores, precision, recall, f1 = re_score([item for pred in output for item in pred['predictions']], [item for pred in output for item in pred['labels']], relations)\n",
    "        self.log('val_prec_micro', precision)\n",
    "        self.log('val_recall_micro', recall)\n",
    "        self.log('val_F1_micro', f1)\n",
    "\n",
    "    def test_epoch_end():\n",
    "\n",
    "        relations = [\"MakePublicStatement\",\"Appeal\",\"ExpressIntentToCooperate\",\"Consult\",\"EngageInDiplomaticCooperation\",\"EngageInMaterialCooperation\",\"ProvideAid\",\"Yield\",\"Investigate\",\"Demand\",\"Disapprove\",\"Reject\",\"Threaten\",\"ExhibitMilitaryPosture\",\"Protest\",\"ReduceRelations\",\"Coerce\",\"Assault\",\"Fight\",\"EngageInUnconventialMassViolence\"]\n",
    "        scores, precision, recall, f1 = re_score([item for pred in output for item in pred['predictions']], [item for pred in output for item in pred['labels']], relations)\n",
    "        self.log('test_prec_micro', precision)\n",
    "        self.log('test_recall_micro', recall)\n",
    "        self.log('test_F1_micro', f1)\n",
    "\n",
    "\n",
    "    # additional functions called in main functions\n",
    "\n",
    "    def generate_triples(self, batch, labels) -> None:\n",
    "\n",
    "        generated_tokens = self.model.generate(\n",
    "            batch[\"input_ids\"].to(self.model.device),\n",
    "            attention_mask=batch[\"attention_mask\"].to(self.model.device),\n",
    "            use_cache = True, max_length = conf.max_length, early_stopping = conf.early_stopping, length_penalty = conf.length_penalty, \n",
    "            no_repeat_ngram_size = conf.no_repeat_ngram_size, num_beans = conf.num_beams)\n",
    "\n",
    "        decoded_preds = self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
    "        decoded_labels = self.tokenizer.batch_decode(torch.where(labels != -100, labels, self.config.pad_token_id), skip_special_tokens=False)\n",
    "\n",
    "        return [extract_triplets(rel) for rel in decoded_preds], [extract_triplets(rel) for rel in decoded_labels]\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        ##### HUH\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.hparams.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr = conf.lr, betas = (conf.beta1, conf.beta2), eps = conf.epsilon, weight_decay = conf.weight_decay)\n",
    "        #### also check warmup later\n",
    "        factor = lambda epoch: 0.95\n",
    "        lr_scheduler = lr_scheduler.MultiplicativeLR(optimizer, factor)\n",
    "        \n",
    "        #scheduler = inverse_square_root(optimizer, num_warmup_steps= conf.warmup_steps)\n",
    "\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    #def compute_metrics():\n",
    "        #looks not needed    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\", use_fast = conf.use_fast_tokenizer,\n",
    "        additional_special_tokens = [\"<obj>\", \"<subj>\", \"<triplet>\", \"<head>\", \"</head>\", \"<tail>\", \"</tail>\"])\n",
    "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\")\n",
    "config = transformers.AutoConfig.from_pretrained(\"Babelscape/rebel-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(conf):\n",
    "    pl.seed_everything(conf.seed)\n",
    "\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\", use_fast = conf.use_fast_tokenizer,\n",
    "        additional_special_tokens = [\"<obj>\", \"<subj>\", \"<triplet>\", \"<head>\", \"</head>\", \"<tail>\", \"</tail>\"])\n",
    "    config = transformers.AutoConfig.from_pretrained(\"Babelscape/rebel-large\")\n",
    "    model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\", config = config)\n",
    "\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    pl_data_module = GetData(conf, tokenizer, model)\n",
    "    pl_module = BaseModule(conf, config, tokenizer, model)\n",
    "\n",
    "    wandb_logger = WandbLogger(project = \"project/finetune\".split('/')[-1].replace('.py', ''), name = \"finetune\")\n",
    "\n",
    "    callbacks_store = []\n",
    "\n",
    "    if conf.early_stopping:\n",
    "        callbacks_store.append(\n",
    "            EarlyStopping(\n",
    "                monitor=conf.monitor_var,\n",
    "                mode=conf.monitor_var_mode,\n",
    "                patience=conf.patience\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # callbacks_store.append(\n",
    "    #     ModelCheckpoint(\n",
    "    #         monitor=conf.monitor_var,\n",
    "    #         # monitor=None,\n",
    "    #         dirpath=f'models/{conf.model_name}',\n",
    "    #         save_top_k=conf.save_top_k,\n",
    "    #         verbose=True,\n",
    "    #         save_last=True,\n",
    "    #         mode=conf.monitor_var_mode\n",
    "    #     )\n",
    "    # )\n",
    "    callbacks_store.append(GenerateTextSamplesCallback(conf.samples_interval))\n",
    "    callbacks_store.append(LearningRateMonitor(logging_interval='step'))\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        gpus=conf.gpus,\n",
    "        accumulate_grad_batches=conf.gradient_acc_steps,\n",
    "        gradient_clip_val=conf.gradient_clip_value,\n",
    "        val_check_interval=conf.val_check_interval,\n",
    "        callbacks=callbacks_store,\n",
    "        max_steps=conf.max_steps,\n",
    "        # max_steps=total_steps,\n",
    "        precision=conf.precision,\n",
    "        amp_level=conf.amp_level,\n",
    "        logger=wandb_logger,\n",
    "        #resume_from_checkpoint=conf.checkpoint_path,\n",
    "        limit_val_batches=conf.val_percent_check\n",
    "    )\n",
    "\n",
    "    # module fit\n",
    "    trainer.fit(pl_module, datamodule=pl_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Missing attribute \"weight_decay\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    291\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'weight_decay'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2568/1673279483.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2568/2857823814.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(conf)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;31m# module fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpl_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpl_data_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    694\u001b[0m         \"\"\"\n\u001b[0;32m    695\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m         self._call_and_handle_interrupt(\n\u001b[0m\u001b[0;32m    697\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    648\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    651\u001b[0m         \u001b[1;31m# TODO(awaelchli): Unify both exceptions below, where `KeyboardError` doesn't re-raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    733\u001b[0m             \u001b[0mckpt_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_provided\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_connected\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m         )\n\u001b[1;32m--> 735\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    736\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    737\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m         \u001b[1;31m# strategy will configure model and move it to the device\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1147\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1149\u001b[0m         \u001b[1;31m# hook\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\pytorch_lightning\\strategies\\single_device.py\u001b[0m in \u001b[0;36msetup\u001b[1;34m(self, trainer)\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msetup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mpl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_to_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 74\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     75\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py\u001b[0m in \u001b[0;36msetup\u001b[1;34m(self, trainer)\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccelerator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_optimizers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup_precision_plugin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0moptimizers_to_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py\u001b[0m in \u001b[0;36msetup_optimizers\u001b[1;34m(self, trainer)\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         self.optimizers, self.lr_scheduler_configs, self.optimizer_frequencies = _init_optimizers_and_lr_schedulers(\n\u001b[0m\u001b[0;32m    142\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlightning_module\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\pytorch_lightning\\core\\optimizer.py\u001b[0m in \u001b[0;36m_init_optimizers_and_lr_schedulers\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m    177\u001b[0m ) -> Tuple[List[Optimizer], List[LRSchedulerConfig], List[int]]:\n\u001b[0;32m    178\u001b[0m     \u001b[1;34m\"\"\"Calls `LightningModule.configure_optimizers` and parses and validates the output.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 179\u001b[1;33m     \u001b[0moptim_conf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_lightning_module_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"configure_optimizers\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpl_module\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0moptim_conf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_call_lightning_module_hook\u001b[1;34m(self, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1548\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1549\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"[LightningModule]{pl_module.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1550\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1552\u001b[0m         \u001b[1;31m# restore current_fx when nested context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_2568/3837862510.py\u001b[0m in \u001b[0;36mconfigure_optimizers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    157\u001b[0m             {\n\u001b[0;32m    158\u001b[0m                 \u001b[1;34m\"params\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnd\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mn\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mnd\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mno_decay\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 159\u001b[1;33m                 \u001b[1;34m\"weight_decay\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhparams\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    160\u001b[0m             },\n\u001b[0;32m    161\u001b[0m             {\n",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    292\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Missing attribute \"{key}\"'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mexp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: Missing attribute \"weight_decay\""
     ]
    }
   ],
   "source": [
    "train(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e68333006553a7c259dbbc354d1c2bf2da12a2e4ac4c5930a431473931cc291e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
