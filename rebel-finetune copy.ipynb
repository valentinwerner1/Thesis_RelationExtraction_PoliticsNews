{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-11-22T13:20:44.703501Z","iopub.status.busy":"2022-11-22T13:20:44.703095Z","iopub.status.idle":"2022-11-22T13:20:53.749446Z","shell.execute_reply":"2022-11-22T13:20:53.748339Z","shell.execute_reply.started":"2022-11-22T13:20:44.703459Z"},"id":"I7G0TrPChwqV","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/werner/thesis_valentin/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import transformers \n","import pandas as pd\n","import numpy as np\n","import json\n","import os\n","os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n","\n","\n","from typing import Any, Union, List, Optional\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from torch.optim import AdamW\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n","from pytorch_lightning.callbacks import LearningRateMonitor\n","import datasets\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForSeq2SeqLM,\n","    AutoTokenizer,\n","    DataCollatorForSeq2Seq,\n","    default_data_collator,\n","    set_seed,\n",")\n","\n","from torch.optim import lr_scheduler\n","import wandb\n","from pytorch_lightning.loggers.wandb import WandbLogger"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2022-11-22T13:20:53.751575Z","iopub.status.busy":"2022-11-22T13:20:53.750854Z","iopub.status.idle":"2022-11-22T13:20:53.849287Z","shell.execute_reply":"2022-11-22T13:20:53.848402Z","shell.execute_reply.started":"2022-11-22T13:20:53.751536Z"},"trusted":true},"outputs":[],"source":["#for pretrain data\n","\n","data = pd.read_csv(\"soft_data/data/out_data/entail_articles_url_coref3.csv.done.csv\", index_col = 0)\n","data = data.rename(columns = {\"label\":\"triplets\"})\n","\n","import re\n","from sklearn.model_selection import train_test_split\n","\n","x_train, x_val, y_train, y_val = train_test_split(data.text, data.triplets, test_size = 0.15)\n","x_val, x_test, y_val, y_test = train_test_split(x_val, y_val, test_size = 0.5)\n","\n","data_dict = {\n","    \"train\": pd.DataFrame(zip(x_train, y_train), columns = [\"text\",\"triplets\"]), \n","    \"val\": pd.DataFrame(zip(x_val, y_val), columns = [\"text\",\"triplets\"]), \n","    \"test\": pd.DataFrame(zip(y_test, y_test), columns = [\"text\",\"triplets\"])\n","    }\n","\n","# ds = datasets.Dataset.from_pandas(data)\n","# ds = ds.remove_columns([\"__index_level_0__\"])\n","\n","# # #### just random splits for testing, change later to proper splitting\n","# ###### and then change later again for CV\n","# split = ds.train_test_split(test_size = 0.15)\n","# split_val = split[\"test\"].train_test_split(test_size = 0.5)\n","# ds = datasets.DatasetDict({\"train\":split[\"train\"],\"val\":split_val[\"train\"], \"test\":split_val[\"test\"]})\n","\n","# # #ds = ds.with_format(\"torch\")\n","# ds"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":337},"execution":{"iopub.execute_input":"2022-11-22T13:20:53.851132Z","iopub.status.busy":"2022-11-22T13:20:53.850779Z","iopub.status.idle":"2022-11-22T13:20:53.965039Z","shell.execute_reply":"2022-11-22T13:20:53.963955Z","shell.execute_reply.started":"2022-11-22T13:20:53.851097Z"},"id":"aV3eJje6h9Ms","outputId":"f94d36d7-c5cf-49ba-90a4-7da89ea77ba8","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>article_id</th>\n","      <th>_input_hash</th>\n","      <th>_task_hash</th>\n","      <th>_is_binary</th>\n","      <th>spans</th>\n","      <th>tokens</th>\n","      <th>_view_id</th>\n","      <th>relations</th>\n","      <th>answer</th>\n","      <th>_timestamp</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Ukraine war: Russian tactics on eastern front ...</td>\n","      <td>0</td>\n","      <td>1192826571</td>\n","      <td>1725000051</td>\n","      <td>False</td>\n","      <td>[{'start': 0, 'end': 7, 'token_start': 0, 'tok...</td>\n","      <td>[{'text': 'Ukraine', 'start': 0, 'end': 7, 'id...</td>\n","      <td>relations</td>\n","      <td>[{'head': 3, 'child': 0, 'head_span': {'start'...</td>\n","      <td>accept</td>\n","      <td>1666853824</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Lee Jae-yong: Samsung appoints Lee Jae-yong to...</td>\n","      <td>2</td>\n","      <td>-386898449</td>\n","      <td>495824270</td>\n","      <td>False</td>\n","      <td>[{'start': 0, 'end': 12, 'token_start': 0, 'to...</td>\n","      <td>[{'text': 'Lee', 'start': 0, 'end': 3, 'id': 0...</td>\n","      <td>relations</td>\n","      <td>[{'head': 31, 'child': 18, 'head_span': {'star...</td>\n","      <td>accept</td>\n","      <td>1666854104</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>New Zealand Instagram couple 'relieved' after ...</td>\n","      <td>3</td>\n","      <td>94784817</td>\n","      <td>-1047716132</td>\n","      <td>False</td>\n","      <td>[{'start': 0, 'end': 28, 'token_start': 0, 'to...</td>\n","      <td>[{'text': 'New', 'start': 0, 'end': 3, 'id': 0...</td>\n","      <td>relations</td>\n","      <td>[{'head': 3, 'child': 9, 'head_span': {'start'...</td>\n","      <td>accept</td>\n","      <td>1666854141</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Iran protests: Police fire on Mahsa Amini mour...</td>\n","      <td>4</td>\n","      <td>-1204324385</td>\n","      <td>1067222802</td>\n","      <td>False</td>\n","      <td>[{'start': 0, 'end': 4, 'token_start': 0, 'tok...</td>\n","      <td>[{'text': 'Iran', 'start': 0, 'end': 4, 'id': ...</td>\n","      <td>relations</td>\n","      <td>[{'head': 3, 'child': 8, 'head_span': {'start'...</td>\n","      <td>accept</td>\n","      <td>1666854246</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Germany plans to legalise recreational cannabi...</td>\n","      <td>5</td>\n","      <td>1523244431</td>\n","      <td>980527859</td>\n","      <td>False</td>\n","      <td>[{'start': 0, 'end': 7, 'token_start': 0, 'tok...</td>\n","      <td>[{'text': 'Germany', 'start': 0, 'end': 7, 'id...</td>\n","      <td>relations</td>\n","      <td>[{'head': 0, 'child': 5, 'head_span': {'start'...</td>\n","      <td>accept</td>\n","      <td>1666854286</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  article_id  _input_hash  \\\n","0  Ukraine war: Russian tactics on eastern front ...           0   1192826571   \n","2  Lee Jae-yong: Samsung appoints Lee Jae-yong to...           2   -386898449   \n","3  New Zealand Instagram couple 'relieved' after ...           3     94784817   \n","4  Iran protests: Police fire on Mahsa Amini mour...           4  -1204324385   \n","5  Germany plans to legalise recreational cannabi...           5   1523244431   \n","\n","   _task_hash  _is_binary                                              spans  \\\n","0  1725000051       False  [{'start': 0, 'end': 7, 'token_start': 0, 'tok...   \n","2   495824270       False  [{'start': 0, 'end': 12, 'token_start': 0, 'to...   \n","3 -1047716132       False  [{'start': 0, 'end': 28, 'token_start': 0, 'to...   \n","4  1067222802       False  [{'start': 0, 'end': 4, 'token_start': 0, 'tok...   \n","5   980527859       False  [{'start': 0, 'end': 7, 'token_start': 0, 'tok...   \n","\n","                                              tokens   _view_id  \\\n","0  [{'text': 'Ukraine', 'start': 0, 'end': 7, 'id...  relations   \n","2  [{'text': 'Lee', 'start': 0, 'end': 3, 'id': 0...  relations   \n","3  [{'text': 'New', 'start': 0, 'end': 3, 'id': 0...  relations   \n","4  [{'text': 'Iran', 'start': 0, 'end': 4, 'id': ...  relations   \n","5  [{'text': 'Germany', 'start': 0, 'end': 7, 'id...  relations   \n","\n","                                           relations  answer  _timestamp  \n","0  [{'head': 3, 'child': 0, 'head_span': {'start'...  accept  1666853824  \n","2  [{'head': 31, 'child': 18, 'head_span': {'star...  accept  1666854104  \n","3  [{'head': 3, 'child': 9, 'head_span': {'start'...  accept  1666854141  \n","4  [{'head': 3, 'child': 8, 'head_span': {'start'...  accept  1666854246  \n","5  [{'head': 0, 'child': 5, 'head_span': {'start'...  accept  1666854286  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["#for train data\n","\n","read = pd.read_json(\"data_src/raw/out_label/summaries_out1.json\", lines = True)\n","df = read[read.answer == \"accept\"]\n","df.head()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-11-22T13:20:53.966927Z","iopub.status.busy":"2022-11-22T13:20:53.966471Z","iopub.status.idle":"2022-11-22T13:20:53.977383Z","shell.execute_reply":"2022-11-22T13:20:53.976078Z","shell.execute_reply.started":"2022-11-22T13:20:53.966889Z"},"id":"uZHFEiugkrGK","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame.\n","Try using .loc[row_indexer,col_indexer] = value instead\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  \n"]}],"source":["#for EDA\n","df[\"relations_count\"] = df.relations.apply(lambda x: len(x))\n","df = df[df.relations_count > 0]"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-11-22T13:20:53.979588Z","iopub.status.busy":"2022-11-22T13:20:53.978721Z","iopub.status.idle":"2022-11-22T13:20:53.988576Z","shell.execute_reply":"2022-11-22T13:20:53.987709Z","shell.execute_reply.started":"2022-11-22T13:20:53.979547Z"},"id":"6ZPJQ0PskslP","trusted":true},"outputs":[{"ename":"NameError","evalue":"name 'df' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn [4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m#prune df\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mdrop(columns \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39m_input_hash\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m_task_hash\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m_is_binary\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m_view_id\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m_timestamp\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      3\u001b[0m df \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mreset_index()\u001b[39m.\u001b[39mdrop(columns \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m\"\u001b[39m])\n","\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"]}],"source":["#prune df\n","df = df.drop(columns = [\"_input_hash\",\"_task_hash\",\"_is_binary\",\"tokens\",\"_view_id\",\"answer\",\"_timestamp\"])\n","df = df.reset_index().drop(columns = [\"index\"])"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-11-22T13:20:53.991075Z","iopub.status.busy":"2022-11-22T13:20:53.990047Z","iopub.status.idle":"2022-11-22T13:20:54.019852Z","shell.execute_reply":"2022-11-22T13:20:54.018970Z","shell.execute_reply.started":"2022-11-22T13:20:53.991037Z"},"id":"H4cXawHhkvT9","trusted":true},"outputs":[],"source":["#get data into needed format and create new DataFrame\n","data = []\n","for index,row in df.iterrows():\n","    rel_list = []\n","    for rel in row.relations:\n","        subj = row.text[rel[\"head_span\"][\"start\"]:rel[\"head_span\"][\"end\"]]\n","        obj = row.text[rel[\"child_span\"][\"start\"]:rel[\"child_span\"][\"end\"]]\n","        rel_list.append(f\"<triplet> {subj} <subj> {obj} <obj> {rel['label']}\")\n","    data.append({\"doc_id\":row.article_id, \"text\": row.text, \"triplets\": \" \".join(rel_list)})\n","\n","data = pd.DataFrame(data, columns = [\"doc_id\",\"text\",\"triplets\"])"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-11-22T13:20:54.023586Z","iopub.status.busy":"2022-11-22T13:20:54.023304Z","iopub.status.idle":"2022-11-22T13:20:54.077208Z","shell.execute_reply":"2022-11-22T13:20:54.076117Z","shell.execute_reply.started":"2022-11-22T13:20:54.023561Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>article_id</th>\n","      <th>spans</th>\n","      <th>relations</th>\n","      <th>relations_count</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Ukraine war: Russian tactics on eastern front ...</td>\n","      <td>0</td>\n","      <td>[{'start': 0, 'end': 7, 'token_start': 0, 'tok...</td>\n","      <td>[{'head': 3, 'child': 0, 'head_span': {'start'...</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Lee Jae-yong: Samsung appoints Lee Jae-yong to...</td>\n","      <td>2</td>\n","      <td>[{'start': 0, 'end': 12, 'token_start': 0, 'to...</td>\n","      <td>[{'head': 31, 'child': 18, 'head_span': {'star...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>New Zealand Instagram couple 'relieved' after ...</td>\n","      <td>3</td>\n","      <td>[{'start': 0, 'end': 28, 'token_start': 0, 'to...</td>\n","      <td>[{'head': 3, 'child': 9, 'head_span': {'start'...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Iran protests: Police fire on Mahsa Amini mour...</td>\n","      <td>4</td>\n","      <td>[{'start': 0, 'end': 4, 'token_start': 0, 'tok...</td>\n","      <td>[{'head': 3, 'child': 8, 'head_span': {'start'...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Germany plans to legalise recreational cannabi...</td>\n","      <td>5</td>\n","      <td>[{'start': 0, 'end': 7, 'token_start': 0, 'tok...</td>\n","      <td>[{'head': 0, 'child': 5, 'head_span': {'start'...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>148</th>\n","      <td>Macri calls for unity but gives little away at...</td>\n","      <td>194</td>\n","      <td>[{'start': 0, 'end': 5, 'token_start': 0, 'tok...</td>\n","      <td>[{'head': 20, 'child': 14, 'head_span': {'star...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>149</th>\n","      <td>Whirlpool touts bet on troubled Argentina as o...</td>\n","      <td>195</td>\n","      <td>[{'start': 0, 'end': 15, 'token_start': 0, 'to...</td>\n","      <td>[{'head': 1, 'child': 5, 'head_span': {'start'...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>150</th>\n","      <td>LGBT football fans fight for safe space in Bra...</td>\n","      <td>197</td>\n","      <td>[{'start': 0, 'end': 18, 'token_start': 0, 'to...</td>\n","      <td>[{'head': 2, 'child': 8, 'head_span': {'start'...</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>151</th>\n","      <td>ANSES: Half a million sign up for emergency fo...</td>\n","      <td>198</td>\n","      <td>[{'start': 7, 'end': 21, 'token_start': 2, 'to...</td>\n","      <td>[{'head': 15, 'child': 32, 'head_span': {'star...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>152</th>\n","      <td>EU ‘fully committed’ to Mercosur free-trade de...</td>\n","      <td>199</td>\n","      <td>[{'start': 0, 'end': 2, 'token_start': 0, 'tok...</td>\n","      <td>[{'head': 0, 'child': 6, 'head_span': {'start'...</td>\n","      <td>4</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>153 rows × 5 columns</p>\n","</div>"],"text/plain":["                                                  text  article_id  \\\n","0    Ukraine war: Russian tactics on eastern front ...           0   \n","1    Lee Jae-yong: Samsung appoints Lee Jae-yong to...           2   \n","2    New Zealand Instagram couple 'relieved' after ...           3   \n","3    Iran protests: Police fire on Mahsa Amini mour...           4   \n","4    Germany plans to legalise recreational cannabi...           5   \n","..                                                 ...         ...   \n","148  Macri calls for unity but gives little away at...         194   \n","149  Whirlpool touts bet on troubled Argentina as o...         195   \n","150  LGBT football fans fight for safe space in Bra...         197   \n","151  ANSES: Half a million sign up for emergency fo...         198   \n","152  EU ‘fully committed’ to Mercosur free-trade de...         199   \n","\n","                                                 spans  \\\n","0    [{'start': 0, 'end': 7, 'token_start': 0, 'tok...   \n","1    [{'start': 0, 'end': 12, 'token_start': 0, 'to...   \n","2    [{'start': 0, 'end': 28, 'token_start': 0, 'to...   \n","3    [{'start': 0, 'end': 4, 'token_start': 0, 'tok...   \n","4    [{'start': 0, 'end': 7, 'token_start': 0, 'tok...   \n","..                                                 ...   \n","148  [{'start': 0, 'end': 5, 'token_start': 0, 'tok...   \n","149  [{'start': 0, 'end': 15, 'token_start': 0, 'to...   \n","150  [{'start': 0, 'end': 18, 'token_start': 0, 'to...   \n","151  [{'start': 7, 'end': 21, 'token_start': 2, 'to...   \n","152  [{'start': 0, 'end': 2, 'token_start': 0, 'tok...   \n","\n","                                             relations  relations_count  \n","0    [{'head': 3, 'child': 0, 'head_span': {'start'...                4  \n","1    [{'head': 31, 'child': 18, 'head_span': {'star...                2  \n","2    [{'head': 3, 'child': 9, 'head_span': {'start'...                1  \n","3    [{'head': 3, 'child': 8, 'head_span': {'start'...                2  \n","4    [{'head': 0, 'child': 5, 'head_span': {'start'...                1  \n","..                                                 ...              ...  \n","148  [{'head': 20, 'child': 14, 'head_span': {'star...                2  \n","149  [{'head': 1, 'child': 5, 'head_span': {'start'...                2  \n","150  [{'head': 2, 'child': 8, 'head_span': {'start'...                2  \n","151  [{'head': 15, 'child': 32, 'head_span': {'star...                1  \n","152  [{'head': 0, 'child': 6, 'head_span': {'start'...                4  \n","\n","[153 rows x 5 columns]"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["df"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-11-22T13:20:54.078927Z","iopub.status.busy":"2022-11-22T13:20:54.078602Z","iopub.status.idle":"2022-11-22T13:20:54.083391Z","shell.execute_reply":"2022-11-22T13:20:54.082441Z","shell.execute_reply.started":"2022-11-22T13:20:54.078893Z"},"id":"BYOOvGK6kwtj","outputId":"d6de6d0f-d61c-4d6d-a437-7fbdd605dd37","trusted":true},"outputs":[],"source":["#ds = datasets.Dataset.from_pandas(data.drop(columns=[\"doc_id\"]))\n","\n","#### just random splits for testing, change later to proper splitting\n","###### and then change later again for CV\n","#split = ds.train_test_split(test_size = 0.3)\n","#split_val = split[\"test\"].train_test_split(test_size = 0.3)\n","#ds = datasets.DatasetDict({\"train\":split[\"train\"],\"val\":split_val[\"train\"], \"test\":split_val[\"test\"]})\n","\n","#ds = ds.with_format(\"torch\")\n","#ds"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2022-11-22T13:20:54.085417Z","iopub.status.busy":"2022-11-22T13:20:54.084802Z","iopub.status.idle":"2022-11-22T13:20:54.094383Z","shell.execute_reply":"2022-11-22T13:20:54.093457Z","shell.execute_reply.started":"2022-11-22T13:20:54.085373Z"},"id":"ERXMfgkhkyFS","trusted":true},"outputs":[],"source":["class conf:\n","    #general\n","    seed = 0\n","    gpus = 1 #more?\n","    \n","    #input\n","    batch_size = 8\n","    max_length = 128\n","    ignore_pad_token_for_loss = True\n","    use_fast_tokenizer = True\n","    gradient_acc_steps = 1\n","    gradient_clip_value = 10.0\n","    load_workers = 8 #8 are local laptop\n","\n","    #optimizer\n","    lr = 0.00005\n","    weight_decay = 0.01\n","    beta1 = 0.9\n","    beta2 = 0.999\n","    epsilon = 0.00000001\n","    warm_up = 1 #num of epochs to warm_up on\n","\n","    #training\n","    max_steps = 1200000\n","    samples_interval = 1000\n","\n","    monitor_var  = \"val_loss\"\n","    monitor_var_mode = \"min\"\n","    # val_check_interval = 0.5\n","    # val_percent_check = 0.1\n","\n","    model_name = \"model1.pth\"\n","    checkpoint_path = f\"models/{model_name}\"\n","    save_top_k = 1\n","\n","    early_stopping = False\n","    patience = \"5\"\n","\n","    length_penalty = 0\n","    no_repeat_ngram_size = 0\n","    num_beams = 3\n","    precision = 16\n","    amp_level = None\n","\n","    "]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-11-22T13:20:54.096451Z","iopub.status.busy":"2022-11-22T13:20:54.096017Z","iopub.status.idle":"2022-11-22T13:20:54.110734Z","shell.execute_reply":"2022-11-22T13:20:54.109688Z","shell.execute_reply.started":"2022-11-22T13:20:54.096404Z"},"id":"mN1ASyHskzec","trusted":true},"outputs":[],"source":["class GetData(pl.LightningDataModule):\n","    def __init__(self, conf: conf, tokenizer: AutoTokenizer, model: AutoModelForSeq2SeqLM):\n","        \"\"\"init params from config\"\"\"\n","        super().__init__()\n","        self.tokenizer = tokenizer\n","        self.model = model\n","        self.datasets = data_dict\n","        \n","        # Data collator\n","        label_pad_token_id = -100                       \n","        self.data_collator = DataCollatorForSeq2Seq(self.tokenizer, self.model, label_pad_token_id=label_pad_token_id, padding = True)\n","\n","    def preprocess_function(self, data):\n","        \"\"\"tokenize, pad, truncate\"\"\"\n","        \n","        #split into input and labels\n","        inputs = data[\"text\"]       \n","        outputs = data[\"triplets\"]\n","\n","        #process input\n","        model_inputs = self.tokenizer(inputs, max_length = conf.max_length, padding = True, truncation = True)\n","\n","        #process labels\n","        with self.tokenizer.as_target_tokenizer():\n","            labels = self.tokenizer(outputs, max_length = conf.max_length, padding = True, truncation = True)\n","\n","        model_inputs[\"labels\"] = labels[\"input_ids\"]\n","        return model_inputs\n","\n","    def apply_mask(self, data):\n","        #masking entities in sentence\n","        new = []\n","        for row in data.iterrows():\n","            split = re.split(\"<\\w*>\", row[1][\"triplets\"])[1:]   #first one is empty\n","            for i in range(int(len(split)/3)):                  #always pairs of 3\n","                sub = split[i*3:i*3+3]\n","                subj = sub[0]\n","                obj = sub[1]\n","                if np.random.binomial(1, 0.15, 1) == 1:\n","                    tok = np.random.choice(sub).rstrip().lstrip()\n","                    new.append([row[1][\"text\"].replace(tok, \"<MASK>\"), row[1][\"triplets\"].replace(tok, \"<MASK>\")])\n","                    break\n","                else: \n","                    new.append([row[1][\"text\"], row[1][\"triplets\"]])\n","\n","        df = pd.DataFrame(new, columns = [\"text\", \"triplets\"])\n","        df = df.drop_duplicates(\"text\")\n","\n","        ds = datasets.Dataset.from_pandas(df)\n","        ds = ds.remove_columns([\"__index_level_0__\"])\n","        \n","        return ds \n","        \n","    #apply the preprocessing and load data\n","    def train_dataloader(self, *args, **kwargs): \n","        self.train_dataset = self.datasets[\"train\"]\n","        self.train_dataset = self.apply_mask(self.train_dataset)\n","        self.train_dataset = self.train_dataset.map(self.preprocess_function, remove_columns = [\"text\", \"triplets\"], batched = True)\n","        return DataLoader(self.train_dataset, batch_size = conf.batch_size, collate_fn = self.data_collator, shuffle = True, num_workers= conf.load_workers)\n","    \n","    def val_dataloader(self, *args, **kwargs): \n","        self.eval_dataset = self.datasets[\"val\"]\n","        self.eval_dataset = self.apply_mask(self.eval_dataset)\n","        self.eval_dataset = self.eval_dataset.map(self.preprocess_function, remove_columns = [\"text\", \"triplets\"], batched = True)\n","        return DataLoader(self.eval_dataset, batch_size = conf.batch_size, collate_fn = self.data_collator, num_workers= conf.load_workers)\n","\n","    def test_dataloader(self, *args, **kwargs): \n","        self.test_dataset = self.datasets[\"test\"]\n","        self.test_dataset = self.apply_mask(self.test_dataset)\n","        self.test_dataset = self.test_dataset.map(self.preprocess_function,  remove_columns = [\"text\", \"triplets\"], batched = True)\n","        return DataLoader(self.test_dataset, batch_size = conf.batch_size, collate_fn = self.data_collator, num_workers= conf.load_workers)"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2022-11-22T13:20:54.112828Z","iopub.status.busy":"2022-11-22T13:20:54.112449Z","iopub.status.idle":"2022-11-22T13:20:54.129188Z","shell.execute_reply":"2022-11-22T13:20:54.127977Z","shell.execute_reply.started":"2022-11-22T13:20:54.112790Z"},"id":"Y9ELEKQHk2-f","trusted":true},"outputs":[],"source":["#from REBEL\n","from typing import Sequence\n","\n","import torch\n","from pytorch_lightning import Callback, LightningModule, Trainer\n","from torch import nn\n","import pandas as pd\n","from torch.nn.utils.rnn import pad_sequence\n","import wandb\n","\n","class GenerateTextSamplesCallback(Callback):\n","    \"\"\"\n","    PL Callback to generate triplets along training\n","    \"\"\"\n","\n","    def __init__(self, logging_batch_interval):\n","        \"\"\"\n","        Args:\n","            logging_batch_interval: How frequently to inspect/potentially plot something\n","        \"\"\"\n","        super().__init__()\n","        self.logging_batch_interval = logging_batch_interval\n","\n","    def on_train_batch_end(self,trainer: Trainer,pl_module: LightningModule, outputs: Sequence, batch: Sequence, batch_idx: int) -> None:\n","        wandb_table = wandb.Table(columns=[\"Source\", \"Pred\", \"Gold\"])\n","        # pl_module.logger.info(\"Executing translation callback\")\n","        labels = batch.pop(\"labels\")\n","        gen_kwargs = {\n","            \"max_length\": conf.max_length,\n","            \"early_stopping\": False,\n","            \"no_repeat_ngram_size\": 0,\n","            \"num_beams\": conf.num_beams\n","        }\n","        pl_module.eval()\n","\n","        decoder_inputs = torch.roll(labels, 1, 1)[:,0:2]\n","        decoder_inputs[:, 0] = 0\n","        generated_tokens = pl_module.model.generate(\n","            batch[\"input_ids\"].to(pl_module.model.device),\n","            attention_mask=batch[\"attention_mask\"].to(pl_module.model.device),\n","            decoder_input_ids=decoder_inputs.to(pl_module.model.device),\n","            **gen_kwargs,\n","        )\n","        # in case the batch is shorter than max length, the output should be padded\n","        if generated_tokens.shape[-1] < gen_kwargs[\"max_length\"]:\n","            generated_tokens = pl_module._pad_tensors_to_max_len(generated_tokens, gen_kwargs[\"max_length\"])\n","        pl_module.train()\n","        decoded_preds = pl_module.tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n","\n","        #If ignore pad token for loss == True \n","            # Replace -100 in the labels as we can't decode them.\n","        labels = torch.where(labels != -100, labels, pl_module.tokenizer.pad_token_id)\n","\n","        decoded_labels = pl_module.tokenizer.batch_decode(labels, skip_special_tokens=False)\n","        decoded_inputs = pl_module.tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=False)\n","\n","        # pl_module.logger.experiment.log_text('generated samples', '\\n'.join(decoded_preds).replace('<pad>', ''))\n","        # pl_module.logger.experiment.log_text('original samples', '\\n'.join(decoded_labels).replace('<pad>', ''))\n","        for source, translation, gold_output in zip(decoded_inputs, decoded_preds, decoded_labels):\n","            wandb_table.add_data(\n","                source.replace('<pad>', ''), translation.replace('<pad>', ''), gold_output.replace('<pad>', '')\n","            )\n","        pl_module.logger.experiment.log({\"Triplets\": wandb_table})"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-11-22T13:20:54.131056Z","iopub.status.busy":"2022-11-22T13:20:54.130636Z","iopub.status.idle":"2022-11-22T13:20:54.143893Z","shell.execute_reply":"2022-11-22T13:20:54.142936Z","shell.execute_reply.started":"2022-11-22T13:20:54.131019Z"},"id":"5NAqweGsk4Op","trusted":true},"outputs":[],"source":["#from REBEL\n","def shift_tokens_left(input_ids: torch.Tensor, pad_token_id: int):\n","    \"\"\"\n","    Shift input ids one token to the right.\n","    \"\"\"\n","    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n","    shifted_input_ids[:, :-1] = input_ids[:, 1:].clone()\n","    shifted_input_ids[:, -1] = pad_token_id\n","\n","    assert pad_token_id is not None, \"self.model.config.pad_token_id has to be defined.\"\n","\n","    return shifted_input_ids"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-11-22T13:20:54.145980Z","iopub.status.busy":"2022-11-22T13:20:54.145403Z","iopub.status.idle":"2022-11-22T13:20:54.157399Z","shell.execute_reply":"2022-11-22T13:20:54.156307Z","shell.execute_reply.started":"2022-11-22T13:20:54.145943Z"},"id":"91a0v9t_k-Hx","trusted":true},"outputs":[],"source":["#from REBEL\n","\n","##### can maybe be rewritten more effective\n","def extract_triplets(text):\n","    triplets = []\n","    relation, subject, relation, object_ = '', '', '', ''\n","    text = text.strip()\n","    current = 'x'\n","    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n","        if token == \"<triplet>\":\n","            current = 't'\n","            if relation != '':\n","                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n","                relation = ''\n","            subject = ''\n","        elif token == \"<subj>\":\n","            current = 's'\n","            if relation != '':\n","                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n","            object_ = ''\n","        elif token == \"<obj>\":\n","            current = 'o'\n","            relation = ''\n","        else:\n","            if current == 't':\n","                subject += ' ' + token\n","            elif current == 's':\n","                object_ += ' ' + token\n","            elif current == 'o':\n","                relation += ' ' + token\n","    if subject != '' and relation != '' and object_ != '':\n","        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n","    return triplets"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-11-22T13:20:54.159715Z","iopub.status.busy":"2022-11-22T13:20:54.159175Z","iopub.status.idle":"2022-11-22T13:20:54.197544Z","shell.execute_reply":"2022-11-22T13:20:54.196406Z","shell.execute_reply.started":"2022-11-22T13:20:54.159680Z"},"id":"MRoOQCczqp4k","trusted":true},"outputs":[],"source":["#from REBEL \n","def score(key, prediction, verbose=False):\n","    correct_by_relation = Counter()\n","    guessed_by_relation = Counter()\n","    gold_by_relation    = Counter()\n","\n","    # Loop over the data to compute a score\n","    for row in range(len(prediction)):\n","        gold = key[row]\n","        guess = prediction[row]\n","         \n","        if gold == NO_RELATION and guess == NO_RELATION:\n","            pass\n","        elif gold == NO_RELATION and guess != NO_RELATION:\n","            guessed_by_relation[guess] += 1\n","        elif gold != NO_RELATION and guess == NO_RELATION:\n","            gold_by_relation[gold] += 1\n","        elif gold != NO_RELATION and guess != NO_RELATION:\n","            guessed_by_relation[guess] += 1\n","            gold_by_relation[gold] += 1\n","            if gold == guess:\n","                correct_by_relation[guess] += 1\n","\n","    # Print verbose information\n","    if verbose:\n","        print(\"Per-relation statistics:\")\n","        relations = gold_by_relation.keys()\n","        longest_relation = 0\n","        for relation in sorted(relations):\n","            longest_relation = max(len(relation), longest_relation)\n","        for relation in sorted(relations):\n","            # (compute the score)\n","            correct = correct_by_relation[relation]\n","            guessed = guessed_by_relation[relation]\n","            gold    = gold_by_relation[relation]\n","            prec = 1.0\n","            if guessed > 0:\n","                prec = float(correct) / float(guessed)\n","            recall = 0.0\n","            if gold > 0:\n","                recall = float(correct) / float(gold)\n","            f1 = 0.0\n","            if prec + recall > 0:\n","                f1 = 2.0 * prec * recall / (prec + recall)\n","            # (print the score)\n","            sys.stdout.write((\"{:<\" + str(longest_relation) + \"}\").format(relation))\n","            sys.stdout.write(\"  P: \")\n","            if prec < 0.1: sys.stdout.write(' ')\n","            if prec < 1.0: sys.stdout.write(' ')\n","            sys.stdout.write(\"{:.2%}\".format(prec))\n","            sys.stdout.write(\"  R: \")\n","            if recall < 0.1: sys.stdout.write(' ')\n","            if recall < 1.0: sys.stdout.write(' ')\n","            sys.stdout.write(\"{:.2%}\".format(recall))\n","            sys.stdout.write(\"  F1: \")\n","            if f1 < 0.1: sys.stdout.write(' ')\n","            if f1 < 1.0: sys.stdout.write(' ')\n","            sys.stdout.write(\"{:.2%}\".format(f1))\n","            sys.stdout.write(\"  #: %d\" % gold)\n","            sys.stdout.write(\"\\n\")\n","        print(\"\")\n","\n","    # Print the aggregate score\n","    if verbose:\n","        print(\"Final Score:\")\n","    prec_micro = 1.0\n","    if sum(guessed_by_relation.values()) > 0:\n","        prec_micro   = float(sum(correct_by_relation.values())) / float(sum(guessed_by_relation.values()))\n","    recall_micro = 0.0\n","    if sum(gold_by_relation.values()) > 0:\n","        recall_micro = float(sum(correct_by_relation.values())) / float(sum(gold_by_relation.values()))\n","    f1_micro = 0.0\n","    if prec_micro + recall_micro > 0.0:\n","        f1_micro = 2.0 * prec_micro * recall_micro / (prec_micro + recall_micro)\n","    print(\"Precision (micro): {:.3%}\".format(prec_micro))\n","    print(\"   Recall (micro): {:.3%}\".format(recall_micro))\n","    print(\"       F1 (micro): {:.3%}\".format(f1_micro))\n","    return prec_micro, recall_micro, f1_micro\n","\n","'''Adapted from: https://github.com/btaille/sincere/blob/6f5472c5aeaf7ef7765edf597ede48fdf1071712/code/utils/evaluation.py'''\n","def re_score(pred_relations, gt_relations, relation_types, mode=\"boundaries\"):\n","    \"\"\"Evaluate RE predictions\n","    Args:\n","        pred_relations (list) :  list of list of predicted relations (several relations in each sentence)\n","        gt_relations (list) :    list of list of ground truth relations\n","            rel = { \"head\": (start_idx (inclusive), end_idx (exclusive)),\n","                    \"tail\": (start_idx (inclusive), end_idx (exclusive)),\n","                    \"head_type\": ent_type,\n","                    \"tail_type\": ent_type,\n","                    \"type\": rel_type}\n","        vocab (Vocab) :         dataset vocabulary\n","        mode (str) :            in 'strict' or 'boundaries' \"\"\"\n","\n","    assert mode in [\"strict\", \"boundaries\"]\n","    relation_types = [\"MakePublicStatement\",\"Appeal\",\"ExpressIntentToCooperate\",\"Consult\",\"EngageInDiplomaticCooperation\",\"EngageInMaterialCooperation\",\"ProvideAid\",\"Yield\",\"Investigate\",\"Demand\",\"Disapprove\",\"Reject\",\"Threaten\",\"ExhibitMilitaryPosture\",\"Protest\",\"ReduceRelations\",\"Coerce\",\"Assault\",\"Fight\",\"EngageInUnconventialMassViolence\"]\n","        \n","    # relation_types = [v for v in relation_types if not v == \"None\"]\n","    scores = {rel: {\"tp\": 0, \"fp\": 0, \"fn\": 0} for rel in relation_types + [\"ALL\"]}\n","\n","    # Count GT relations and Predicted relations\n","    n_sents = len(gt_relations)\n","    n_rels = sum([len([rel for rel in sent]) for sent in gt_relations])\n","    n_found = sum([len([rel for rel in sent]) for sent in pred_relations])\n","\n","    # Count TP, FP and FN per type\n","    for pred_sent, gt_sent in zip(pred_relations, gt_relations):\n","        for rel_type in relation_types:\n","            # strict mode takes argument types into account\n","            if mode == \"strict\":\n","                pred_rels = {(rel[\"head\"], rel[\"head_type\"], rel[\"tail\"], rel[\"tail_type\"]) for rel in pred_sent if\n","                             rel[\"type\"] == rel_type}\n","                gt_rels = {(rel[\"head\"], rel[\"head_type\"], rel[\"tail\"], rel[\"tail_type\"]) for rel in gt_sent if\n","                           rel[\"type\"] == rel_type}\n","\n","            # boundaries mode only takes argument spans into account\n","            elif mode == \"boundaries\":\n","                pred_rels = {(rel[\"head\"], rel[\"tail\"]) for rel in pred_sent if rel[\"type\"] == rel_type}\n","                gt_rels = {(rel[\"head\"], rel[\"tail\"]) for rel in gt_sent if rel[\"type\"] == rel_type}\n","\n","            scores[rel_type][\"tp\"] += len(pred_rels & gt_rels)\n","            scores[rel_type][\"fp\"] += len(pred_rels - gt_rels)\n","            scores[rel_type][\"fn\"] += len(gt_rels - pred_rels)\n","\n","    # Compute per relation Precision / Recall / F1\n","    for rel_type in scores.keys():\n","        if scores[rel_type][\"tp\"]:\n","            scores[rel_type][\"p\"] = 100 * scores[rel_type][\"tp\"] / (scores[rel_type][\"fp\"] + scores[rel_type][\"tp\"])\n","            scores[rel_type][\"r\"] = 100 * scores[rel_type][\"tp\"] / (scores[rel_type][\"fn\"] + scores[rel_type][\"tp\"])\n","        else:\n","            scores[rel_type][\"p\"], scores[rel_type][\"r\"] = 0, 0\n","\n","        if not scores[rel_type][\"p\"] + scores[rel_type][\"r\"] == 0:\n","            scores[rel_type][\"f1\"] = 2 * scores[rel_type][\"p\"] * scores[rel_type][\"r\"] / (\n","                    scores[rel_type][\"p\"] + scores[rel_type][\"r\"])\n","        else:\n","            scores[rel_type][\"f1\"] = 0\n","\n","    # Compute micro F1 Scores\n","    tp = sum([scores[rel_type][\"tp\"] for rel_type in relation_types])\n","    fp = sum([scores[rel_type][\"fp\"] for rel_type in relation_types])\n","    fn = sum([scores[rel_type][\"fn\"] for rel_type in relation_types])\n","\n","    if tp:\n","        precision = 100 * tp / (tp + fp)\n","        recall = 100 * tp / (tp + fn)\n","        f1 = 2 * precision * recall / (precision + recall)\n","\n","    else:\n","        precision, recall, f1 = 0, 0, 0\n","\n","    scores[\"ALL\"][\"p\"] = precision\n","    scores[\"ALL\"][\"r\"] = recall\n","    scores[\"ALL\"][\"f1\"] = f1\n","    scores[\"ALL\"][\"tp\"] = tp\n","    scores[\"ALL\"][\"fp\"] = fp\n","    scores[\"ALL\"][\"fn\"] = fn\n","\n","    # Compute Macro F1 Scores\n","    scores[\"ALL\"][\"Macro_f1\"] = np.mean([scores[ent_type][\"f1\"] for ent_type in relation_types])\n","    scores[\"ALL\"][\"Macro_p\"] = np.mean([scores[ent_type][\"p\"] for ent_type in relation_types])\n","    scores[\"ALL\"][\"Macro_r\"] = np.mean([scores[ent_type][\"r\"] for ent_type in relation_types])\n","\n","    print(f\"RE Evaluation in *** {mode.upper()} *** mode\")\n","\n","    print(\n","        \"processed {} sentences with {} relations; found: {} relations; correct: {}.\".format(n_sents, n_rels, n_found,\n","                                                                                             tp))\n","    print(\n","        \"\\tALL\\t TP: {};\\tFP: {};\\tFN: {}\".format(\n","            scores[\"ALL\"][\"tp\"],\n","            scores[\"ALL\"][\"fp\"],\n","            scores[\"ALL\"][\"fn\"]))\n","    print(\n","        \"\\t\\t(m avg): precision: {:.2f};\\trecall: {:.2f};\\tf1: {:.2f} (micro)\".format(\n","            precision,\n","            recall,\n","            f1))\n","    print(\n","        \"\\t\\t(M avg): precision: {:.2f};\\trecall: {:.2f};\\tf1: {:.2f} (Macro)\\n\".format(\n","            scores[\"ALL\"][\"Macro_p\"],\n","            scores[\"ALL\"][\"Macro_r\"],\n","            scores[\"ALL\"][\"Macro_f1\"]))\n","\n","    for rel_type in relation_types:\n","        print(\"\\t{}: \\tTP: {};\\tFP: {};\\tFN: {};\\tprecision: {:.2f};\\trecall: {:.2f};\\tf1: {:.2f};\\t{}\".format(\n","            rel_type,\n","            scores[rel_type][\"tp\"],\n","            scores[rel_type][\"fp\"],\n","            scores[rel_type][\"fn\"],\n","            scores[rel_type][\"p\"],\n","            scores[rel_type][\"r\"],\n","            scores[rel_type][\"f1\"],\n","            scores[rel_type][\"tp\"] +\n","            scores[rel_type][\n","                \"fp\"]))\n","\n","    return scores, precision, recall, f1"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2022-11-22T13:20:54.200142Z","iopub.status.busy":"2022-11-22T13:20:54.198987Z","iopub.status.idle":"2022-11-22T13:20:54.233334Z","shell.execute_reply":"2022-11-22T13:20:54.232413Z","shell.execute_reply.started":"2022-11-22T13:20:54.200043Z"},"id":"33KLRy0hk_4f","trusted":true},"outputs":[],"source":["class BaseModule(pl.LightningModule):\n","\n","    def __init__(self, conf, config: AutoConfig, tokenizer: AutoTokenizer, model: AutoModelForSeq2SeqLM):\n","        super().__init__()\n","        self.config = config\n","        self.model = model\n","        self.tokenizer = tokenizer\n","        #self.loss_fn = label_smoothed_nll_loss\n","        self.loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n","        self.num_beams = conf.num_beams\n","\n","    def forward(self, inputs, labels, *args):\n","        ##### Check later if smooth labeled loss is better \n","        outputs = self.model(**inputs, labels = labels, use_cache = False, return_dict = True, output_hidden_states = True)\n","        output_dict = {'loss': outputs['loss'], 'logits': outputs['logits']}\n","        return output_dict\n","\n","    def training_step(self, batch: dict, batch_idx: int):\n","        ##### check later if labels = batch[\"labels\"] also works\n","        labels = batch.pop(\"labels\")\n","        labels_original = labels.clone()\n","        batch[\"decoder_input_ids\"] = torch.where(labels != -100, labels, self.config.pad_token_id)\n","        labels = shift_tokens_left(labels, -100)\n","\n","        forward_output = self.forward(batch, labels)\n","        self.log('loss', forward_output['loss'])\n","\n","        batch[\"labels\"] = labels_original\n","\n","        #### ig i dont have this\n","        if 'loss_aux' in forward_output:\n","            self.log('loss_classifier', forward_output['loss_aux'])\n","            return forward_output['loss'] + forward_output['loss_aux']\n","\n","        return forward_output['loss']\n","\n","    def validation_step(self, batch: dict, batch_idx):\n","        #### pop maybe not needed?\n","        labels = batch.pop(\"labels\")\n","        batch[\"decoder_input_ids\"] = torch.where(labels != -100, labels, self.config.pad_token_id)\n","        labels = shift_tokens_left(labels, -100)\n","        \n","        with torch.no_grad():\n","            # compute loss on predict data\n","            forward_output = self.forward(batch, labels)\n","\n","        forward_output['loss'] = forward_output['loss'].mean().detach()\n","\n","        #### probably not needed ? why would i want only pred loss\n","        # if self.hparams.prediction_loss_only:\n","        #     self.log('val_loss', forward_output['loss'])\n","        #     return\n","\n","        forward_output['logits'] = forward_output['logits'].detach()\n","\n","        if labels.shape[-1] < conf.max_length:\n","            forward_output['labels'] = self._pad_tensors_to_max_len(labels, conf.max_length)\n","        else:\n","            forward_output['labels'] = labels\n","\n","        metrics = {}\n","        metrics['val_loss'] = forward_output['loss']\n","\n","        #### only 1? so why loop lmao\n","        for key in sorted(metrics.keys()):\n","            self.log(key, metrics[key])\n","\n","        outputs = {}\n","        outputs['predictions'], outputs['labels'] = self.generate_triples(batch, labels)\n","        return outputs\n","\n","\n","    def test_step(self, batch, batch_idx):\n","\n","        #### popping again\n","        labels = batch.pop(\"labels\")\n","        batch[\"decoder_input_ids\"] = torch.where(labels != -100, labels, self.config.pad_token_id)\n","        labels = shift_tokens_left(labels, -100)\n","\n","        with torch.no_grad():\n","            # compute loss on predict data\n","            forward_output = self.forward(batch, labels)\n","\n","        forward_output['loss'] = forward_output['loss'].mean().detach()\n","\n","        forward_output['logits'] = forward_output['logits'].detach()\n","\n","        if labels.shape[-1] < conf.max_length:\n","            forward_output['labels'] = self._pad_tensors_to_max_len(labels, conf.max_length)\n","        else:\n","            forward_output['labels'] = labels\n","\n","\n","        metrics = {}\n","        metrics['test_loss'] = forward_output['loss']\n","\n","        #### dont i only have one metric anyways?\n","        for key in sorted(metrics.keys()):\n","            self.log(key, metrics[key], prog_bar=True)\n","        \n","        self.log(\"lr\", optimizer.get_last_lr())\n","\n","        #### what does this actually do? how does this change everything\n","        # if self.hparams.finetune:\n","        #     return {'predictions': self.forward_samples(batch, labels)}\n","        # else:\n","\n","        outputs = {}\n","        outputs['predictions'], outputs['labels'] = self.generate_triples(batch, labels)\n","        return outputs\n","\n","\n","\n","    def validation_epoch_end(self, output: dict):\n","        \n","        relations = [\"MakePublicStatement\",\"Appeal\",\"ExpressIntentToCooperate\",\"Consult\",\"EngageInDiplomaticCooperation\",\"EngageInMaterialCooperation\",\"ProvideAid\",\"Yield\",\"Investigate\",\"Demand\",\"Disapprove\",\"Reject\",\"Threaten\",\"ExhibitMilitaryPosture\",\"Protest\",\"ReduceRelations\",\"Coerce\",\"Assault\",\"Fight\",\"EngageInUnconventialMassViolence\"]\n","        scores, precision, recall, f1 = re_score([item for pred in output for item in pred['predictions']], [item for pred in output for item in pred['labels']], relations)\n","        self.log('val_prec_micro', precision)\n","        self.log('val_recall_micro', recall)\n","        self.log('val_F1_micro', f1)\n","\n","    def test_epoch_end(self, output: dict):\n","\n","        relations = [\"MakePublicStatement\",\"Appeal\",\"ExpressIntentToCooperate\",\"Consult\",\"EngageInDiplomaticCooperation\",\"EngageInMaterialCooperation\",\"ProvideAid\",\"Yield\",\"Investigate\",\"Demand\",\"Disapprove\",\"Reject\",\"Threaten\",\"ExhibitMilitaryPosture\",\"Protest\",\"ReduceRelations\",\"Coerce\",\"Assault\",\"Fight\",\"EngageInUnconventialMassViolence\"]\n","        scores, precision, recall, f1 = re_score([item for pred in output for item in pred['predictions']], [item for pred in output for item in pred['labels']], relations)\n","        self.log('test_prec_micro', precision)\n","        self.log('test_recall_micro', recall)\n","        self.log('test_F1_micro', f1)\n","\n","\n","    # additional functions called in main functions\n","\n","    def generate_triples(self, batch, labels) -> None:\n","\n","        generated_tokens = self.model.generate(\n","            batch[\"input_ids\"].to(self.model.device),\n","            attention_mask=batch[\"attention_mask\"].to(self.model.device),\n","            use_cache = True, max_length = conf.max_length, early_stopping = conf.early_stopping, length_penalty = conf.length_penalty, \n","            no_repeat_ngram_size = conf.no_repeat_ngram_size, num_beams = conf.num_beams)\n","\n","        decoded_preds = self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n","        decoded_labels = self.tokenizer.batch_decode(torch.where(labels != -100, labels, self.config.pad_token_id), skip_special_tokens=False)\n","\n","        return [extract_triplets(rel) for rel in decoded_preds], [extract_triplets(rel) for rel in decoded_labels]\n","\n","    def _pad_tensors_to_max_len(self, tensor, max_length):\n","        # If PAD token is not defined at least EOS token has to be defined\n","        pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else self.config.eos_token_id\n","\n","        if pad_token_id is None:\n","            raise ValueError(\n","                f\"Make sure that either `config.pad_token_id` or `config.eos_token_id` is defined if tensor has to be padded to `max_length`={max_length}\"\n","            )\n","\n","        padded_tensor = pad_token_id * torch.ones(\n","            (tensor.shape[0], max_length), dtype=tensor.dtype, device=tensor.device\n","        )\n","        padded_tensor[:, : tensor.shape[-1]] = tensor\n","        return padded_tensor\n","\n","    def configure_optimizers(self):\n","\n","        ##### HUH\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        optimizer_grouped_parameters = [\n","            {\n","                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": conf.weight_decay,\n","            },\n","            {\n","                \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","        \n","        optimizer = AdamW(optimizer_grouped_parameters, lr = conf.lr, betas = (conf.beta1, conf.beta2), eps = conf.epsilon, weight_decay = conf.weight_decay)\n","\n","        def lr_schedule(epoch):\n","            if epoch < conf.warm_up: lr_scale =  0.1\n","            else: lr_scale = 1 / (epoch**0.3)\n","            return lr_scale\n","\n","        scheduler = lr_scheduler.LambdaLR(\n","            optimizer,\n","            lr_lambda=lr_schedule\n","        )\n","        #scheduler = inverse_square_root(optimizer, num_warmup_steps= conf.warmup_steps)\n","\n","        return [optimizer], [scheduler]\n","\n","    #def compute_metrics():\n","        #looks not needed    "]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-11-22T13:20:54.235484Z","iopub.status.busy":"2022-11-22T13:20:54.234808Z","iopub.status.idle":"2022-11-22T13:20:54.247498Z","shell.execute_reply":"2022-11-22T13:20:54.246529Z","shell.execute_reply.started":"2022-11-22T13:20:54.235447Z"},"id":"I-qhi66IlBPX","trusted":true},"outputs":[],"source":["def train(conf):\n","    pl.seed_everything(conf.seed)\n","\n","    tokenizer = transformers.AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\", use_fast = conf.use_fast_tokenizer,\n","        additional_special_tokens = [\"<obj>\", \"<subj>\", \"<triplet>\", \"<head>\", \"</head>\", \"<tail>\", \"</tail>\", \"<MASK>\"])\n","    config = transformers.AutoConfig.from_pretrained(\"Babelscape/rebel-large\")\n","    model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\", config = config)\n","\n","    model.resize_token_embeddings(len(tokenizer))\n","\n","    pl_data_module = GetData(conf, tokenizer, model)\n","    pl_module = BaseModule(conf, config, tokenizer, model)\n","\n","    wandb_logger = WandbLogger(project = \"project/finetune\".split('/')[-1].replace('.py', ''), name = \"finetune\")\n","\n","    callbacks_store = []\n","\n","    if conf.early_stopping:\n","        callbacks_store.append(\n","            EarlyStopping(\n","                monitor=conf.monitor_var,\n","                mode=conf.monitor_var_mode,\n","                patience=conf.patience\n","            )\n","        )\n","\n","    # callbacks_store.append(\n","    #     ModelCheckpoint(\n","    #         monitor=conf.monitor_var,\n","    #         # monitor=None,\n","    #         dirpath=f'models/{conf.model_name}',\n","    #         save_top_k=conf.save_top_k,\n","    #         verbose=True,\n","    #         save_last=True,\n","    #         mode=conf.monitor_var_mode\n","    #     )\n","    # )\n","    callbacks_store.append(GenerateTextSamplesCallback(conf.samples_interval))\n","    callbacks_store.append(LearningRateMonitor(logging_interval='step'))\n","\n","    trainer = pl.Trainer(\n","        accelerator = \"gpu\",\n","        devices = conf.gpus,\n","        accumulate_grad_batches=conf.gradient_acc_steps,\n","        gradient_clip_val=conf.gradient_clip_value,\n","        #val_check_interval=conf.val_check_interval,\n","        max_epochs = 30,\n","        min_epochs = 5,\n","        callbacks=callbacks_store,\n","        max_steps=conf.max_steps,\n","        # max_steps=total_steps,\n","        precision=conf.precision,\n","        amp_level=conf.amp_level,\n","        logger=wandb_logger,\n","        #resume_from_checkpoint=conf.checkpoint_path,\n","        #limit_val_batches=conf.val_percent_check\n","    )\n","\n","    # module fit\n","    trainer.fit(pl_module, datamodule=pl_data_module)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["150a10a7600e4b7c81bae0d8f2074fb5","9d9f0941f63b4591b5dfb60b3f04b5eb","83b7f2b300774b57bc9a6d161d1ab361","369e2460fef14cf0b608ee6606b9a958","d31b8fc829d2490f8fe535b25665fd92","ff2b281cf4b2404eac573758fe98a514","82fa24fea072448fb581209b493f94af","908372568f5949778e4cdbcf4c6aef12","f366bf93bf994f53aad7570486246d0c","ba03625f26154813bc49b987ca0b0d82","574a438a44b04defa82bdd0424235a7f","8a2b47dcdc404fff8526e82712ba8ff1","e0bf74faf96c4644b2700af0e82bc3c2","f18929c9b22e42079fabcb0b81cf2555","baa2cb71da6a433da0dc27b698816fd1","cf33818563cc476983451e5b7155e32e","447b7a6abb4240a3b1493329300c84e5","d1af56d58aad44c5b7365f9b0bc53ba0","6390c74629db4f5abfb4f52c53dc6796","7232c60b8a544b7ab418cbbf3f90720e","531d5a86ef5644089d61291f92f8f639","62092f8a23944cb1a684839f61b8da4a","9b27b142777943eb98a4367fc253cc0e","2bb2f84d2127499b8ea398d27df9a5cb","5bc78be9136749578de53aed6185756e","6acdde5760364257ae70a57d5bbfbd7c","9c0784b612c742b395691825fa5971de","30f05c40bada49639644cfc5b646c0b3","0e8de96d82244c589947cffef206d26f","98f926c5ed954d818da6cf0025802907","17833a9dc3954892ade426c385e14006","eaaa70cbd0df4742a1406f3a3ebf4ec6","aa96ab2d2d3e4310bac4ac3d35c9d3e8","eb752942c88e4a2e9f0baf8f1ff88ada","e240a887867b44a4b77e3bd1d61da4d3","988be07f83dd49f5b1052d178acb265d","646b8524eb294d3aa85f90cf3b311ad5","762bfd3d61524e579db9791b0e5c822f","ffd67d27fa164c4a90b5a27695378525","676472d809674f0ea42e82ca6cc9dbec","e0982ae149b04402aafd0507b0250d78","966af13a165a4bdcab767330ad57ab79","c7225060efb14bfbad9003100ec47a63","898a55d0c4cb41879312a9baa217ab40","173944ae52604cfb8040a9727cf55367","f4d445e20cd64e40860d4ea1718ff4fd","e993024d78b6443bbe82684b314a1ce2","ec6264a5574042c3941aa1172d258935","f43018b3490141dfb36940ca58118883","3c69166d38a243d5bb92547fa524cf58","1211eb58256245cea32f856fd3a0a811","fd25fb346299476ab239f4b4a7653423","f7adc9ed848348c6acd422cb2a0b32e8","d1db33134e7944f1b1b887f7a66c5858","abfe68dc0ff2469aa4e1c226d82a6a57"]},"execution":{"iopub.execute_input":"2022-11-22T13:20:54.249474Z","iopub.status.busy":"2022-11-22T13:20:54.248919Z"},"id":"f1WDgwqOlD5U","outputId":"3f81075a-86c4-4521-bd64-cb74560d327d","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Global seed set to 0\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvalentinwerner\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"data":{"text/html":["wandb version 0.13.5 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.12.21"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>./wandb/run-20221128_155031-2lx42e86</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/valentinwerner/finetune/runs/2lx42e86\" target=\"_blank\">finetune</a></strong> to <a href=\"https://wandb.ai/valentinwerner/finetune\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Using 16bit native Automatic Mixed Precision (AMP)\n","GPU available: True (cuda), used: True\n","TPU available: False, using: 0 TPU cores\n","IPU available: False, using: 0 IPUs\n","HPU available: False, using: 0 HPUs\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n","\n","  | Name    | Type                         | Params\n","---------------------------------------------------------\n","0 | model   | BartForConditionalGeneration | 406 M \n","1 | loss_fn | CrossEntropyLoss             | 0     \n","---------------------------------------------------------\n","406 M     Trainable params\n","0         Non-trainable params\n","406 M     Total params\n","812.599   Total estimated model params size (MB)\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking: 0it [00:00, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1/1 [00:00<00:00, 16.40ba/s]\n"]},{"name":"stdout","output_type":"stream","text":["Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:01<00:00,  1.53it/s]RE Evaluation in *** BOUNDARIES *** mode\n","processed 16 sentences with 17 relations; found: 23 relations; correct: 0.\n","\tALL\t TP: 0;\tFP: 0;\tFN: 10\n","\t\t(m avg): precision: 0.00;\trecall: 0.00;\tf1: 0.00 (micro)\n","\t\t(M avg): precision: 0.00;\trecall: 0.00;\tf1: 0.00 (Macro)\n","\n","\tMakePublicStatement: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tAppeal: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tExpressIntentToCooperate: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tConsult: \tTP: 0;\tFP: 0;\tFN: 6;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tEngageInDiplomaticCooperation: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tEngageInMaterialCooperation: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tProvideAid: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tYield: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tInvestigate: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tDemand: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tDisapprove: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tReject: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tThreaten: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tExhibitMilitaryPosture: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tProtest: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tReduceRelations: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tCoerce: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tAssault: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tFight: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tEngageInUnconventialMassViolence: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","                                                                           \r"]},{"name":"stderr","output_type":"stream","text":["/home/werner/thesis_valentin/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('val_prec_micro', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n","  warning_cache.warn(\n","/home/werner/thesis_valentin/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('val_recall_micro', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n","  warning_cache.warn(\n","/home/werner/thesis_valentin/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('val_F1_micro', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n","  warning_cache.warn(\n","Parameter 'function'=<function GetData.preprocess_function at 0x7fe899635160> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n","100%|██████████| 3/3 [00:00<00:00,  6.16ba/s]\n"]},{"name":"stdout","output_type":"stream","text":["Epoch 0: 100%|██████████| 303/303 [05:17<00:00,  1.05s/it, loss=3.53, v_num=2e86]RE Evaluation in *** BOUNDARIES *** mode\n","processed 197 sentences with 206 relations; found: 193 relations; correct: 2.\n","\tALL\t TP: 2;\tFP: 118;\tFN: 96\n","\t\t(m avg): precision: 1.67;\trecall: 2.04;\tf1: 1.83 (micro)\n","\t\t(M avg): precision: 5.05;\trecall: 5.11;\tf1: 5.07 (Macro)\n","\n","\tMakePublicStatement: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tAppeal: \tTP: 0;\tFP: 0;\tFN: 6;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tExpressIntentToCooperate: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tConsult: \tTP: 1;\tFP: 96;\tFN: 45;\tprecision: 1.03;\trecall: 2.17;\tf1: 1.40;\t97\n","\tEngageInDiplomaticCooperation: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tEngageInMaterialCooperation: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tProvideAid: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tYield: \tTP: 0;\tFP: 11;\tFN: 13;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t11\n","\tInvestigate: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tDemand: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tDisapprove: \tTP: 0;\tFP: 2;\tFN: 12;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t2\n","\tReject: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tThreaten: \tTP: 1;\tFP: 0;\tFN: 0;\tprecision: 100.00;\trecall: 100.00;\tf1: 100.00;\t1\n","\tExhibitMilitaryPosture: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tProtest: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tReduceRelations: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tCoerce: \tTP: 0;\tFP: 9;\tFN: 15;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t9\n","\tAssault: \tTP: 0;\tFP: 0;\tFN: 2;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tFight: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tEngageInUnconventialMassViolence: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","Epoch 0: 100%|██████████| 303/303 [05:17<00:00,  1.05s/it, loss=3.53, v_num=2e86]"]},{"name":"stderr","output_type":"stream","text":["--- Logging error ---\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.8/logging/__init__.py\", line 1089, in emit\n","    self.flush()\n","  File \"/usr/lib/python3.8/logging/__init__.py\", line 1069, in flush\n","    self.stream.flush()\n","OSError: [Errno 122] Disk quota exceeded\n","Call stack:\n","  File \"/usr/lib/python3.8/threading.py\", line 890, in _bootstrap\n","    self._bootstrap_inner()\n","  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n","    self.run()\n","  File \"/home/werner/thesis_valentin/lib/python3.8/site-packages/wandb/filesync/upload_job.py\", line 62, in run\n","    success = self.push()\n","  File \"/home/werner/thesis_valentin/lib/python3.8/site-packages/wandb/filesync/upload_job.py\", line 138, in push\n","    logger.info(\"Uploaded file %s\", self.save_path)\n","Message: 'Uploaded file %s'\n","Arguments: ('/tmp/tmp86mjobe9wandb/15lib5hz-media/table/Triplets_146_904f08a6c74065be2738.table.json',)\n","--- Logging error ---\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.8/logging/__init__.py\", line 1089, in emit\n","    self.flush()\n","  File \"/usr/lib/python3.8/logging/__init__.py\", line 1069, in flush\n","    self.stream.flush()\n","OSError: [Errno 122] Disk quota exceeded\n","Call stack:\n","  File \"/usr/lib/python3.8/threading.py\", line 890, in _bootstrap\n","    self._bootstrap_inner()\n","  File \"/usr/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n","    self.run()\n","  File \"/home/werner/thesis_valentin/lib/python3.8/site-packages/wandb/sdk/service/streams.py\", line 40, in run\n","    self._target(**self._kwargs)\n","  File \"/home/werner/thesis_valentin/lib/python3.8/site-packages/wandb/sdk/internal/internal.py\", line 165, in wandb_internal\n","    logger.error(f\"Thread {thread.name}:\", exc_info=exc_info)\n","Message: 'Thread SenderThread:'\n","Arguments: ()\n","Thread SenderThread:\n","OSError: [Errno 122] Disk quota exceeded\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/home/werner/thesis_valentin/lib/python3.8/site-packages/wandb/sdk/internal/internal_util.py\", line 51, in run\n","    self._run()\n","  File \"/home/werner/thesis_valentin/lib/python3.8/site-packages/wandb/sdk/internal/internal_util.py\", line 102, in _run\n","    self._process(record)\n","  File \"/home/werner/thesis_valentin/lib/python3.8/site-packages/wandb/sdk/internal/internal.py\", line 310, in _process\n","    self._sm.send(record)\n","  File \"/home/werner/thesis_valentin/lib/python3.8/site-packages/wandb/sdk/internal/sender.py\", line 238, in send\n","    send_handler(record)\n","  File \"/home/werner/thesis_valentin/lib/python3.8/site-packages/wandb/sdk/internal/sender.py\", line 833, in send_summary\n","    self._update_summary()\n","  File \"/home/werner/thesis_valentin/lib/python3.8/site-packages/wandb/sdk/internal/sender.py\", line 846, in _update_summary\n","    f.write(json_summary)\n","OSError: [Errno 122] Disk quota exceeded\n","wandb: ERROR Internal wandb error: file data was not synced\n"]},{"ename":"OSError","evalue":"[Errno 122] Disk quota exceeded","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","Cell \u001b[0;32mIn [11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(conf)\n\u001b[1;32m      2\u001b[0m \u001b[39m#api key is fcfb005aa20d2c3af3389e0a2a6d58a829bfd2ee\u001b[39;00m\n","Cell \u001b[0;32mIn [10], line 60\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(conf)\u001b[0m\n\u001b[1;32m     41\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39mTrainer(\n\u001b[1;32m     42\u001b[0m     accelerator \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mgpu\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     43\u001b[0m     devices \u001b[39m=\u001b[39m conf\u001b[39m.\u001b[39mgpus,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[39m#limit_val_batches=conf.val_percent_check\u001b[39;00m\n\u001b[1;32m     57\u001b[0m )\n\u001b[1;32m     59\u001b[0m \u001b[39m# module fit\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(pl_module, datamodule\u001b[39m=\u001b[39;49mpl_data_module)\n","File \u001b[0;32m~/thesis_valentin/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:582\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`Trainer.fit()` requires a `LightningModule`, got: \u001b[39m\u001b[39m{\u001b[39;00mmodel\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    581\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39m_lightning_module \u001b[39m=\u001b[39m model\n\u001b[0;32m--> 582\u001b[0m call\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    583\u001b[0m     \u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    584\u001b[0m )\n","File \u001b[0;32m~/thesis_valentin/lib/python3.8/site-packages/pytorch_lightning/trainer/call.py:38\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39mtrainer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     37\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     40\u001b[0m \u001b[39mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     41\u001b[0m     trainer\u001b[39m.\u001b[39m_call_teardown_hook()\n","File \u001b[0;32m~/thesis_valentin/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:624\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    617\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    618\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39m_set_ckpt_path(\n\u001b[1;32m    619\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mfn,\n\u001b[1;32m    620\u001b[0m     ckpt_path,  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    621\u001b[0m     model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m    622\u001b[0m     model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    623\u001b[0m )\n\u001b[0;32m--> 624\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    626\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    627\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n","File \u001b[0;32m~/thesis_valentin/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1061\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   1057\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   1059\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> 1061\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   1063\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   1064\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n","File \u001b[0;32m~/thesis_valentin/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1140\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1138\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   1139\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> 1140\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n","File \u001b[0;32m~/thesis_valentin/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1163\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit_loop\u001b[39m.\u001b[39mtrainer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\n\u001b[1;32m   1162\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mset_detect_anomaly(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1163\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit_loop\u001b[39m.\u001b[39;49mrun()\n","File \u001b[0;32m~/thesis_valentin/lib/python3.8/site-packages/pytorch_lightning/loops/loop.py:200\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    199\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madvance(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mon_advance_end()\n\u001b[1;32m    201\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n","File \u001b[0;32m~/thesis_valentin/lib/python3.8/site-packages/pytorch_lightning/loops/fit_loop.py:295\u001b[0m, in \u001b[0;36mFitLoop.on_advance_end\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepoch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    294\u001b[0m \u001b[39m# call train epoch end hooks\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_callback_hooks(\u001b[39m\"\u001b[39;49m\u001b[39mon_train_epoch_end\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    296\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_lightning_module_hook(\u001b[39m\"\u001b[39m\u001b[39mon_train_epoch_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    298\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_logger_connector\u001b[39m.\u001b[39mon_epoch_end()\n","File \u001b[0;32m~/thesis_valentin/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1343\u001b[0m, in \u001b[0;36mTrainer._call_callback_hooks\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1341\u001b[0m     \u001b[39mif\u001b[39;00m callable(fn):\n\u001b[1;32m   1342\u001b[0m         \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Callback]\u001b[39m\u001b[39m{\u001b[39;00mcallback\u001b[39m.\u001b[39mstate_key\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> 1343\u001b[0m             fn(\u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1345\u001b[0m \u001b[39mif\u001b[39;00m pl_module:\n\u001b[1;32m   1346\u001b[0m     \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   1347\u001b[0m     pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n","File \u001b[0;32m~/thesis_valentin/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:312\u001b[0m, in \u001b[0;36mModelCheckpoint.on_train_epoch_end\u001b[0;34m(self, trainer, pl_module)\u001b[0m\n\u001b[1;32m    310\u001b[0m monitor_candidates \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_monitor_candidates(trainer)\n\u001b[1;32m    311\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_every_n_epochs \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m (trainer\u001b[39m.\u001b[39mcurrent_epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_every_n_epochs \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 312\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_topk_checkpoint(trainer, monitor_candidates)\n\u001b[1;32m    313\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_last_checkpoint(trainer, monitor_candidates)\n","File \u001b[0;32m~/thesis_valentin/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:371\u001b[0m, in \u001b[0;36mModelCheckpoint._save_topk_checkpoint\u001b[0;34m(self, trainer, monitor_candidates)\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_monitor_checkpoint(trainer, monitor_candidates)\n\u001b[1;32m    370\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 371\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_none_monitor_checkpoint(trainer, monitor_candidates)\n","File \u001b[0;32m~/thesis_valentin/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:660\u001b[0m, in \u001b[0;36mModelCheckpoint._save_none_monitor_checkpoint\u001b[0;34m(self, trainer, monitor_candidates)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[39m# set the best model path before saving because it will be part of the state.\u001b[39;00m\n\u001b[1;32m    659\u001b[0m previous, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_model_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbest_model_path, filepath\n\u001b[0;32m--> 660\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_save_checkpoint(trainer, filepath)\n\u001b[1;32m    661\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave_top_k \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m previous \u001b[39mand\u001b[39;00m previous \u001b[39m!=\u001b[39m filepath:\n\u001b[1;32m    662\u001b[0m     trainer\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mremove_checkpoint(previous)\n","File \u001b[0;32m~/thesis_valentin/lib/python3.8/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:374\u001b[0m, in \u001b[0;36mModelCheckpoint._save_checkpoint\u001b[0;34m(self, trainer, filepath)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_save_checkpoint\u001b[39m(\u001b[39mself\u001b[39m, trainer: \u001b[39m\"\u001b[39m\u001b[39mpl.Trainer\u001b[39m\u001b[39m\"\u001b[39m, filepath: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 374\u001b[0m     trainer\u001b[39m.\u001b[39;49msave_checkpoint(filepath, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msave_weights_only)\n\u001b[1;32m    376\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_last_global_step_saved \u001b[39m=\u001b[39m trainer\u001b[39m.\u001b[39mglobal_step\n\u001b[1;32m    378\u001b[0m     \u001b[39m# notify loggers\u001b[39;00m\n","File \u001b[0;32m~/thesis_valentin/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:1904\u001b[0m, in \u001b[0;36mTrainer.save_checkpoint\u001b[0;34m(self, filepath, weights_only, storage_options)\u001b[0m\n\u001b[1;32m   1899\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1900\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m   1901\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSaving a checkpoint is only possible if a model is attached to the Trainer. Did you call\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1902\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m `Trainer.save_checkpoint()` before calling `Trainer.\u001b[39m\u001b[39m{\u001b[39m\u001b[39mfit,validate,test,predict}`?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1903\u001b[0m     )\n\u001b[0;32m-> 1904\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_checkpoint_connector\u001b[39m.\u001b[39;49msave_checkpoint(filepath, weights_only\u001b[39m=\u001b[39;49mweights_only, storage_options\u001b[39m=\u001b[39;49mstorage_options)\n","File \u001b[0;32m~/thesis_valentin/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/checkpoint_connector.py:540\u001b[0m, in \u001b[0;36mCheckpointConnector.save_checkpoint\u001b[0;34m(self, filepath, weights_only, storage_options)\u001b[0m\n\u001b[1;32m    532\u001b[0m \u001b[39m\"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\u001b[39;00m\n\u001b[1;32m    533\u001b[0m \n\u001b[1;32m    534\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[39m    storage_options: parameter for how to save to storage, passed to ``CheckpointIO`` plugin\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    539\u001b[0m _checkpoint \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdump_checkpoint(weights_only)\n\u001b[0;32m--> 540\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mstrategy\u001b[39m.\u001b[39;49msave_checkpoint(_checkpoint, filepath, storage_options\u001b[39m=\u001b[39;49mstorage_options)\n","File \u001b[0;32m~/thesis_valentin/lib/python3.8/site-packages/pytorch_lightning/strategies/strategy.py:466\u001b[0m, in \u001b[0;36mStrategy.save_checkpoint\u001b[0;34m(self, checkpoint, filepath, storage_options)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39m\"\"\"Save model/training states as a checkpoint file through state-dump and file-write.\u001b[39;00m\n\u001b[1;32m    459\u001b[0m \n\u001b[1;32m    460\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[39m    storage_options: parameter for how to save to storage, passed to ``CheckpointIO`` plugin\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_global_zero:\n\u001b[0;32m--> 466\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcheckpoint_io\u001b[39m.\u001b[39;49msave_checkpoint(checkpoint, filepath, storage_options\u001b[39m=\u001b[39;49mstorage_options)\n","File \u001b[0;32m~/thesis_valentin/lib/python3.8/site-packages/lightning_lite/plugins/io/torch_io.py:54\u001b[0m, in \u001b[0;36mTorchCheckpointIO.save_checkpoint\u001b[0;34m(self, checkpoint, path, storage_options)\u001b[0m\n\u001b[1;32m     51\u001b[0m fs\u001b[39m.\u001b[39mmakedirs(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mdirname(path), exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[39m# write the checkpoint dictionary on the file\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     _atomic_save(checkpoint, path)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m     56\u001b[0m     \u001b[39m# todo: is this try catch necessary still?\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[39m# https://github.com/Lightning-AI/lightning/pull/431\u001b[39;00m\n\u001b[1;32m     58\u001b[0m     \u001b[39m# TODO(lite): Lite doesn't support hyperparameters in the checkpoint, so this should be refactored\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     key \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhyper_parameters\u001b[39m\u001b[39m\"\u001b[39m\n","File \u001b[0;32m~/thesis_valentin/lib/python3.8/site-packages/lightning_lite/utilities/cloud_io.py:69\u001b[0m, in \u001b[0;36m_atomic_save\u001b[0;34m(checkpoint, filepath)\u001b[0m\n\u001b[1;32m     67\u001b[0m torch\u001b[39m.\u001b[39msave(checkpoint, bytesbuffer)\n\u001b[1;32m     68\u001b[0m \u001b[39mwith\u001b[39;00m fsspec\u001b[39m.\u001b[39mopen(filepath, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 69\u001b[0m     f\u001b[39m.\u001b[39;49mwrite(bytesbuffer\u001b[39m.\u001b[39;49mgetvalue())\n","File \u001b[0;32m~/thesis_valentin/lib/python3.8/site-packages/fsspec/implementations/local.py:340\u001b[0m, in \u001b[0;36mLocalFileOpener.write\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrite\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 340\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf\u001b[39m.\u001b[39;49mwrite(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n","\u001b[0;31mOSError\u001b[0m: [Errno 122] Disk quota exceeded"]}],"source":["train(conf)\n","#api key is fcfb005aa20d2c3af3389e0a2a6d58a829bfd2ee"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3.8.10 ('thesis_valentin': venv)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"vscode":{"interpreter":{"hash":"a1350fe81f24607af7015099983099ac829ebf1f4acb969c2cf14b7230b10f2d"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"0e8de96d82244c589947cffef206d26f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1211eb58256245cea32f856fd3a0a811":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"150a10a7600e4b7c81bae0d8f2074fb5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9d9f0941f63b4591b5dfb60b3f04b5eb","IPY_MODEL_83b7f2b300774b57bc9a6d161d1ab361","IPY_MODEL_369e2460fef14cf0b608ee6606b9a958"],"layout":"IPY_MODEL_d31b8fc829d2490f8fe535b25665fd92"}},"173944ae52604cfb8040a9727cf55367":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f4d445e20cd64e40860d4ea1718ff4fd","IPY_MODEL_e993024d78b6443bbe82684b314a1ce2","IPY_MODEL_ec6264a5574042c3941aa1172d258935"],"layout":"IPY_MODEL_f43018b3490141dfb36940ca58118883"}},"17833a9dc3954892ade426c385e14006":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2bb2f84d2127499b8ea398d27df9a5cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30f05c40bada49639644cfc5b646c0b3","placeholder":"​","style":"IPY_MODEL_0e8de96d82244c589947cffef206d26f","value":"  0%"}},"30f05c40bada49639644cfc5b646c0b3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"369e2460fef14cf0b608ee6606b9a958":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba03625f26154813bc49b987ca0b0d82","placeholder":"​","style":"IPY_MODEL_574a438a44b04defa82bdd0424235a7f","value":" 2/2 [00:04&lt;00:00,  2.24s/it]"}},"3c69166d38a243d5bb92547fa524cf58":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"447b7a6abb4240a3b1493329300c84e5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"531d5a86ef5644089d61291f92f8f639":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"574a438a44b04defa82bdd0424235a7f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5bc78be9136749578de53aed6185756e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_98f926c5ed954d818da6cf0025802907","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_17833a9dc3954892ade426c385e14006","value":0}},"62092f8a23944cb1a684839f61b8da4a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6390c74629db4f5abfb4f52c53dc6796":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"646b8524eb294d3aa85f90cf3b311ad5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7225060efb14bfbad9003100ec47a63","placeholder":"​","style":"IPY_MODEL_898a55d0c4cb41879312a9baa217ab40","value":" 36/36 [00:51&lt;00:00,  1.43s/it, loss=9.19, v_num=jbqs]"}},"676472d809674f0ea42e82ca6cc9dbec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6acdde5760364257ae70a57d5bbfbd7c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eaaa70cbd0df4742a1406f3a3ebf4ec6","placeholder":"​","style":"IPY_MODEL_aa96ab2d2d3e4310bac4ac3d35c9d3e8","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"7232c60b8a544b7ab418cbbf3f90720e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"762bfd3d61524e579db9791b0e5c822f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"82fa24fea072448fb581209b493f94af":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"83b7f2b300774b57bc9a6d161d1ab361":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_908372568f5949778e4cdbcf4c6aef12","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f366bf93bf994f53aad7570486246d0c","value":2}},"898a55d0c4cb41879312a9baa217ab40":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8a2b47dcdc404fff8526e82712ba8ff1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e0bf74faf96c4644b2700af0e82bc3c2","IPY_MODEL_f18929c9b22e42079fabcb0b81cf2555","IPY_MODEL_baa2cb71da6a433da0dc27b698816fd1"],"layout":"IPY_MODEL_cf33818563cc476983451e5b7155e32e"}},"908372568f5949778e4cdbcf4c6aef12":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"966af13a165a4bdcab767330ad57ab79":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"988be07f83dd49f5b1052d178acb265d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0982ae149b04402aafd0507b0250d78","max":36,"min":0,"orientation":"horizontal","style":"IPY_MODEL_966af13a165a4bdcab767330ad57ab79","value":36}},"98f926c5ed954d818da6cf0025802907":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b27b142777943eb98a4367fc253cc0e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2bb2f84d2127499b8ea398d27df9a5cb","IPY_MODEL_5bc78be9136749578de53aed6185756e","IPY_MODEL_6acdde5760364257ae70a57d5bbfbd7c"],"layout":"IPY_MODEL_9c0784b612c742b395691825fa5971de"}},"9c0784b612c742b395691825fa5971de":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d9f0941f63b4591b5dfb60b3f04b5eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel"}}