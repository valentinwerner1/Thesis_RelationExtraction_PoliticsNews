{"metadata":{"colab":{"provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"150a10a7600e4b7c81bae0d8f2074fb5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9d9f0941f63b4591b5dfb60b3f04b5eb","IPY_MODEL_83b7f2b300774b57bc9a6d161d1ab361","IPY_MODEL_369e2460fef14cf0b608ee6606b9a958"],"layout":"IPY_MODEL_d31b8fc829d2490f8fe535b25665fd92"}},"9d9f0941f63b4591b5dfb60b3f04b5eb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff2b281cf4b2404eac573758fe98a514","placeholder":"​","style":"IPY_MODEL_82fa24fea072448fb581209b493f94af","value":"Sanity Checking DataLoader 0: 100%"}},"83b7f2b300774b57bc9a6d161d1ab361":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_908372568f5949778e4cdbcf4c6aef12","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f366bf93bf994f53aad7570486246d0c","value":2}},"369e2460fef14cf0b608ee6606b9a958":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba03625f26154813bc49b987ca0b0d82","placeholder":"​","style":"IPY_MODEL_574a438a44b04defa82bdd0424235a7f","value":" 2/2 [00:04&lt;00:00,  2.24s/it]"}},"d31b8fc829d2490f8fe535b25665fd92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"ff2b281cf4b2404eac573758fe98a514":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82fa24fea072448fb581209b493f94af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"908372568f5949778e4cdbcf4c6aef12":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f366bf93bf994f53aad7570486246d0c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ba03625f26154813bc49b987ca0b0d82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"574a438a44b04defa82bdd0424235a7f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8a2b47dcdc404fff8526e82712ba8ff1":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e0bf74faf96c4644b2700af0e82bc3c2","IPY_MODEL_f18929c9b22e42079fabcb0b81cf2555","IPY_MODEL_baa2cb71da6a433da0dc27b698816fd1"],"layout":"IPY_MODEL_cf33818563cc476983451e5b7155e32e"}},"e0bf74faf96c4644b2700af0e82bc3c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_447b7a6abb4240a3b1493329300c84e5","placeholder":"​","style":"IPY_MODEL_d1af56d58aad44c5b7365f9b0bc53ba0","value":"  0%"}},"f18929c9b22e42079fabcb0b81cf2555":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_6390c74629db4f5abfb4f52c53dc6796","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7232c60b8a544b7ab418cbbf3f90720e","value":0}},"baa2cb71da6a433da0dc27b698816fd1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_531d5a86ef5644089d61291f92f8f639","placeholder":"​","style":"IPY_MODEL_62092f8a23944cb1a684839f61b8da4a","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"cf33818563cc476983451e5b7155e32e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"447b7a6abb4240a3b1493329300c84e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1af56d58aad44c5b7365f9b0bc53ba0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6390c74629db4f5abfb4f52c53dc6796":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7232c60b8a544b7ab418cbbf3f90720e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"531d5a86ef5644089d61291f92f8f639":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"62092f8a23944cb1a684839f61b8da4a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b27b142777943eb98a4367fc253cc0e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2bb2f84d2127499b8ea398d27df9a5cb","IPY_MODEL_5bc78be9136749578de53aed6185756e","IPY_MODEL_6acdde5760364257ae70a57d5bbfbd7c"],"layout":"IPY_MODEL_9c0784b612c742b395691825fa5971de"}},"2bb2f84d2127499b8ea398d27df9a5cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30f05c40bada49639644cfc5b646c0b3","placeholder":"​","style":"IPY_MODEL_0e8de96d82244c589947cffef206d26f","value":"  0%"}},"5bc78be9136749578de53aed6185756e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_98f926c5ed954d818da6cf0025802907","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_17833a9dc3954892ade426c385e14006","value":0}},"6acdde5760364257ae70a57d5bbfbd7c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eaaa70cbd0df4742a1406f3a3ebf4ec6","placeholder":"​","style":"IPY_MODEL_aa96ab2d2d3e4310bac4ac3d35c9d3e8","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"9c0784b612c742b395691825fa5971de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"30f05c40bada49639644cfc5b646c0b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0e8de96d82244c589947cffef206d26f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"98f926c5ed954d818da6cf0025802907":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17833a9dc3954892ade426c385e14006":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eaaa70cbd0df4742a1406f3a3ebf4ec6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"aa96ab2d2d3e4310bac4ac3d35c9d3e8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"eb752942c88e4a2e9f0baf8f1ff88ada":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e240a887867b44a4b77e3bd1d61da4d3","IPY_MODEL_988be07f83dd49f5b1052d178acb265d","IPY_MODEL_646b8524eb294d3aa85f90cf3b311ad5"],"layout":"IPY_MODEL_762bfd3d61524e579db9791b0e5c822f"}},"e240a887867b44a4b77e3bd1d61da4d3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ffd67d27fa164c4a90b5a27695378525","placeholder":"​","style":"IPY_MODEL_676472d809674f0ea42e82ca6cc9dbec","value":"Epoch 0: 100%"}},"988be07f83dd49f5b1052d178acb265d":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0982ae149b04402aafd0507b0250d78","max":36,"min":0,"orientation":"horizontal","style":"IPY_MODEL_966af13a165a4bdcab767330ad57ab79","value":36}},"646b8524eb294d3aa85f90cf3b311ad5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7225060efb14bfbad9003100ec47a63","placeholder":"​","style":"IPY_MODEL_898a55d0c4cb41879312a9baa217ab40","value":" 36/36 [00:51&lt;00:00,  1.43s/it, loss=9.19, v_num=jbqs]"}},"762bfd3d61524e579db9791b0e5c822f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"ffd67d27fa164c4a90b5a27695378525":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"676472d809674f0ea42e82ca6cc9dbec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e0982ae149b04402aafd0507b0250d78":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"966af13a165a4bdcab767330ad57ab79":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c7225060efb14bfbad9003100ec47a63":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"898a55d0c4cb41879312a9baa217ab40":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"173944ae52604cfb8040a9727cf55367":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f4d445e20cd64e40860d4ea1718ff4fd","IPY_MODEL_e993024d78b6443bbe82684b314a1ce2","IPY_MODEL_ec6264a5574042c3941aa1172d258935"],"layout":"IPY_MODEL_f43018b3490141dfb36940ca58118883"}},"f4d445e20cd64e40860d4ea1718ff4fd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c69166d38a243d5bb92547fa524cf58","placeholder":"​","style":"IPY_MODEL_1211eb58256245cea32f856fd3a0a811","value":"Validation DataLoader 0: 100%"}},"e993024d78b6443bbe82684b314a1ce2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd25fb346299476ab239f4b4a7653423","max":15,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f7adc9ed848348c6acd422cb2a0b32e8","value":15}},"ec6264a5574042c3941aa1172d258935":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1db33134e7944f1b1b887f7a66c5858","placeholder":"​","style":"IPY_MODEL_abfe68dc0ff2469aa4e1c226d82a6a57","value":" 15/15 [00:15&lt;00:00,  1.05s/it]"}},"f43018b3490141dfb36940ca58118883":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"3c69166d38a243d5bb92547fa524cf58":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1211eb58256245cea32f856fd3a0a811":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fd25fb346299476ab239f4b4a7653423":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f7adc9ed848348c6acd422cb2a0b32e8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d1db33134e7944f1b1b887f7a66c5858":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abfe68dc0ff2469aa4e1c226d82a6a57":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! git clone https://github.com/valentinwerner1/Thesis_RelationExtraction_PoliticsNews.git","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i2Llr6fegsAk","outputId":"34617256-5947-46b8-abee-31bee7b7c68c","execution":{"iopub.status.busy":"2022-10-25T16:19:37.453545Z","iopub.execute_input":"2022-10-25T16:19:37.454555Z","iopub.status.idle":"2022-10-25T16:19:39.520037Z","shell.execute_reply.started":"2022-10-25T16:19:37.454451Z","shell.execute_reply":"2022-10-25T16:19:39.518733Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Cloning into 'Thesis_RelationExtraction_PoliticsNews'...\nremote: Enumerating objects: 219, done.\u001b[K\nremote: Counting objects: 100% (219/219), done.\u001b[K\nremote: Compressing objects: 100% (136/136), done.\u001b[K\nremote: Total 219 (delta 91), reused 200 (delta 75), pack-reused 0\u001b[K\nReceiving objects: 100% (219/219), 2.66 MiB | 12.80 MiB/s, done.\nResolving deltas: 100% (91/91), done.\n","output_type":"stream"}]},{"cell_type":"code","source":"!cd Thesis_RelationExtraction_PoliticsNews && git checkout dev","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q4MafT2BhDB3","outputId":"7aadeeac-3c78-4d61-885d-207fed48dbf0","execution":{"iopub.status.busy":"2022-10-25T16:19:39.522620Z","iopub.execute_input":"2022-10-25T16:19:39.522946Z","iopub.status.idle":"2022-10-25T16:19:40.751125Z","shell.execute_reply.started":"2022-10-25T16:19:39.522896Z","shell.execute_reply":"2022-10-25T16:19:40.749304Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Branch 'dev' set up to track remote branch 'dev' from 'origin'.\nSwitched to a new branch 'dev'\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers\n!pip install pytorch_lightning\n!pip install datasets\n!pip install wandb","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4b-3jSUmh-HQ","outputId":"3f0df5a1-46d3-49ba-f626-33d707b183c5","execution":{"iopub.status.busy":"2022-10-25T16:19:40.753374Z","iopub.execute_input":"2022-10-25T16:19:40.753971Z","iopub.status.idle":"2022-10-25T16:20:22.352340Z","shell.execute_reply.started":"2022-10-25T16:19:40.753918Z","shell.execute_reply":"2022-10-25T16:20:22.351078Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.13.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.9.24)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: pytorch_lightning in /opt/conda/lib/python3.7/site-packages (1.7.7)\nRequirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (6.0)\nRequirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (21.3)\nRequirement already satisfied: tensorboard>=2.9.1 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (2.10.1)\nRequirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (0.10.0)\nRequirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (1.21.6)\nRequirement already satisfied: pyDeprecate>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (0.3.2)\nRequirement already satisfied: torch>=1.9.* in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (1.11.0)\nRequirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (4.1.1)\nRequirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (4.64.0)\nRequirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (2022.8.2)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.28.1)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.7/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.8.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=17.0->pytorch_lightning) (3.0.9)\nRequirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (0.37.1)\nRequirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (2.2.2)\nRequirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (0.4.6)\nRequirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (0.6.1)\nRequirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (1.8.1)\nRequirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (59.8.0)\nRequirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (1.35.0)\nRequirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (1.43.0)\nRequirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (3.3.7)\nRequirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (0.15.0)\nRequirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (3.19.4)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py>=0.4->tensorboard>=2.9.1->pytorch_lightning) (1.15.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.4.0)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.7.2)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (0.13.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (4.0.2)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (6.0.2)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.3.0)\nRequirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (4.8)\nRequirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (4.2.4)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (0.2.7)\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch_lightning) (1.3.1)\nRequirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch_lightning) (4.13.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.3)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2022.9.24)\nRequirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->pytorch_lightning) (2.1.1)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch_lightning) (3.8.0)\nRequirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (0.4.8)\nRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch_lightning) (3.2.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (2.1.0)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2022.8.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.21.6)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.13)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.28.1)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.64.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (5.0.0)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.10.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.13.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.5.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.1)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (2.1.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.1.1)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (21.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2022.9.24)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.3)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.8.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2022.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: wandb in /opt/conda/lib/python3.7/site-packages (0.12.21)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.9.10)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.7/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.9.1)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.28.1)\nRequirement already satisfied: protobuf<4.0dev,>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.19.4)\nRequirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.0.9)\nRequirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (8.0.4)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from wandb) (6.0)\nRequirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.1.27)\nRequirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.3)\nRequirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.15.0)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from wandb) (59.8.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from Click!=8.0.0,>=7.0->wandb) (4.13.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.1.1)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->Click!=8.0.0,>=7.0->wandb) (3.8.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import transformers \nimport pandas as pd\nimport numpy as np\nimport json\nimport os\n\n\nfrom typing import Any, Union, List, Optional\n\nimport torch\nfrom torch.utils.data import DataLoader\nfrom torch.optim import AdamW\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\nfrom pytorch_lightning.callbacks import LearningRateMonitor\nimport datasets\nfrom transformers import (\n    AutoConfig,\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    DataCollatorForSeq2Seq,\n    default_data_collator,\n    set_seed,\n)\n\nfrom torch.optim import lr_scheduler\nimport wandb\nfrom pytorch_lightning.loggers.wandb import WandbLogger","metadata":{"id":"I7G0TrPChwqV","execution":{"iopub.status.busy":"2022-10-25T16:20:22.356798Z","iopub.execute_input":"2022-10-25T16:20:22.357118Z","iopub.status.idle":"2022-10-25T16:20:33.456332Z","shell.execute_reply.started":"2022-10-25T16:20:22.357087Z","shell.execute_reply":"2022-10-25T16:20:33.455273Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"read = pd.read_json(\"Thesis_RelationExtraction_PoliticsNews/data_src/raw/out1.json\", lines = True)\ndf = read[read.answer == \"accept\"]\ndf.head()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":337},"id":"aV3eJje6h9Ms","outputId":"f94d36d7-c5cf-49ba-90a4-7da89ea77ba8","execution":{"iopub.status.busy":"2022-10-25T16:20:33.457841Z","iopub.execute_input":"2022-10-25T16:20:33.458626Z","iopub.status.idle":"2022-10-25T16:20:33.571707Z","shell.execute_reply.started":"2022-10-25T16:20:33.458585Z","shell.execute_reply":"2022-10-25T16:20:33.570580Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                                                text  article_id  sentence_id  \\\n0  Still, the restrictions imposed on the project...         183        18315   \n1  Days later, a released fighter and others, mos...          13         1317   \n2  Are countries allowed to turn away asylum seek...          11         1111   \n3  One man, 28, spoke to The Washington Post on t...         167         1673   \n4  A U.S. defense official said Monday that two N...         174         1749   \n\n   _input_hash  _task_hash  _is_binary  \\\n0  -1595512685   351917106       False   \n1    399796307  1377411962       False   \n2  -1470052778  -160075520       False   \n3    -61112669 -1028177687       False   \n4  -2118794475 -1006559555       False   \n\n                                               spans  \\\n0  [{'start': 35, 'end': 46, 'token_start': 6, 't...   \n1  [{'start': 12, 'end': 30, 'token_start': 3, 't...   \n2  [{'start': 4, 'end': 13, 'token_start': 1, 'to...   \n3  [{'start': 0, 'end': 11, 'token_start': 0, 'to...   \n4  [{'start': 0, 'end': 23, 'token_start': 0, 'to...   \n\n                                              tokens   _view_id  \\\n0  [{'text': 'Still', 'start': 0, 'end': 5, 'id':...  relations   \n1  [{'text': 'Days', 'start': 0, 'end': 4, 'id': ...  relations   \n2  [{'text': 'Are', 'start': 0, 'end': 3, 'id': 0...  relations   \n3  [{'text': 'One', 'start': 0, 'end': 3, 'id': 0...  relations   \n4  [{'text': 'A', 'start': 0, 'end': 1, 'id': 0, ...  relations   \n\n                                           relations  answer  _timestamp  \n0  [{'head': 18, 'child': 14, 'head_span': {'star...  accept  1666082270  \n1  [{'head': 20, 'child': 5, 'head_span': {'start...  accept  1666082881  \n2  [{'head': 1, 'child': 7, 'head_span': {'start'...  accept  1666082907  \n3  [{'head': 3, 'child': 9, 'head_span': {'start'...  accept  1666082929  \n4                                                 []  accept  1666082966  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>article_id</th>\n      <th>sentence_id</th>\n      <th>_input_hash</th>\n      <th>_task_hash</th>\n      <th>_is_binary</th>\n      <th>spans</th>\n      <th>tokens</th>\n      <th>_view_id</th>\n      <th>relations</th>\n      <th>answer</th>\n      <th>_timestamp</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Still, the restrictions imposed on the project...</td>\n      <td>183</td>\n      <td>18315</td>\n      <td>-1595512685</td>\n      <td>351917106</td>\n      <td>False</td>\n      <td>[{'start': 35, 'end': 46, 'token_start': 6, 't...</td>\n      <td>[{'text': 'Still', 'start': 0, 'end': 5, 'id':...</td>\n      <td>relations</td>\n      <td>[{'head': 18, 'child': 14, 'head_span': {'star...</td>\n      <td>accept</td>\n      <td>1666082270</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Days later, a released fighter and others, mos...</td>\n      <td>13</td>\n      <td>1317</td>\n      <td>399796307</td>\n      <td>1377411962</td>\n      <td>False</td>\n      <td>[{'start': 12, 'end': 30, 'token_start': 3, 't...</td>\n      <td>[{'text': 'Days', 'start': 0, 'end': 4, 'id': ...</td>\n      <td>relations</td>\n      <td>[{'head': 20, 'child': 5, 'head_span': {'start...</td>\n      <td>accept</td>\n      <td>1666082881</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Are countries allowed to turn away asylum seek...</td>\n      <td>11</td>\n      <td>1111</td>\n      <td>-1470052778</td>\n      <td>-160075520</td>\n      <td>False</td>\n      <td>[{'start': 4, 'end': 13, 'token_start': 1, 'to...</td>\n      <td>[{'text': 'Are', 'start': 0, 'end': 3, 'id': 0...</td>\n      <td>relations</td>\n      <td>[{'head': 1, 'child': 7, 'head_span': {'start'...</td>\n      <td>accept</td>\n      <td>1666082907</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>One man, 28, spoke to The Washington Post on t...</td>\n      <td>167</td>\n      <td>1673</td>\n      <td>-61112669</td>\n      <td>-1028177687</td>\n      <td>False</td>\n      <td>[{'start': 0, 'end': 11, 'token_start': 0, 'to...</td>\n      <td>[{'text': 'One', 'start': 0, 'end': 3, 'id': 0...</td>\n      <td>relations</td>\n      <td>[{'head': 3, 'child': 9, 'head_span': {'start'...</td>\n      <td>accept</td>\n      <td>1666082929</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>A U.S. defense official said Monday that two N...</td>\n      <td>174</td>\n      <td>1749</td>\n      <td>-2118794475</td>\n      <td>-1006559555</td>\n      <td>False</td>\n      <td>[{'start': 0, 'end': 23, 'token_start': 0, 'to...</td>\n      <td>[{'text': 'A', 'start': 0, 'end': 1, 'id': 0, ...</td>\n      <td>relations</td>\n      <td>[]</td>\n      <td>accept</td>\n      <td>1666082966</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#fix sentence_id mistake\ndf = df.astype({\"sentence_id\":str,\"article_id\":str}) \n\ndef fix_id(row):\n    return row.sentence_id[:len(row.article_id)] + \"_\" + row.sentence_id[len(row.article_id):]\n\ndf[\"sentence_id\"] = df.apply(fix_id, axis = 1)\n\n#add annotator column in case more annotators will be used\ndf[\"annotator\"] = 0","metadata":{"id":"3bH0NG1zkfUZ","execution":{"iopub.status.busy":"2022-10-25T16:20:33.573276Z","iopub.execute_input":"2022-10-25T16:20:33.573758Z","iopub.status.idle":"2022-10-25T16:20:33.591980Z","shell.execute_reply.started":"2022-10-25T16:20:33.573717Z","shell.execute_reply":"2022-10-25T16:20:33.591141Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#for EDA\ndf[\"relations_count\"] = df.relations.apply(lambda x: len(x))","metadata":{"id":"uZHFEiugkrGK","execution":{"iopub.status.busy":"2022-10-25T16:20:33.593586Z","iopub.execute_input":"2022-10-25T16:20:33.593971Z","iopub.status.idle":"2022-10-25T16:20:33.606813Z","shell.execute_reply.started":"2022-10-25T16:20:33.593915Z","shell.execute_reply":"2022-10-25T16:20:33.605726Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"#prune df\ndf = df.drop(columns = [\"_input_hash\",\"_task_hash\",\"_is_binary\",\"tokens\",\"_view_id\",\"answer\",\"_timestamp\"])\ndf = df.reset_index().drop(columns = [\"index\"])","metadata":{"id":"6ZPJQ0PskslP","execution":{"iopub.status.busy":"2022-10-25T16:20:33.608536Z","iopub.execute_input":"2022-10-25T16:20:33.608930Z","iopub.status.idle":"2022-10-25T16:20:33.621777Z","shell.execute_reply.started":"2022-10-25T16:20:33.608894Z","shell.execute_reply":"2022-10-25T16:20:33.620810Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#get data into needed format and create new DataFrame\nrelations = []\nfor index,row in df.iterrows():\n    for rel in row.relations:\n        relations.append(rel['label'])\n\nrelations = list(set(relations))\nrelations","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0kZi5qlVkuFu","outputId":"afeeaedf-ac3e-41ee-e4f9-00d2c3ba3d1c","execution":{"iopub.status.busy":"2022-10-25T16:20:33.623695Z","iopub.execute_input":"2022-10-25T16:20:33.623989Z","iopub.status.idle":"2022-10-25T16:20:33.644963Z","shell.execute_reply.started":"2022-10-25T16:20:33.623923Z","shell.execute_reply":"2022-10-25T16:20:33.644081Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"['Appeal',\n 'Investigate',\n 'EngageInMaterialCooperation',\n 'Assault',\n 'ReduceRelations',\n 'ProvideAid',\n 'Reject',\n 'EngageInDiplomaticCooperation',\n 'ExpressIntentToCooperate',\n 'Yield',\n 'Coerce',\n 'Protest',\n 'Fight',\n 'MakePublicStatement',\n 'Disapprove',\n 'Demand',\n 'ExhibitMilitaryPosture',\n 'Consult']"},"metadata":{}}]},{"cell_type":"code","source":"#get data into needed format and create new DataFrame\ndata = []\nfor index,row in df.iterrows():\n    rel_list = []\n    for rel in row.relations:\n        subj = row.text[rel[\"head_span\"][\"start\"]:rel[\"head_span\"][\"end\"]]\n        obj = row.text[rel[\"child_span\"][\"start\"]:rel[\"child_span\"][\"end\"]]\n        rel_list.append(f\"<triplet> {subj} <subj> {obj} <obj> {rel['label']}\")\n    data.append({\"doc_id\":row.sentence_id, \"text\": row.text, \"triplets\": \" \".join(rel_list)})\n\ndata = pd.DataFrame(data, columns = [\"doc_id\",\"text\",\"triplets\"])","metadata":{"id":"H4cXawHhkvT9","execution":{"iopub.status.busy":"2022-10-25T16:20:33.649391Z","iopub.execute_input":"2022-10-25T16:20:33.650075Z","iopub.status.idle":"2022-10-25T16:20:33.674191Z","shell.execute_reply.started":"2022-10-25T16:20:33.650037Z","shell.execute_reply":"2022-10-25T16:20:33.673120Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"ds = datasets.Dataset.from_pandas(data.drop(columns=[\"doc_id\"]))\n\n#### just random splits for testing, change later to proper splitting\n###### and then change later again for CV\nsplit = ds.train_test_split(test_size = 0.3)\nsplit_val = split[\"test\"].train_test_split(test_size = 0.3)\nds = datasets.DatasetDict({\"train\":split[\"train\"],\"val\":split_val[\"train\"], \"test\":split_val[\"test\"]})\n\n#ds = ds.with_format(\"torch\")\nds","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BYOOvGK6kwtj","outputId":"d6de6d0f-d61c-4d6d-a437-7fbdd605dd37","execution":{"iopub.status.busy":"2022-10-25T16:20:33.676691Z","iopub.execute_input":"2022-10-25T16:20:33.677732Z","iopub.status.idle":"2022-10-25T16:20:33.729175Z","shell.execute_reply.started":"2022-10-25T16:20:33.677692Z","shell.execute_reply":"2022-10-25T16:20:33.728200Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['text', 'triplets'],\n        num_rows: 114\n    })\n    val: Dataset({\n        features: ['text', 'triplets'],\n        num_rows: 35\n    })\n    test: Dataset({\n        features: ['text', 'triplets'],\n        num_rows: 15\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"class conf:\n    #general\n    seed = 0\n    gpus = 1\n    \n    #input\n    batch_size = 8\n    max_length = 128\n    ignore_pad_token_for_loss = True\n    use_fast_tokenizer = True\n    gradient_acc_steps = 1\n    gradient_clip_value = 10.0\n    load_workers = 8\n\n    #optimizer\n    lr = 0.00005\n    weight_decay = 0.01\n    beta1 = 0.9\n    beta2 = 0.999\n    epsilon = 0.00000001\n    warmup_steps = 1000\n\n    #training\n    max_steps = 1200000\n    samples_interval = 1000\n\n    monitor_var  = \"val_loss\"\n    monitor_var_mode = \"min\"\n    # val_check_interval = 0.5\n    # val_percent_check = 0.1\n\n    model_name = \"model1.pth\"\n    checkpoint_path = f\"models/{model_name}\"\n    save_top_k = 1\n\n    early_stopping = False\n    patience = \"5\"\n\n    length_penalty = 0\n    no_repeat_ngram_size = 0\n    num_beams = 3\n    precision = 16\n    amp_level = None\n\n    ","metadata":{"id":"ERXMfgkhkyFS","execution":{"iopub.status.busy":"2022-10-25T16:20:33.730511Z","iopub.execute_input":"2022-10-25T16:20:33.730856Z","iopub.status.idle":"2022-10-25T16:20:33.740642Z","shell.execute_reply.started":"2022-10-25T16:20:33.730816Z","shell.execute_reply":"2022-10-25T16:20:33.739693Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class GetData(pl.LightningDataModule):\n    def __init__(self, conf: conf, tokenizer: AutoTokenizer, model: AutoModelForSeq2SeqLM):\n        \"\"\"init params from config\"\"\"\n        super().__init__()\n        self.tokenizer = tokenizer\n        self.model = model\n        self.datasets = ds\n        \n        # Data collator\n        label_pad_token_id = -100                       \n        self.data_collator = DataCollatorForSeq2Seq(self.tokenizer, self.model, label_pad_token_id=label_pad_token_id, padding = True)\n\n    def preprocess_function(self, data):\n        \"\"\"tokenize, pad, truncate\"\"\"\n        #split into input and labels\n        inputs = data[\"text\"]       \n        outputs = data[\"triplets\"]\n\n        #process input\n        model_inputs = self.tokenizer(inputs, max_length = conf.max_length, padding = True, truncation = True)\n\n        #process labels\n        with self.tokenizer.as_target_tokenizer():\n            labels = self.tokenizer(outputs, max_length = conf.max_length, padding = True, truncation = True)\n\n        model_inputs[\"labels\"] = labels[\"input_ids\"]\n        return model_inputs\n        \n    #apply the preprocessing and load data\n    def train_dataloader(self, *args, **kwargs): \n        self.train_dataset = self.datasets[\"train\"]\n        self.train_dataset = self.train_dataset.map(self.preprocess_function, remove_columns = [\"text\", \"triplets\"], batched = True)\n        return DataLoader(self.train_dataset, batch_size = conf.batch_size, collate_fn = self.data_collator, shuffle = True, num_workers= conf.load_workers)\n\n    #to check if learning works (val on train)\n    def val_dataloader(self, *args, **kwargs): \n        self.train_dataset = self.datasets[\"train\"]\n        self.train_dataset = self.train_dataset.map(self.preprocess_function, remove_columns = [\"text\", \"triplets\"], batched = True)\n        return DataLoader(self.train_dataset, batch_size = conf.batch_size, collate_fn = self.data_collator, shuffle = True, num_workers= conf.load_workers)\n    \n   # def val_dataloader(self, *args, **kwargs): \n   #     self.eval_dataset = self.datasets[\"val\"]\n   #     self.eval_dataset = self.eval_dataset.map(self.preprocess_function, remove_columns = [\"text\", \"triplets\"], batched = True)\n   #     return DataLoader(self.eval_dataset, batch_size = conf.batch_size, collate_fn = self.data_collator, num_workers= conf.load_workers)\n\n    def test_dataloader(self, *args, **kwargs): \n        self.test_dataset = self.datasets[\"test\"]\n        self.test_dataset = self.test_dataset.map(self.preprocess_function,  remove_columns = [\"text\", \"triplets\"], batched = True)\n        return DataLoader(self.test_dataset, batch_size = conf.batch_size, collate_fn = self.data_collator, num_workers= conf.load_workers)","metadata":{"id":"mN1ASyHskzec","execution":{"iopub.status.busy":"2022-10-25T16:20:33.743919Z","iopub.execute_input":"2022-10-25T16:20:33.744246Z","iopub.status.idle":"2022-10-25T16:20:33.756651Z","shell.execute_reply.started":"2022-10-25T16:20:33.744219Z","shell.execute_reply":"2022-10-25T16:20:33.755507Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"config","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pXLqZoiyuVNN","outputId":"999d901d-8f6b-48ff-ab53-12f7df674744","execution":{"iopub.status.busy":"2022-10-25T16:20:33.758436Z","iopub.execute_input":"2022-10-25T16:20:33.759131Z","iopub.status.idle":"2022-10-25T16:20:33.776943Z","shell.execute_reply.started":"2022-10-25T16:20:33.759092Z","shell.execute_reply":"2022-10-25T16:20:33.775824Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Available objects for config:\n     AliasManager\n     DisplayFormatter\n     HistoryManager\n     IPCompleter\n     IPKernelApp\n     InlineBackend\n     LoggingMagics\n     MagicsManager\n     OSMagics\n     PrefilterManager\n     ScriptMagics\n     SqlMagic\n     StoreMagics\n     ZMQInteractiveShell\n","output_type":"stream"}]},{"cell_type":"code","source":"#from REBEL\nfrom typing import Sequence\n\nimport torch\nfrom pytorch_lightning import Callback, LightningModule, Trainer\nfrom torch import nn\nimport pandas as pd\nfrom torch.nn.utils.rnn import pad_sequence\nimport wandb\n\nclass GenerateTextSamplesCallback(Callback):\n    \"\"\"\n    PL Callback to generate triplets along training\n    \"\"\"\n\n    def __init__(self, logging_batch_interval):\n        \"\"\"\n        Args:\n            logging_batch_interval: How frequently to inspect/potentially plot something\n        \"\"\"\n        super().__init__()\n        self.logging_batch_interval = logging_batch_interval\n\n    def on_train_batch_end(self,trainer: Trainer,pl_module: LightningModule, outputs: Sequence, batch: Sequence, batch_idx: int) -> None:\n        wandb_table = wandb.Table(columns=[\"Source\", \"Pred\", \"Gold\"])\n        # pl_module.logger.info(\"Executing translation callback\")\n        labels = batch.pop(\"labels\")\n        gen_kwargs = {\n            \"max_length\": conf.max_length,\n            \"early_stopping\": False,\n            \"no_repeat_ngram_size\": 0,\n            \"num_beams\": conf.num_beams\n        }\n        pl_module.eval()\n\n        decoder_inputs = torch.roll(labels, 1, 1)[:,0:2]\n        decoder_inputs[:, 0] = 0\n        generated_tokens = pl_module.model.generate(\n            batch[\"input_ids\"].to(pl_module.model.device),\n            attention_mask=batch[\"attention_mask\"].to(pl_module.model.device),\n            decoder_input_ids=decoder_inputs.to(pl_module.model.device),\n            **gen_kwargs,\n        )\n        # in case the batch is shorter than max length, the output should be padded\n        if generated_tokens.shape[-1] < gen_kwargs[\"max_length\"]:\n            generated_tokens = pl_module._pad_tensors_to_max_len(generated_tokens, gen_kwargs[\"max_length\"])\n        pl_module.train()\n        decoded_preds = pl_module.tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n\n        #If ignore pad token for loss == True \n            # Replace -100 in the labels as we can't decode them.\n        labels = torch.where(labels != -100, labels, pl_module.tokenizer.pad_token_id)\n\n        decoded_labels = pl_module.tokenizer.batch_decode(labels, skip_special_tokens=False)\n        decoded_inputs = pl_module.tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=False)\n\n        # pl_module.logger.experiment.log_text('generated samples', '\\n'.join(decoded_preds).replace('<pad>', ''))\n        # pl_module.logger.experiment.log_text('original samples', '\\n'.join(decoded_labels).replace('<pad>', ''))\n        for source, translation, gold_output in zip(decoded_inputs, decoded_preds, decoded_labels):\n            wandb_table.add_data(\n                source.replace('<pad>', ''), translation.replace('<pad>', ''), gold_output.replace('<pad>', '')\n            )\n        pl_module.logger.experiment.log({\"Triplets\": wandb_table})","metadata":{"id":"Y9ELEKQHk2-f","execution":{"iopub.status.busy":"2022-10-25T16:20:33.778535Z","iopub.execute_input":"2022-10-25T16:20:33.778975Z","iopub.status.idle":"2022-10-25T16:20:33.793218Z","shell.execute_reply.started":"2022-10-25T16:20:33.778924Z","shell.execute_reply":"2022-10-25T16:20:33.792223Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#from REBEL\ndef shift_tokens_left(input_ids: torch.Tensor, pad_token_id: int):\n    \"\"\"\n    Shift input ids one token to the right.\n    \"\"\"\n    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n    shifted_input_ids[:, :-1] = input_ids[:, 1:].clone()\n    shifted_input_ids[:, -1] = pad_token_id\n\n    assert pad_token_id is not None, \"self.model.config.pad_token_id has to be defined.\"\n\n    return shifted_input_ids","metadata":{"id":"5NAqweGsk4Op","execution":{"iopub.status.busy":"2022-10-25T16:20:33.794421Z","iopub.execute_input":"2022-10-25T16:20:33.794970Z","iopub.status.idle":"2022-10-25T16:20:33.808703Z","shell.execute_reply.started":"2022-10-25T16:20:33.794915Z","shell.execute_reply":"2022-10-25T16:20:33.807745Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"#from REBEL\n\n##### can maybe be rewritten more effective\ndef extract_triplets(text):\n    triplets = []\n    relation, subject, relation, object_ = '', '', '', ''\n    text = text.strip()\n    current = 'x'\n    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n        if token == \"<triplet>\":\n            current = 't'\n            if relation != '':\n                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n                relation = ''\n            subject = ''\n        elif token == \"<subj>\":\n            current = 's'\n            if relation != '':\n                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n            object_ = ''\n        elif token == \"<obj>\":\n            current = 'o'\n            relation = ''\n        else:\n            if current == 't':\n                subject += ' ' + token\n            elif current == 's':\n                object_ += ' ' + token\n            elif current == 'o':\n                relation += ' ' + token\n    if subject != '' and relation != '' and object_ != '':\n        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n    return triplets","metadata":{"id":"91a0v9t_k-Hx","execution":{"iopub.status.busy":"2022-10-25T16:20:33.810243Z","iopub.execute_input":"2022-10-25T16:20:33.810667Z","iopub.status.idle":"2022-10-25T16:20:33.822269Z","shell.execute_reply.started":"2022-10-25T16:20:33.810629Z","shell.execute_reply":"2022-10-25T16:20:33.821222Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"#from REBEL \ndef score(key, prediction, verbose=False):\n    correct_by_relation = Counter()\n    guessed_by_relation = Counter()\n    gold_by_relation    = Counter()\n\n    # Loop over the data to compute a score\n    for row in range(len(prediction)):\n        gold = key[row]\n        guess = prediction[row]\n         \n        if gold == NO_RELATION and guess == NO_RELATION:\n            pass\n        elif gold == NO_RELATION and guess != NO_RELATION:\n            guessed_by_relation[guess] += 1\n        elif gold != NO_RELATION and guess == NO_RELATION:\n            gold_by_relation[gold] += 1\n        elif gold != NO_RELATION and guess != NO_RELATION:\n            guessed_by_relation[guess] += 1\n            gold_by_relation[gold] += 1\n            if gold == guess:\n                correct_by_relation[guess] += 1\n\n    # Print verbose information\n    if verbose:\n        print(\"Per-relation statistics:\")\n        relations = gold_by_relation.keys()\n        longest_relation = 0\n        for relation in sorted(relations):\n            longest_relation = max(len(relation), longest_relation)\n        for relation in sorted(relations):\n            # (compute the score)\n            correct = correct_by_relation[relation]\n            guessed = guessed_by_relation[relation]\n            gold    = gold_by_relation[relation]\n            prec = 1.0\n            if guessed > 0:\n                prec = float(correct) / float(guessed)\n            recall = 0.0\n            if gold > 0:\n                recall = float(correct) / float(gold)\n            f1 = 0.0\n            if prec + recall > 0:\n                f1 = 2.0 * prec * recall / (prec + recall)\n            # (print the score)\n            sys.stdout.write((\"{:<\" + str(longest_relation) + \"}\").format(relation))\n            sys.stdout.write(\"  P: \")\n            if prec < 0.1: sys.stdout.write(' ')\n            if prec < 1.0: sys.stdout.write(' ')\n            sys.stdout.write(\"{:.2%}\".format(prec))\n            sys.stdout.write(\"  R: \")\n            if recall < 0.1: sys.stdout.write(' ')\n            if recall < 1.0: sys.stdout.write(' ')\n            sys.stdout.write(\"{:.2%}\".format(recall))\n            sys.stdout.write(\"  F1: \")\n            if f1 < 0.1: sys.stdout.write(' ')\n            if f1 < 1.0: sys.stdout.write(' ')\n            sys.stdout.write(\"{:.2%}\".format(f1))\n            sys.stdout.write(\"  #: %d\" % gold)\n            sys.stdout.write(\"\\n\")\n        print(\"\")\n\n    # Print the aggregate score\n    if verbose:\n        print(\"Final Score:\")\n    prec_micro = 1.0\n    if sum(guessed_by_relation.values()) > 0:\n        prec_micro   = float(sum(correct_by_relation.values())) / float(sum(guessed_by_relation.values()))\n    recall_micro = 0.0\n    if sum(gold_by_relation.values()) > 0:\n        recall_micro = float(sum(correct_by_relation.values())) / float(sum(gold_by_relation.values()))\n    f1_micro = 0.0\n    if prec_micro + recall_micro > 0.0:\n        f1_micro = 2.0 * prec_micro * recall_micro / (prec_micro + recall_micro)\n    print(\"Precision (micro): {:.3%}\".format(prec_micro))\n    print(\"   Recall (micro): {:.3%}\".format(recall_micro))\n    print(\"       F1 (micro): {:.3%}\".format(f1_micro))\n    return prec_micro, recall_micro, f1_micro\n\n'''Adapted from: https://github.com/btaille/sincere/blob/6f5472c5aeaf7ef7765edf597ede48fdf1071712/code/utils/evaluation.py'''\ndef re_score(pred_relations, gt_relations, relation_types, mode=\"boundaries\"):\n    \"\"\"Evaluate RE predictions\n    Args:\n        pred_relations (list) :  list of list of predicted relations (several relations in each sentence)\n        gt_relations (list) :    list of list of ground truth relations\n            rel = { \"head\": (start_idx (inclusive), end_idx (exclusive)),\n                    \"tail\": (start_idx (inclusive), end_idx (exclusive)),\n                    \"head_type\": ent_type,\n                    \"tail_type\": ent_type,\n                    \"type\": rel_type}\n        vocab (Vocab) :         dataset vocabulary\n        mode (str) :            in 'strict' or 'boundaries' \"\"\"\n\n    assert mode in [\"strict\", \"boundaries\"]\n    relation_types = [\"MakePublicStatement\",\"Appeal\",\"ExpressIntentToCooperate\",\"Consult\",\"EngageInDiplomaticCooperation\",\"EngageInMaterialCooperation\",\"ProvideAid\",\"Yield\",\"Investigate\",\"Demand\",\"Disapprove\",\"Reject\",\"Threaten\",\"ExhibitMilitaryPosture\",\"Protest\",\"ReduceRelations\",\"Coerce\",\"Assault\",\"Fight\",\"EngageInUnconventialMassViolence\"]\n        \n    # relation_types = [v for v in relation_types if not v == \"None\"]\n    scores = {rel: {\"tp\": 0, \"fp\": 0, \"fn\": 0} for rel in relation_types + [\"ALL\"]}\n\n    # Count GT relations and Predicted relations\n    n_sents = len(gt_relations)\n    n_rels = sum([len([rel for rel in sent]) for sent in gt_relations])\n    n_found = sum([len([rel for rel in sent]) for sent in pred_relations])\n\n    # Count TP, FP and FN per type\n    for pred_sent, gt_sent in zip(pred_relations, gt_relations):\n        for rel_type in relation_types:\n            # strict mode takes argument types into account\n            if mode == \"strict\":\n                pred_rels = {(rel[\"head\"], rel[\"head_type\"], rel[\"tail\"], rel[\"tail_type\"]) for rel in pred_sent if\n                             rel[\"type\"] == rel_type}\n                gt_rels = {(rel[\"head\"], rel[\"head_type\"], rel[\"tail\"], rel[\"tail_type\"]) for rel in gt_sent if\n                           rel[\"type\"] == rel_type}\n\n            # boundaries mode only takes argument spans into account\n            elif mode == \"boundaries\":\n                pred_rels = {(rel[\"head\"], rel[\"tail\"]) for rel in pred_sent if rel[\"type\"] == rel_type}\n                gt_rels = {(rel[\"head\"], rel[\"tail\"]) for rel in gt_sent if rel[\"type\"] == rel_type}\n\n            scores[rel_type][\"tp\"] += len(pred_rels & gt_rels)\n            scores[rel_type][\"fp\"] += len(pred_rels - gt_rels)\n            scores[rel_type][\"fn\"] += len(gt_rels - pred_rels)\n\n    # Compute per relation Precision / Recall / F1\n    for rel_type in scores.keys():\n        if scores[rel_type][\"tp\"]:\n            scores[rel_type][\"p\"] = 100 * scores[rel_type][\"tp\"] / (scores[rel_type][\"fp\"] + scores[rel_type][\"tp\"])\n            scores[rel_type][\"r\"] = 100 * scores[rel_type][\"tp\"] / (scores[rel_type][\"fn\"] + scores[rel_type][\"tp\"])\n        else:\n            scores[rel_type][\"p\"], scores[rel_type][\"r\"] = 0, 0\n\n        if not scores[rel_type][\"p\"] + scores[rel_type][\"r\"] == 0:\n            scores[rel_type][\"f1\"] = 2 * scores[rel_type][\"p\"] * scores[rel_type][\"r\"] / (\n                    scores[rel_type][\"p\"] + scores[rel_type][\"r\"])\n        else:\n            scores[rel_type][\"f1\"] = 0\n\n    # Compute micro F1 Scores\n    tp = sum([scores[rel_type][\"tp\"] for rel_type in relation_types])\n    fp = sum([scores[rel_type][\"fp\"] for rel_type in relation_types])\n    fn = sum([scores[rel_type][\"fn\"] for rel_type in relation_types])\n\n    if tp:\n        precision = 100 * tp / (tp + fp)\n        recall = 100 * tp / (tp + fn)\n        f1 = 2 * precision * recall / (precision + recall)\n\n    else:\n        precision, recall, f1 = 0, 0, 0\n\n    scores[\"ALL\"][\"p\"] = precision\n    scores[\"ALL\"][\"r\"] = recall\n    scores[\"ALL\"][\"f1\"] = f1\n    scores[\"ALL\"][\"tp\"] = tp\n    scores[\"ALL\"][\"fp\"] = fp\n    scores[\"ALL\"][\"fn\"] = fn\n\n    # Compute Macro F1 Scores\n    scores[\"ALL\"][\"Macro_f1\"] = np.mean([scores[ent_type][\"f1\"] for ent_type in relation_types])\n    scores[\"ALL\"][\"Macro_p\"] = np.mean([scores[ent_type][\"p\"] for ent_type in relation_types])\n    scores[\"ALL\"][\"Macro_r\"] = np.mean([scores[ent_type][\"r\"] for ent_type in relation_types])\n\n    print(f\"RE Evaluation in *** {mode.upper()} *** mode\")\n\n    print(\n        \"processed {} sentences with {} relations; found: {} relations; correct: {}.\".format(n_sents, n_rels, n_found,\n                                                                                             tp))\n    print(\n        \"\\tALL\\t TP: {};\\tFP: {};\\tFN: {}\".format(\n            scores[\"ALL\"][\"tp\"],\n            scores[\"ALL\"][\"fp\"],\n            scores[\"ALL\"][\"fn\"]))\n    print(\n        \"\\t\\t(m avg): precision: {:.2f};\\trecall: {:.2f};\\tf1: {:.2f} (micro)\".format(\n            precision,\n            recall,\n            f1))\n    print(\n        \"\\t\\t(M avg): precision: {:.2f};\\trecall: {:.2f};\\tf1: {:.2f} (Macro)\\n\".format(\n            scores[\"ALL\"][\"Macro_p\"],\n            scores[\"ALL\"][\"Macro_r\"],\n            scores[\"ALL\"][\"Macro_f1\"]))\n\n    for rel_type in relation_types:\n        print(\"\\t{}: \\tTP: {};\\tFP: {};\\tFN: {};\\tprecision: {:.2f};\\trecall: {:.2f};\\tf1: {:.2f};\\t{}\".format(\n            rel_type,\n            scores[rel_type][\"tp\"],\n            scores[rel_type][\"fp\"],\n            scores[rel_type][\"fn\"],\n            scores[rel_type][\"p\"],\n            scores[rel_type][\"r\"],\n            scores[rel_type][\"f1\"],\n            scores[rel_type][\"tp\"] +\n            scores[rel_type][\n                \"fp\"]))\n\n    return scores, precision, recall, f1","metadata":{"id":"MRoOQCczqp4k","execution":{"iopub.status.busy":"2022-10-25T16:20:33.824154Z","iopub.execute_input":"2022-10-25T16:20:33.824588Z","iopub.status.idle":"2022-10-25T16:20:33.863195Z","shell.execute_reply.started":"2022-10-25T16:20:33.824550Z","shell.execute_reply":"2022-10-25T16:20:33.862197Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class BaseModule(pl.LightningModule):\n\n    def __init__(self, conf, config: AutoConfig, tokenizer: AutoTokenizer, model: AutoModelForSeq2SeqLM):\n        super().__init__()\n        self.config = config\n        self.model = model\n        self.tokenizer = tokenizer\n        #self.loss_fn = label_smoothed_nll_loss\n        self.loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n        self.num_beams = conf.num_beams\n\n    def forward(self, inputs, labels, *args):\n        ##### Check later if smooth labeled loss is better \n        outputs = self.model(**inputs, labels = labels, use_cache = False, return_dict = True, output_hidden_states = True)\n        output_dict = {'loss': outputs['loss'], 'logits': outputs['logits']}\n        return output_dict\n\n    def training_step(self, batch: dict, batch_idx: int):\n        ##### check later if labels = batch[\"labels\"] also works\n        labels = batch.pop(\"labels\")\n        labels_original = labels.clone()\n        batch[\"decoder_input_ids\"] = torch.where(labels != -100, labels, self.config.pad_token_id)\n        labels = shift_tokens_left(labels, -100)\n\n        forward_output = self.forward(batch, labels)\n        self.log('loss', forward_output['loss'])\n\n        batch[\"labels\"] = labels_original\n\n        #### ig i dont have this\n        if 'loss_aux' in forward_output:\n            self.log('loss_classifier', forward_output['loss_aux'])\n            return forward_output['loss'] + forward_output['loss_aux']\n\n        return forward_output['loss']\n\n    def validation_step(self, batch: dict, batch_idx):\n        #### pop maybe not needed?\n        labels = batch.pop(\"labels\")\n        batch[\"decoder_input_ids\"] = torch.where(labels != -100, labels, self.config.pad_token_id)\n        labels = shift_tokens_left(labels, -100)\n        \n        with torch.no_grad():\n            # compute loss on predict data\n            forward_output = self.forward(batch, labels)\n\n        forward_output['loss'] = forward_output['loss'].mean().detach()\n\n        #### probably not needed ? why would i want only pred loss\n        # if self.hparams.prediction_loss_only:\n        #     self.log('val_loss', forward_output['loss'])\n        #     return\n\n        forward_output['logits'] = forward_output['logits'].detach()\n\n        if labels.shape[-1] < conf.max_length:\n            forward_output['labels'] = self._pad_tensors_to_max_len(labels, conf.max_length)\n        else:\n            forward_output['labels'] = labels\n\n        metrics = {}\n        metrics['val_loss'] = forward_output['loss']\n\n        #### only 1? so why loop lmao\n        for key in sorted(metrics.keys()):\n            self.log(key, metrics[key])\n\n        outputs = {}\n        outputs['predictions'], outputs['labels'] = self.generate_triples(batch, labels)\n        return outputs\n\n\n    def test_step(self, batch, batch_idx):\n\n        #### popping again\n        labels = batch.pop(\"labels\")\n        batch[\"decoder_input_ids\"] = torch.where(labels != -100, labels, self.config.pad_token_id)\n        labels = shift_tokens_left(labels, -100)\n\n        with torch.no_grad():\n            # compute loss on predict data\n            forward_output = self.forward(batch, labels)\n\n        forward_output['loss'] = forward_output['loss'].mean().detach()\n\n        ##### probably not needed? would would i only want pred loss\n        # if self.hparams.prediction_loss_only:\n        #     self.log('test_loss', forward_output['loss'])\n        #     return\n\n        forward_output['logits'] = forward_output['logits'].detach()\n\n        if labels.shape[-1] < conf.max_length:\n            forward_output['labels'] = self._pad_tensors_to_max_len(labels, conf.max_length)\n        else:\n            forward_output['labels'] = labels\n\n\n        metrics = {}\n        metrics['test_loss'] = forward_output['loss']\n\n        #### dont i only have one metric anyways?\n        for key in sorted(metrics.keys()):\n            self.log(key, metrics[key], prog_bar=True)\n\n        #### what does this actually do? how does this change everything\n        # if self.hparams.finetune:\n        #     return {'predictions': self.forward_samples(batch, labels)}\n        # else:\n\n        outputs = {}\n        outputs['predictions'], outputs['labels'] = self.generate_triples(batch, labels)\n        return outputs\n\n\n\n    def validation_epoch_end(self, output: dict):\n        \n        relations = [\"MakePublicStatement\",\"Appeal\",\"ExpressIntentToCooperate\",\"Consult\",\"EngageInDiplomaticCooperation\",\"EngageInMaterialCooperation\",\"ProvideAid\",\"Yield\",\"Investigate\",\"Demand\",\"Disapprove\",\"Reject\",\"Threaten\",\"ExhibitMilitaryPosture\",\"Protest\",\"ReduceRelations\",\"Coerce\",\"Assault\",\"Fight\",\"EngageInUnconventialMassViolence\"]\n        scores, precision, recall, f1 = re_score([item for pred in output for item in pred['predictions']], [item for pred in output for item in pred['labels']], relations)\n        self.log('val_prec_micro', precision)\n        self.log('val_recall_micro', recall)\n        self.log('val_F1_micro', f1)\n\n    def test_epoch_end(self, output: dict):\n\n        relations = [\"MakePublicStatement\",\"Appeal\",\"ExpressIntentToCooperate\",\"Consult\",\"EngageInDiplomaticCooperation\",\"EngageInMaterialCooperation\",\"ProvideAid\",\"Yield\",\"Investigate\",\"Demand\",\"Disapprove\",\"Reject\",\"Threaten\",\"ExhibitMilitaryPosture\",\"Protest\",\"ReduceRelations\",\"Coerce\",\"Assault\",\"Fight\",\"EngageInUnconventialMassViolence\"]\n        scores, precision, recall, f1 = re_score([item for pred in output for item in pred['predictions']], [item for pred in output for item in pred['labels']], relations)\n        self.log('test_prec_micro', precision)\n        self.log('test_recall_micro', recall)\n        self.log('test_F1_micro', f1)\n\n\n    # additional functions called in main functions\n\n    def generate_triples(self, batch, labels) -> None:\n\n        generated_tokens = self.model.generate(\n            batch[\"input_ids\"].to(self.model.device),\n            attention_mask=batch[\"attention_mask\"].to(self.model.device),\n            use_cache = True, max_length = conf.max_length, early_stopping = conf.early_stopping, length_penalty = conf.length_penalty, \n            no_repeat_ngram_size = conf.no_repeat_ngram_size, num_beams = conf.num_beams)\n\n        decoded_preds = self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n        decoded_labels = self.tokenizer.batch_decode(torch.where(labels != -100, labels, self.config.pad_token_id), skip_special_tokens=False)\n\n        return [extract_triplets(rel) for rel in decoded_preds], [extract_triplets(rel) for rel in decoded_labels]\n\n    def _pad_tensors_to_max_len(self, tensor, max_length):\n        # If PAD token is not defined at least EOS token has to be defined\n        pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else self.config.eos_token_id\n\n        if pad_token_id is None:\n            raise ValueError(\n                f\"Make sure that either `config.pad_token_id` or `config.eos_token_id` is defined if tensor has to be padded to `max_length`={max_length}\"\n            )\n\n        padded_tensor = pad_token_id * torch.ones(\n            (tensor.shape[0], max_length), dtype=tensor.dtype, device=tensor.device\n        )\n        padded_tensor[:, : tensor.shape[-1]] = tensor\n        return padded_tensor\n\n    def configure_optimizers(self):\n\n        ##### HUH\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n                \"weight_decay\": conf.weight_decay,\n            },\n            {\n                \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n                \"weight_decay\": 0.0,\n            },\n        ]\n        optimizer = AdamW(optimizer_grouped_parameters, lr = conf.lr, betas = (conf.beta1, conf.beta2), eps = conf.epsilon, weight_decay = conf.weight_decay)\n        #### also check warmup later\n        factor = lambda epoch: 0.95\n        scheduler = lr_scheduler.MultiplicativeLR(optimizer, factor)\n        \n        #scheduler = inverse_square_root(optimizer, num_warmup_steps= conf.warmup_steps)\n\n        return [optimizer], [scheduler]\n\n    #def compute_metrics():\n        #looks not needed    ","metadata":{"id":"33KLRy0hk_4f","execution":{"iopub.status.busy":"2022-10-25T16:20:33.864871Z","iopub.execute_input":"2022-10-25T16:20:33.865604Z","iopub.status.idle":"2022-10-25T16:20:33.898433Z","shell.execute_reply.started":"2022-10-25T16:20:33.865565Z","shell.execute_reply":"2022-10-25T16:20:33.897431Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def train(conf):\n    pl.seed_everything(conf.seed)\n\n    tokenizer = transformers.AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\", use_fast = conf.use_fast_tokenizer,\n        additional_special_tokens = [\"<obj>\", \"<subj>\", \"<triplet>\", \"<head>\", \"</head>\", \"<tail>\", \"</tail>\"])\n    config = transformers.AutoConfig.from_pretrained(\"Babelscape/rebel-large\")\n    model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\", config = config)\n\n    model.resize_token_embeddings(len(tokenizer))\n\n    pl_data_module = GetData(conf, tokenizer, model)\n    pl_module = BaseModule(conf, config, tokenizer, model)\n\n    wandb_logger = WandbLogger(project = \"project/finetune\".split('/')[-1].replace('.py', ''), name = \"finetune\")\n\n    callbacks_store = []\n\n    if conf.early_stopping:\n        callbacks_store.append(\n            EarlyStopping(\n                monitor=conf.monitor_var,\n                mode=conf.monitor_var_mode,\n                patience=conf.patience\n            )\n        )\n\n    # callbacks_store.append(\n    #     ModelCheckpoint(\n    #         monitor=conf.monitor_var,\n    #         # monitor=None,\n    #         dirpath=f'models/{conf.model_name}',\n    #         save_top_k=conf.save_top_k,\n    #         verbose=True,\n    #         save_last=True,\n    #         mode=conf.monitor_var_mode\n    #     )\n    # )\n    callbacks_store.append(GenerateTextSamplesCallback(conf.samples_interval))\n    callbacks_store.append(LearningRateMonitor(logging_interval='step'))\n\n    trainer = pl.Trainer(\n        gpus=conf.gpus,\n        accumulate_grad_batches=conf.gradient_acc_steps,\n        gradient_clip_val=conf.gradient_clip_value,\n        #val_check_interval=conf.val_check_interval,\n        max_epochs = 10,\n        min_epochs = 5,\n        callbacks=callbacks_store,\n        max_steps=conf.max_steps,\n        # max_steps=total_steps,\n        precision=conf.precision,\n        amp_level=conf.amp_level,\n        logger=wandb_logger,\n        #resume_from_checkpoint=conf.checkpoint_path,\n        #limit_val_batches=conf.val_percent_check\n    )\n\n    # module fit\n    trainer.fit(pl_module, datamodule=pl_data_module)","metadata":{"id":"I-qhi66IlBPX","execution":{"iopub.status.busy":"2022-10-25T16:20:33.899869Z","iopub.execute_input":"2022-10-25T16:20:33.900411Z","iopub.status.idle":"2022-10-25T16:20:33.914011Z","shell.execute_reply.started":"2022-10-25T16:20:33.900375Z","shell.execute_reply":"2022-10-25T16:20:33.913045Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"train(conf)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["150a10a7600e4b7c81bae0d8f2074fb5","9d9f0941f63b4591b5dfb60b3f04b5eb","83b7f2b300774b57bc9a6d161d1ab361","369e2460fef14cf0b608ee6606b9a958","d31b8fc829d2490f8fe535b25665fd92","ff2b281cf4b2404eac573758fe98a514","82fa24fea072448fb581209b493f94af","908372568f5949778e4cdbcf4c6aef12","f366bf93bf994f53aad7570486246d0c","ba03625f26154813bc49b987ca0b0d82","574a438a44b04defa82bdd0424235a7f","8a2b47dcdc404fff8526e82712ba8ff1","e0bf74faf96c4644b2700af0e82bc3c2","f18929c9b22e42079fabcb0b81cf2555","baa2cb71da6a433da0dc27b698816fd1","cf33818563cc476983451e5b7155e32e","447b7a6abb4240a3b1493329300c84e5","d1af56d58aad44c5b7365f9b0bc53ba0","6390c74629db4f5abfb4f52c53dc6796","7232c60b8a544b7ab418cbbf3f90720e","531d5a86ef5644089d61291f92f8f639","62092f8a23944cb1a684839f61b8da4a","9b27b142777943eb98a4367fc253cc0e","2bb2f84d2127499b8ea398d27df9a5cb","5bc78be9136749578de53aed6185756e","6acdde5760364257ae70a57d5bbfbd7c","9c0784b612c742b395691825fa5971de","30f05c40bada49639644cfc5b646c0b3","0e8de96d82244c589947cffef206d26f","98f926c5ed954d818da6cf0025802907","17833a9dc3954892ade426c385e14006","eaaa70cbd0df4742a1406f3a3ebf4ec6","aa96ab2d2d3e4310bac4ac3d35c9d3e8","eb752942c88e4a2e9f0baf8f1ff88ada","e240a887867b44a4b77e3bd1d61da4d3","988be07f83dd49f5b1052d178acb265d","646b8524eb294d3aa85f90cf3b311ad5","762bfd3d61524e579db9791b0e5c822f","ffd67d27fa164c4a90b5a27695378525","676472d809674f0ea42e82ca6cc9dbec","e0982ae149b04402aafd0507b0250d78","966af13a165a4bdcab767330ad57ab79","c7225060efb14bfbad9003100ec47a63","898a55d0c4cb41879312a9baa217ab40","173944ae52604cfb8040a9727cf55367","f4d445e20cd64e40860d4ea1718ff4fd","e993024d78b6443bbe82684b314a1ce2","ec6264a5574042c3941aa1172d258935","f43018b3490141dfb36940ca58118883","3c69166d38a243d5bb92547fa524cf58","1211eb58256245cea32f856fd3a0a811","fd25fb346299476ab239f4b4a7653423","f7adc9ed848348c6acd422cb2a0b32e8","d1db33134e7944f1b1b887f7a66c5858","abfe68dc0ff2469aa4e1c226d82a6a57"]},"id":"f1WDgwqOlD5U","outputId":"3f81075a-86c4-4521-bd64-cb74560d327d","execution":{"iopub.status.busy":"2022-10-25T16:20:33.917424Z","iopub.execute_input":"2022-10-25T16:20:33.917692Z","iopub.status.idle":"2022-10-25T16:40:04.200996Z","shell.execute_reply.started":"2022-10-25T16:20:33.917667Z","shell.execute_reply":"2022-10-25T16:40:04.198570Z"},"trusted":true},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29245e1bd4834979b650cee886027ebc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/780k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26f88c60b83f47fd99d4ed6d7513e44d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b58ca16be354ce4b177c16a3f9b2921"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e030174027c2436da95cdc56f0d8bcf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/123 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"79195c52e32f482cb3ccf0e77de416c2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/344 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71f3807c4f5b4986988084959261566d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.39k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c27d35fe0b524be0bd488f0952692590"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.51G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eae7b555942e47438b9ec9b9337dc242"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.13.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.12.21"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>./wandb/run-20221025_162204-3pk4nq6f</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href=\"https://wandb.ai/valentinwerner/finetune/runs/3pk4nq6f\" target=\"_blank\">finetune</a></strong> to <a href=\"https://wandb.ai/valentinwerner/finetune\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:448: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1fcd29208717481faf1f731f8ebdb164"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n  category=PossibleUserWarning,\n","output_type":"stream"},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRE Evaluation in *** BOUNDARIES *** mode\nprocessed 16 sentences with 25 relations; found: 17 relations; correct: 0.\n\tALL\t TP: 0;\tFP: 0;\tFN: 24\n\t\t(m avg): precision: 0.00;\trecall: 0.00;\tf1: 0.00 (micro)\n\t\t(M avg): precision: 0.00;\trecall: 0.00;\tf1: 0.00 (Macro)\n\n\tMakePublicStatement: \tTP: 0;\tFP: 0;\tFN: 11;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tAppeal: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tExpressIntentToCooperate: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tConsult: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tEngageInDiplomaticCooperation: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tEngageInMaterialCooperation: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tProvideAid: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tYield: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tInvestigate: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tDemand: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tDisapprove: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tReject: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tThreaten: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tExhibitMilitaryPosture: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tProtest: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tReduceRelations: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tCoerce: \tTP: 0;\tFP: 0;\tFN: 2;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tAssault: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tFight: \tTP: 0;\tFP: 0;\tFN: 5;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tEngageInUnconventialMassViolence: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('val_prec_micro', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('val_recall_micro', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('val_F1_micro', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f26525d8dd7942b9bbf71b9733db3dd3"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:1896: PossibleUserWarning: The number of training batches (15) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n  category=PossibleUserWarning,\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9f9eb0b0e5fc49539bbaffd5091d346a"}},"metadata":{}},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRE Evaluation in *** BOUNDARIES *** mode\nprocessed 114 sentences with 202 relations; found: 50 relations; correct: 0.\n\tALL\t TP: 0;\tFP: 0;\tFN: 195\n\t\t(m avg): precision: 0.00;\trecall: 0.00;\tf1: 0.00 (micro)\n\t\t(M avg): precision: 0.00;\trecall: 0.00;\tf1: 0.00 (Macro)\n\n\tMakePublicStatement: \tTP: 0;\tFP: 0;\tFN: 79;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tAppeal: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tExpressIntentToCooperate: \tTP: 0;\tFP: 0;\tFN: 5;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tConsult: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tEngageInDiplomaticCooperation: \tTP: 0;\tFP: 0;\tFN: 6;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tEngageInMaterialCooperation: \tTP: 0;\tFP: 0;\tFN: 6;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tProvideAid: \tTP: 0;\tFP: 0;\tFN: 11;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tYield: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tInvestigate: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tDemand: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tDisapprove: \tTP: 0;\tFP: 0;\tFN: 18;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tReject: \tTP: 0;\tFP: 0;\tFN: 2;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tThreaten: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tExhibitMilitaryPosture: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tProtest: \tTP: 0;\tFP: 0;\tFN: 5;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tReduceRelations: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tCoerce: \tTP: 0;\tFP: 0;\tFN: 11;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tAssault: \tTP: 0;\tFP: 0;\tFN: 10;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tFight: \tTP: 0;\tFP: 0;\tFN: 22;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tEngageInUnconventialMassViolence: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRE Evaluation in *** BOUNDARIES *** mode\nprocessed 114 sentences with 202 relations; found: 70 relations; correct: 17.\n\tALL\t TP: 17;\tFP: 30;\tFN: 178\n\t\t(m avg): precision: 36.17;\trecall: 8.72;\tf1: 14.05 (micro)\n\t\t(M avg): precision: 1.81;\trecall: 1.08;\tf1: 1.35 (Macro)\n\n\tMakePublicStatement: \tTP: 17;\tFP: 30;\tFN: 62;\tprecision: 36.17;\trecall: 21.52;\tf1: 26.98;\t47\n\tAppeal: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tExpressIntentToCooperate: \tTP: 0;\tFP: 0;\tFN: 5;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tConsult: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tEngageInDiplomaticCooperation: \tTP: 0;\tFP: 0;\tFN: 6;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tEngageInMaterialCooperation: \tTP: 0;\tFP: 0;\tFN: 6;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tProvideAid: \tTP: 0;\tFP: 0;\tFN: 11;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tYield: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tInvestigate: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tDemand: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tDisapprove: \tTP: 0;\tFP: 0;\tFN: 18;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tReject: \tTP: 0;\tFP: 0;\tFN: 2;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tThreaten: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tExhibitMilitaryPosture: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tProtest: \tTP: 0;\tFP: 0;\tFN: 5;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tReduceRelations: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tCoerce: \tTP: 0;\tFP: 0;\tFN: 11;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tAssault: \tTP: 0;\tFP: 0;\tFN: 10;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tFight: \tTP: 0;\tFP: 0;\tFN: 22;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tEngageInUnconventialMassViolence: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRE Evaluation in *** BOUNDARIES *** mode\nprocessed 114 sentences with 202 relations; found: 51 relations; correct: 20.\n\tALL\t TP: 20;\tFP: 25;\tFN: 175\n\t\t(m avg): precision: 44.44;\trecall: 10.26;\tf1: 16.67 (micro)\n\t\t(M avg): precision: 6.06;\trecall: 1.64;\tf1: 2.41 (Macro)\n\n\tMakePublicStatement: \tTP: 18;\tFP: 21;\tFN: 61;\tprecision: 46.15;\trecall: 22.78;\tf1: 30.51;\t39\n\tAppeal: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tExpressIntentToCooperate: \tTP: 0;\tFP: 0;\tFN: 5;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tConsult: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tEngageInDiplomaticCooperation: \tTP: 0;\tFP: 0;\tFN: 6;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tEngageInMaterialCooperation: \tTP: 0;\tFP: 0;\tFN: 6;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tProvideAid: \tTP: 0;\tFP: 0;\tFN: 11;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tYield: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tInvestigate: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tDemand: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tDisapprove: \tTP: 1;\tFP: 1;\tFN: 17;\tprecision: 50.00;\trecall: 5.56;\tf1: 10.00;\t2\n\tReject: \tTP: 0;\tFP: 0;\tFN: 2;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tThreaten: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tExhibitMilitaryPosture: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tProtest: \tTP: 0;\tFP: 0;\tFN: 5;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tReduceRelations: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tCoerce: \tTP: 0;\tFP: 0;\tFN: 11;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tAssault: \tTP: 0;\tFP: 0;\tFN: 10;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tFight: \tTP: 1;\tFP: 3;\tFN: 21;\tprecision: 25.00;\trecall: 4.55;\tf1: 7.69;\t4\n\tEngageInUnconventialMassViolence: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRE Evaluation in *** BOUNDARIES *** mode\nprocessed 114 sentences with 202 relations; found: 53 relations; correct: 32.\n\tALL\t TP: 32;\tFP: 16;\tFN: 163\n\t\t(m avg): precision: 66.67;\trecall: 16.41;\tf1: 26.34 (micro)\n\t\t(M avg): precision: 8.08;\trecall: 2.90;\tf1: 4.16 (Macro)\n\n\tMakePublicStatement: \tTP: 27;\tFP: 11;\tFN: 52;\tprecision: 71.05;\trecall: 34.18;\tf1: 46.15;\t38\n\tAppeal: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tExpressIntentToCooperate: \tTP: 0;\tFP: 0;\tFN: 5;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tConsult: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tEngageInDiplomaticCooperation: \tTP: 0;\tFP: 0;\tFN: 6;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tEngageInMaterialCooperation: \tTP: 0;\tFP: 0;\tFN: 6;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tProvideAid: \tTP: 0;\tFP: 0;\tFN: 11;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tYield: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tInvestigate: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tDemand: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tDisapprove: \tTP: 1;\tFP: 2;\tFN: 17;\tprecision: 33.33;\trecall: 5.56;\tf1: 9.52;\t3\n\tReject: \tTP: 0;\tFP: 0;\tFN: 2;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tThreaten: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tExhibitMilitaryPosture: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tProtest: \tTP: 0;\tFP: 0;\tFN: 5;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tReduceRelations: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tCoerce: \tTP: 0;\tFP: 0;\tFN: 11;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tAssault: \tTP: 0;\tFP: 0;\tFN: 10;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tFight: \tTP: 4;\tFP: 3;\tFN: 18;\tprecision: 57.14;\trecall: 18.18;\tf1: 27.59;\t7\n\tEngageInUnconventialMassViolence: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRE Evaluation in *** BOUNDARIES *** mode\nprocessed 114 sentences with 202 relations; found: 104 relations; correct: 43.\n\tALL\t TP: 43;\tFP: 40;\tFN: 152\n\t\t(m avg): precision: 51.81;\trecall: 22.05;\tf1: 30.94 (micro)\n\t\t(M avg): precision: 11.70;\trecall: 5.49;\tf1: 7.19 (Macro)\n\n\tMakePublicStatement: \tTP: 33;\tFP: 19;\tFN: 46;\tprecision: 63.46;\trecall: 41.77;\tf1: 50.38;\t52\n\tAppeal: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tExpressIntentToCooperate: \tTP: 0;\tFP: 0;\tFN: 5;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tConsult: \tTP: 1;\tFP: 0;\tFN: 3;\tprecision: 100.00;\trecall: 25.00;\tf1: 40.00;\t1\n\tEngageInDiplomaticCooperation: \tTP: 0;\tFP: 3;\tFN: 6;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t3\n\tEngageInMaterialCooperation: \tTP: 0;\tFP: 0;\tFN: 6;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tProvideAid: \tTP: 0;\tFP: 0;\tFN: 11;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tYield: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tInvestigate: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tDemand: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tDisapprove: \tTP: 2;\tFP: 10;\tFN: 16;\tprecision: 16.67;\trecall: 11.11;\tf1: 13.33;\t12\n\tReject: \tTP: 0;\tFP: 0;\tFN: 2;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tThreaten: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tExhibitMilitaryPosture: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tProtest: \tTP: 0;\tFP: 0;\tFN: 5;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tReduceRelations: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tCoerce: \tTP: 0;\tFP: 2;\tFN: 11;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t2\n\tAssault: \tTP: 0;\tFP: 0;\tFN: 10;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tFight: \tTP: 7;\tFP: 6;\tFN: 15;\tprecision: 53.85;\trecall: 31.82;\tf1: 40.00;\t13\n\tEngageInUnconventialMassViolence: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRE Evaluation in *** BOUNDARIES *** mode\nprocessed 114 sentences with 202 relations; found: 107 relations; correct: 59.\n\tALL\t TP: 59;\tFP: 30;\tFN: 136\n\t\t(m avg): precision: 66.29;\trecall: 30.26;\tf1: 41.55 (micro)\n\t\t(M avg): precision: 36.74;\trecall: 15.63;\tf1: 20.82 (Macro)\n\n\tMakePublicStatement: \tTP: 35;\tFP: 12;\tFN: 44;\tprecision: 74.47;\trecall: 44.30;\tf1: 55.56;\t47\n\tAppeal: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tExpressIntentToCooperate: \tTP: 0;\tFP: 0;\tFN: 5;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tConsult: \tTP: 1;\tFP: 0;\tFN: 3;\tprecision: 100.00;\trecall: 25.00;\tf1: 40.00;\t1\n\tEngageInDiplomaticCooperation: \tTP: 2;\tFP: 7;\tFN: 4;\tprecision: 22.22;\trecall: 33.33;\tf1: 26.67;\t9\n\tEngageInMaterialCooperation: \tTP: 0;\tFP: 0;\tFN: 6;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tProvideAid: \tTP: 1;\tFP: 1;\tFN: 10;\tprecision: 50.00;\trecall: 9.09;\tf1: 15.38;\t2\n\tYield: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tInvestigate: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tDemand: \tTP: 1;\tFP: 0;\tFN: 3;\tprecision: 100.00;\trecall: 25.00;\tf1: 40.00;\t1\n\tDisapprove: \tTP: 4;\tFP: 4;\tFN: 14;\tprecision: 50.00;\trecall: 22.22;\tf1: 30.77;\t8\n\tReject: \tTP: 1;\tFP: 0;\tFN: 1;\tprecision: 100.00;\trecall: 50.00;\tf1: 66.67;\t1\n\tThreaten: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tExhibitMilitaryPosture: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tProtest: \tTP: 2;\tFP: 0;\tFN: 3;\tprecision: 100.00;\trecall: 40.00;\tf1: 57.14;\t2\n\tReduceRelations: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tCoerce: \tTP: 2;\tFP: 1;\tFN: 9;\tprecision: 66.67;\trecall: 18.18;\tf1: 28.57;\t3\n\tAssault: \tTP: 0;\tFP: 1;\tFN: 10;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t1\n\tFight: \tTP: 10;\tFP: 4;\tFN: 12;\tprecision: 71.43;\trecall: 45.45;\tf1: 55.56;\t14\n\tEngageInUnconventialMassViolence: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRE Evaluation in *** BOUNDARIES *** mode\nprocessed 114 sentences with 202 relations; found: 155 relations; correct: 87.\n\tALL\t TP: 87;\tFP: 27;\tFN: 108\n\t\t(m avg): precision: 76.32;\trecall: 44.62;\tf1: 56.31 (micro)\n\t\t(M avg): precision: 47.69;\trecall: 23.68;\tf1: 29.50 (Macro)\n\n\tMakePublicStatement: \tTP: 45;\tFP: 8;\tFN: 34;\tprecision: 84.91;\trecall: 56.96;\tf1: 68.18;\t53\n\tAppeal: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tExpressIntentToCooperate: \tTP: 0;\tFP: 0;\tFN: 5;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tConsult: \tTP: 1;\tFP: 0;\tFN: 3;\tprecision: 100.00;\trecall: 25.00;\tf1: 40.00;\t1\n\tEngageInDiplomaticCooperation: \tTP: 1;\tFP: 1;\tFN: 5;\tprecision: 50.00;\trecall: 16.67;\tf1: 25.00;\t2\n\tEngageInMaterialCooperation: \tTP: 2;\tFP: 10;\tFN: 4;\tprecision: 16.67;\trecall: 33.33;\tf1: 22.22;\t12\n\tProvideAid: \tTP: 1;\tFP: 0;\tFN: 10;\tprecision: 100.00;\trecall: 9.09;\tf1: 16.67;\t1\n\tYield: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tInvestigate: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tDemand: \tTP: 2;\tFP: 1;\tFN: 2;\tprecision: 66.67;\trecall: 50.00;\tf1: 57.14;\t3\n\tDisapprove: \tTP: 8;\tFP: 2;\tFN: 10;\tprecision: 80.00;\trecall: 44.44;\tf1: 57.14;\t10\n\tReject: \tTP: 1;\tFP: 0;\tFN: 1;\tprecision: 100.00;\trecall: 50.00;\tf1: 66.67;\t1\n\tThreaten: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tExhibitMilitaryPosture: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tProtest: \tTP: 2;\tFP: 0;\tFN: 3;\tprecision: 100.00;\trecall: 40.00;\tf1: 57.14;\t2\n\tReduceRelations: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tCoerce: \tTP: 5;\tFP: 2;\tFN: 6;\tprecision: 71.43;\trecall: 45.45;\tf1: 55.56;\t7\n\tAssault: \tTP: 3;\tFP: 0;\tFN: 7;\tprecision: 100.00;\trecall: 30.00;\tf1: 46.15;\t3\n\tFight: \tTP: 16;\tFP: 3;\tFN: 6;\tprecision: 84.21;\trecall: 72.73;\tf1: 78.05;\t19\n\tEngageInUnconventialMassViolence: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRE Evaluation in *** BOUNDARIES *** mode\nprocessed 114 sentences with 202 relations; found: 145 relations; correct: 97.\n\tALL\t TP: 97;\tFP: 18;\tFN: 98\n\t\t(m avg): precision: 84.35;\trecall: 49.74;\tf1: 62.58 (micro)\n\t\t(M avg): precision: 63.60;\trecall: 32.58;\tf1: 41.87 (Macro)\n\n\tMakePublicStatement: \tTP: 50;\tFP: 10;\tFN: 29;\tprecision: 83.33;\trecall: 63.29;\tf1: 71.94;\t60\n\tAppeal: \tTP: 1;\tFP: 1;\tFN: 3;\tprecision: 50.00;\trecall: 25.00;\tf1: 33.33;\t2\n\tExpressIntentToCooperate: \tTP: 2;\tFP: 0;\tFN: 3;\tprecision: 100.00;\trecall: 40.00;\tf1: 57.14;\t2\n\tConsult: \tTP: 1;\tFP: 1;\tFN: 3;\tprecision: 50.00;\trecall: 25.00;\tf1: 33.33;\t2\n\tEngageInDiplomaticCooperation: \tTP: 1;\tFP: 0;\tFN: 5;\tprecision: 100.00;\trecall: 16.67;\tf1: 28.57;\t1\n\tEngageInMaterialCooperation: \tTP: 0;\tFP: 0;\tFN: 6;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tProvideAid: \tTP: 4;\tFP: 2;\tFN: 7;\tprecision: 66.67;\trecall: 36.36;\tf1: 47.06;\t6\n\tYield: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tInvestigate: \tTP: 2;\tFP: 1;\tFN: 1;\tprecision: 66.67;\trecall: 66.67;\tf1: 66.67;\t3\n\tDemand: \tTP: 2;\tFP: 0;\tFN: 2;\tprecision: 100.00;\trecall: 50.00;\tf1: 66.67;\t2\n\tDisapprove: \tTP: 7;\tFP: 1;\tFN: 11;\tprecision: 87.50;\trecall: 38.89;\tf1: 53.85;\t8\n\tReject: \tTP: 1;\tFP: 0;\tFN: 1;\tprecision: 100.00;\trecall: 50.00;\tf1: 66.67;\t1\n\tThreaten: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tExhibitMilitaryPosture: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tProtest: \tTP: 3;\tFP: 0;\tFN: 2;\tprecision: 100.00;\trecall: 60.00;\tf1: 75.00;\t3\n\tReduceRelations: \tTP: 1;\tFP: 0;\tFN: 2;\tprecision: 100.00;\trecall: 33.33;\tf1: 50.00;\t1\n\tCoerce: \tTP: 3;\tFP: 1;\tFN: 8;\tprecision: 75.00;\trecall: 27.27;\tf1: 40.00;\t4\n\tAssault: \tTP: 6;\tFP: 0;\tFN: 4;\tprecision: 100.00;\trecall: 60.00;\tf1: 75.00;\t6\n\tFight: \tTP: 13;\tFP: 1;\tFN: 9;\tprecision: 92.86;\trecall: 59.09;\tf1: 72.22;\t14\n\tEngageInUnconventialMassViolence: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRE Evaluation in *** BOUNDARIES *** mode\nprocessed 114 sentences with 202 relations; found: 156 relations; correct: 114.\n\tALL\t TP: 114;\tFP: 22;\tFN: 81\n\t\t(m avg): precision: 83.82;\trecall: 58.46;\tf1: 68.88 (micro)\n\t\t(M avg): precision: 62.95;\trecall: 45.07;\tf1: 50.52 (Macro)\n\n\tMakePublicStatement: \tTP: 45;\tFP: 3;\tFN: 34;\tprecision: 93.75;\trecall: 56.96;\tf1: 70.87;\t48\n\tAppeal: \tTP: 2;\tFP: 0;\tFN: 2;\tprecision: 100.00;\trecall: 50.00;\tf1: 66.67;\t2\n\tExpressIntentToCooperate: \tTP: 2;\tFP: 2;\tFN: 3;\tprecision: 50.00;\trecall: 40.00;\tf1: 44.44;\t4\n\tConsult: \tTP: 2;\tFP: 0;\tFN: 2;\tprecision: 100.00;\trecall: 50.00;\tf1: 66.67;\t2\n\tEngageInDiplomaticCooperation: \tTP: 4;\tFP: 1;\tFN: 2;\tprecision: 80.00;\trecall: 66.67;\tf1: 72.73;\t5\n\tEngageInMaterialCooperation: \tTP: 5;\tFP: 5;\tFN: 1;\tprecision: 50.00;\trecall: 83.33;\tf1: 62.50;\t10\n\tProvideAid: \tTP: 5;\tFP: 0;\tFN: 6;\tprecision: 100.00;\trecall: 45.45;\tf1: 62.50;\t5\n\tYield: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tInvestigate: \tTP: 1;\tFP: 0;\tFN: 2;\tprecision: 100.00;\trecall: 33.33;\tf1: 50.00;\t1\n\tDemand: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tDisapprove: \tTP: 13;\tFP: 3;\tFN: 5;\tprecision: 81.25;\trecall: 72.22;\tf1: 76.47;\t16\n\tReject: \tTP: 2;\tFP: 0;\tFN: 0;\tprecision: 100.00;\trecall: 100.00;\tf1: 100.00;\t2\n\tThreaten: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tExhibitMilitaryPosture: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tProtest: \tTP: 4;\tFP: 0;\tFN: 1;\tprecision: 100.00;\trecall: 80.00;\tf1: 88.89;\t4\n\tReduceRelations: \tTP: 1;\tFP: 1;\tFN: 2;\tprecision: 50.00;\trecall: 33.33;\tf1: 40.00;\t2\n\tCoerce: \tTP: 9;\tFP: 3;\tFN: 2;\tprecision: 75.00;\trecall: 81.82;\tf1: 78.26;\t12\n\tAssault: \tTP: 4;\tFP: 0;\tFN: 6;\tprecision: 100.00;\trecall: 40.00;\tf1: 57.14;\t4\n\tFight: \tTP: 15;\tFP: 4;\tFN: 7;\tprecision: 78.95;\trecall: 68.18;\tf1: 73.17;\t19\n\tEngageInUnconventialMassViolence: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRE Evaluation in *** BOUNDARIES *** mode\nprocessed 114 sentences with 202 relations; found: 159 relations; correct: 120.\n\tALL\t TP: 120;\tFP: 23;\tFN: 75\n\t\t(m avg): precision: 83.92;\trecall: 61.54;\tf1: 71.01 (micro)\n\t\t(M avg): precision: 70.68;\trecall: 54.63;\tf1: 59.39 (Macro)\n\n\tMakePublicStatement: \tTP: 47;\tFP: 5;\tFN: 32;\tprecision: 90.38;\trecall: 59.49;\tf1: 71.76;\t52\n\tAppeal: \tTP: 2;\tFP: 0;\tFN: 2;\tprecision: 100.00;\trecall: 50.00;\tf1: 66.67;\t2\n\tExpressIntentToCooperate: \tTP: 3;\tFP: 1;\tFN: 2;\tprecision: 75.00;\trecall: 60.00;\tf1: 66.67;\t4\n\tConsult: \tTP: 3;\tFP: 0;\tFN: 1;\tprecision: 100.00;\trecall: 75.00;\tf1: 85.71;\t3\n\tEngageInDiplomaticCooperation: \tTP: 1;\tFP: 0;\tFN: 5;\tprecision: 100.00;\trecall: 16.67;\tf1: 28.57;\t1\n\tEngageInMaterialCooperation: \tTP: 2;\tFP: 2;\tFN: 4;\tprecision: 50.00;\trecall: 33.33;\tf1: 40.00;\t4\n\tProvideAid: \tTP: 5;\tFP: 0;\tFN: 6;\tprecision: 100.00;\trecall: 45.45;\tf1: 62.50;\t5\n\tYield: \tTP: 0;\tFP: 1;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t1\n\tInvestigate: \tTP: 0;\tFP: 1;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t1\n\tDemand: \tTP: 3;\tFP: 0;\tFN: 1;\tprecision: 100.00;\trecall: 75.00;\tf1: 85.71;\t3\n\tDisapprove: \tTP: 13;\tFP: 6;\tFN: 5;\tprecision: 68.42;\trecall: 72.22;\tf1: 70.27;\t19\n\tReject: \tTP: 2;\tFP: 0;\tFN: 0;\tprecision: 100.00;\trecall: 100.00;\tf1: 100.00;\t2\n\tThreaten: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n\tExhibitMilitaryPosture: \tTP: 1;\tFP: 0;\tFN: 0;\tprecision: 100.00;\trecall: 100.00;\tf1: 100.00;\t1\n\tProtest: \tTP: 4;\tFP: 0;\tFN: 1;\tprecision: 100.00;\trecall: 80.00;\tf1: 88.89;\t4\n\tReduceRelations: \tTP: 3;\tFP: 1;\tFN: 0;\tprecision: 75.00;\trecall: 100.00;\tf1: 85.71;\t4\n\tCoerce: \tTP: 9;\tFP: 0;\tFN: 2;\tprecision: 100.00;\trecall: 81.82;\tf1: 90.00;\t9\n\tAssault: \tTP: 8;\tFP: 5;\tFN: 2;\tprecision: 61.54;\trecall: 80.00;\tf1: 69.57;\t13\n\tFight: \tTP: 14;\tFP: 1;\tFN: 8;\tprecision: 93.33;\trecall: 63.64;\tf1: 75.68;\t15\n\tEngageInUnconventialMassViolence: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{"id":"gecF7GsHHxcP"},"execution_count":null,"outputs":[]}]}