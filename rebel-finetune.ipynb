{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-10-25T16:19:37.454555Z","iopub.status.busy":"2022-10-25T16:19:37.453545Z","iopub.status.idle":"2022-10-25T16:19:39.520037Z","shell.execute_reply":"2022-10-25T16:19:39.518733Z","shell.execute_reply.started":"2022-10-25T16:19:37.454451Z"},"id":"i2Llr6fegsAk","outputId":"34617256-5947-46b8-abee-31bee7b7c68c","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'Thesis_RelationExtraction_PoliticsNews'...\n","remote: Enumerating objects: 219, done.\u001b[K\n","remote: Counting objects: 100% (219/219), done.\u001b[K\n","remote: Compressing objects: 100% (136/136), done.\u001b[K\n","remote: Total 219 (delta 91), reused 200 (delta 75), pack-reused 0\u001b[K\n","Receiving objects: 100% (219/219), 2.66 MiB | 12.80 MiB/s, done.\n","Resolving deltas: 100% (91/91), done.\n"]}],"source":["! git clone https://github.com/valentinwerner1/Thesis_RelationExtraction_PoliticsNews.git"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-10-25T16:19:39.522946Z","iopub.status.busy":"2022-10-25T16:19:39.522620Z","iopub.status.idle":"2022-10-25T16:19:40.751125Z","shell.execute_reply":"2022-10-25T16:19:40.749304Z","shell.execute_reply.started":"2022-10-25T16:19:39.522896Z"},"id":"Q4MafT2BhDB3","outputId":"7aadeeac-3c78-4d61-885d-207fed48dbf0","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Branch 'dev' set up to track remote branch 'dev' from 'origin'.\n","Switched to a new branch 'dev'\n"]}],"source":["!cd (\"Thesis_RelationExtraction_PoliticsNews\") && git checkout dev"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-10-25T16:19:40.753971Z","iopub.status.busy":"2022-10-25T16:19:40.753374Z","iopub.status.idle":"2022-10-25T16:20:22.352340Z","shell.execute_reply":"2022-10-25T16:20:22.351078Z","shell.execute_reply.started":"2022-10-25T16:19:40.753918Z"},"id":"4b-3jSUmh-HQ","outputId":"3f0df5a1-46d3-49ba-f626-33d707b183c5","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.10.1)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.13.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.12)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: pytorch_lightning in /opt/conda/lib/python3.7/site-packages (1.7.7)\n","Requirement already satisfied: PyYAML>=5.4 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (6.0)\n","Requirement already satisfied: packaging>=17.0 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (21.3)\n","Requirement already satisfied: tensorboard>=2.9.1 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (2.10.1)\n","Requirement already satisfied: torchmetrics>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (0.10.0)\n","Requirement already satisfied: numpy>=1.17.2 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (1.21.6)\n","Requirement already satisfied: pyDeprecate>=0.3.1 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (0.3.2)\n","Requirement already satisfied: torch>=1.9.* in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (1.11.0)\n","Requirement already satisfied: typing-extensions>=4.0.0 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (4.1.1)\n","Requirement already satisfied: tqdm>=4.57.0 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (4.64.0)\n","Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from pytorch_lightning) (2022.8.2)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.28.1)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.7/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.8.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=17.0->pytorch_lightning) (3.0.9)\n","Requirement already satisfied: wheel>=0.26 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (0.37.1)\n","Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (2.2.2)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (0.4.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (1.8.1)\n","Requirement already satisfied: setuptools>=41.0.0 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (59.8.0)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (1.35.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (1.43.0)\n","Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (3.3.7)\n","Requirement already satisfied: absl-py>=0.4 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (0.15.0)\n","Requirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.7/site-packages (from tensorboard>=2.9.1->pytorch_lightning) (3.19.4)\n","Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from absl-py>=0.4->tensorboard>=2.9.1->pytorch_lightning) (1.15.0)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.4.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.7.2)\n","Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (0.13.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (4.0.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (6.0.2)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.1.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (4.8)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (0.2.7)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch_lightning) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard>=2.9.1->pytorch_lightning) (4.13.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.26.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2022.9.24)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.7/site-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->pytorch_lightning) (2.1.1)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.9.1->pytorch_lightning) (3.8.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.9.1->pytorch_lightning) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.9.1->pytorch_lightning) (3.2.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (2.1.0)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2022.8.2)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.21.6)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.13)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\n","Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.28.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.64.0)\n","Requirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (21.3)\n","Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\n","Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (5.0.0)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.10.1)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.13.0)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.5.1)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.1)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\n","Requirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (2.1.0)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.1.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.0)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (21.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (3.0.9)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2022.9.24)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.12)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.3)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.8.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2022.1)\n","Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0mRequirement already satisfied: wandb in /opt/conda/lib/python3.7/site-packages (0.12.21)\n","Requirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.9.10)\n","Requirement already satisfied: setproctitle in /opt/conda/lib/python3.7/site-packages (from wandb) (1.3.2)\n","Requirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (5.9.1)\n","Requirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.28.1)\n","Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.19.4)\n","Requirement already satisfied: shortuuid>=0.5.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.0.9)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (8.0.4)\n","Requirement already satisfied: PyYAML in /opt/conda/lib/python3.7/site-packages (from wandb) (6.0)\n","Requirement already satisfied: GitPython>=1.0.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (3.1.27)\n","Requirement already satisfied: promise<3,>=2.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (2.3)\n","Requirement already satisfied: six>=1.13.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (1.15.0)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.7/site-packages (from wandb) (0.4.0)\n","Requirement already satisfied: pathtools in /opt/conda/lib/python3.7/site-packages (from wandb) (0.1.2)\n","Requirement already satisfied: setuptools in /opt/conda/lib/python3.7/site-packages (from wandb) (59.8.0)\n","Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from Click!=8.0.0,>=7.0->wandb) (4.13.0)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (1.26.12)\n","Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2.1.0)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (3.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3,>=2.0.0->wandb) (2022.9.24)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.7/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (3.0.5)\n","Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->Click!=8.0.0,>=7.0->wandb) (3.8.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}],"source":["!pip install transformers\n","!pip install pytorch_lightning\n","!pip install datasets\n","!pip install wandb"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2022-10-25T16:20:22.357118Z","iopub.status.busy":"2022-10-25T16:20:22.356798Z","iopub.status.idle":"2022-10-25T16:20:33.456332Z","shell.execute_reply":"2022-10-25T16:20:33.455273Z","shell.execute_reply.started":"2022-10-25T16:20:22.357087Z"},"id":"I7G0TrPChwqV","trusted":true},"outputs":[],"source":["import transformers \n","import pandas as pd\n","import numpy as np\n","import json\n","import os\n","\n","\n","from typing import Any, Union, List, Optional\n","\n","import torch\n","from torch.utils.data import DataLoader\n","from torch.optim import AdamW\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n","from pytorch_lightning.callbacks import LearningRateMonitor\n","import datasets\n","from transformers import (\n","    AutoConfig,\n","    AutoModelForSeq2SeqLM,\n","    AutoTokenizer,\n","    DataCollatorForSeq2Seq,\n","    default_data_collator,\n","    set_seed,\n",")\n","\n","from torch.optim import lr_scheduler\n","import wandb\n","from pytorch_lightning.loggers.wandb import WandbLogger"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import pandas as pd"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":337},"execution":{"iopub.execute_input":"2022-10-25T16:20:33.458626Z","iopub.status.busy":"2022-10-25T16:20:33.457841Z","iopub.status.idle":"2022-10-25T16:20:33.571707Z","shell.execute_reply":"2022-10-25T16:20:33.570580Z","shell.execute_reply.started":"2022-10-25T16:20:33.458585Z"},"id":"aV3eJje6h9Ms","outputId":"f94d36d7-c5cf-49ba-90a4-7da89ea77ba8","trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>text</th>\n","      <th>article_id</th>\n","      <th>_input_hash</th>\n","      <th>_task_hash</th>\n","      <th>_is_binary</th>\n","      <th>spans</th>\n","      <th>tokens</th>\n","      <th>_view_id</th>\n","      <th>relations</th>\n","      <th>answer</th>\n","      <th>_timestamp</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Ukraine war: Russian tactics on eastern front ...</td>\n","      <td>0</td>\n","      <td>1192826571</td>\n","      <td>1725000051</td>\n","      <td>False</td>\n","      <td>[{'start': 0, 'end': 7, 'token_start': 0, 'tok...</td>\n","      <td>[{'text': 'Ukraine', 'start': 0, 'end': 7, 'id...</td>\n","      <td>relations</td>\n","      <td>[{'head': 3, 'child': 0, 'head_span': {'start'...</td>\n","      <td>accept</td>\n","      <td>1666853824</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Lee Jae-yong: Samsung appoints Lee Jae-yong to...</td>\n","      <td>2</td>\n","      <td>-386898449</td>\n","      <td>495824270</td>\n","      <td>False</td>\n","      <td>[{'start': 0, 'end': 12, 'token_start': 0, 'to...</td>\n","      <td>[{'text': 'Lee', 'start': 0, 'end': 3, 'id': 0...</td>\n","      <td>relations</td>\n","      <td>[{'head': 31, 'child': 18, 'head_span': {'star...</td>\n","      <td>accept</td>\n","      <td>1666854104</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>New Zealand Instagram couple 'relieved' after ...</td>\n","      <td>3</td>\n","      <td>94784817</td>\n","      <td>-1047716132</td>\n","      <td>False</td>\n","      <td>[{'start': 0, 'end': 28, 'token_start': 0, 'to...</td>\n","      <td>[{'text': 'New', 'start': 0, 'end': 3, 'id': 0...</td>\n","      <td>relations</td>\n","      <td>[{'head': 3, 'child': 9, 'head_span': {'start'...</td>\n","      <td>accept</td>\n","      <td>1666854141</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Iran protests: Police fire on Mahsa Amini mour...</td>\n","      <td>4</td>\n","      <td>-1204324385</td>\n","      <td>1067222802</td>\n","      <td>False</td>\n","      <td>[{'start': 0, 'end': 4, 'token_start': 0, 'tok...</td>\n","      <td>[{'text': 'Iran', 'start': 0, 'end': 4, 'id': ...</td>\n","      <td>relations</td>\n","      <td>[{'head': 3, 'child': 8, 'head_span': {'start'...</td>\n","      <td>accept</td>\n","      <td>1666854246</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Germany plans to legalise recreational cannabi...</td>\n","      <td>5</td>\n","      <td>1523244431</td>\n","      <td>980527859</td>\n","      <td>False</td>\n","      <td>[{'start': 0, 'end': 7, 'token_start': 0, 'tok...</td>\n","      <td>[{'text': 'Germany', 'start': 0, 'end': 7, 'id...</td>\n","      <td>relations</td>\n","      <td>[{'head': 0, 'child': 5, 'head_span': {'start'...</td>\n","      <td>accept</td>\n","      <td>1666854286</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                text  article_id  _input_hash  \\\n","0  Ukraine war: Russian tactics on eastern front ...           0   1192826571   \n","2  Lee Jae-yong: Samsung appoints Lee Jae-yong to...           2   -386898449   \n","3  New Zealand Instagram couple 'relieved' after ...           3     94784817   \n","4  Iran protests: Police fire on Mahsa Amini mour...           4  -1204324385   \n","5  Germany plans to legalise recreational cannabi...           5   1523244431   \n","\n","   _task_hash  _is_binary                                              spans  \\\n","0  1725000051       False  [{'start': 0, 'end': 7, 'token_start': 0, 'tok...   \n","2   495824270       False  [{'start': 0, 'end': 12, 'token_start': 0, 'to...   \n","3 -1047716132       False  [{'start': 0, 'end': 28, 'token_start': 0, 'to...   \n","4  1067222802       False  [{'start': 0, 'end': 4, 'token_start': 0, 'tok...   \n","5   980527859       False  [{'start': 0, 'end': 7, 'token_start': 0, 'tok...   \n","\n","                                              tokens   _view_id  \\\n","0  [{'text': 'Ukraine', 'start': 0, 'end': 7, 'id...  relations   \n","2  [{'text': 'Lee', 'start': 0, 'end': 3, 'id': 0...  relations   \n","3  [{'text': 'New', 'start': 0, 'end': 3, 'id': 0...  relations   \n","4  [{'text': 'Iran', 'start': 0, 'end': 4, 'id': ...  relations   \n","5  [{'text': 'Germany', 'start': 0, 'end': 7, 'id...  relations   \n","\n","                                           relations  answer  _timestamp  \n","0  [{'head': 3, 'child': 0, 'head_span': {'start'...  accept  1666853824  \n","2  [{'head': 31, 'child': 18, 'head_span': {'star...  accept  1666854104  \n","3  [{'head': 3, 'child': 9, 'head_span': {'start'...  accept  1666854141  \n","4  [{'head': 3, 'child': 8, 'head_span': {'start'...  accept  1666854246  \n","5  [{'head': 0, 'child': 5, 'head_span': {'start'...  accept  1666854286  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["read = pd.read_json(r\"C:\\Users\\svawe\\Thesis_RelationExtraction_PoliticsNews\\data_src\\raw\\out_label\\summaries_out1.json\", lines = True)\n","df = read[read.answer == \"accept\"]\n","df.head()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2022-10-25T16:20:33.573758Z","iopub.status.busy":"2022-10-25T16:20:33.573276Z","iopub.status.idle":"2022-10-25T16:20:33.591980Z","shell.execute_reply":"2022-10-25T16:20:33.591141Z","shell.execute_reply.started":"2022-10-25T16:20:33.573717Z"},"id":"3bH0NG1zkfUZ","trusted":true},"outputs":[],"source":["#fix sentence_id mistake\n","df = df.astype({\"sentence_id\":str,\"article_id\":str}) \n","\n","def fix_id(row):\n","    return row.sentence_id[:len(row.article_id)] + \"_\" + row.sentence_id[len(row.article_id):]\n","\n","df[\"sentence_id\"] = df.apply(fix_id, axis = 1)\n","\n","#add annotator column in case more annotators will be used\n","df[\"annotator\"] = 0"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2022-10-25T16:20:33.593971Z","iopub.status.busy":"2022-10-25T16:20:33.593586Z","iopub.status.idle":"2022-10-25T16:20:33.606813Z","shell.execute_reply":"2022-10-25T16:20:33.605726Z","shell.execute_reply.started":"2022-10-25T16:20:33.593915Z"},"id":"uZHFEiugkrGK","trusted":true},"outputs":[],"source":["#for EDA\n","df[\"relations_count\"] = df.relations.apply(lambda x: len(x))"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2022-10-25T16:20:33.608930Z","iopub.status.busy":"2022-10-25T16:20:33.608536Z","iopub.status.idle":"2022-10-25T16:20:33.621777Z","shell.execute_reply":"2022-10-25T16:20:33.620810Z","shell.execute_reply.started":"2022-10-25T16:20:33.608894Z"},"id":"6ZPJQ0PskslP","trusted":true},"outputs":[],"source":["#prune df\n","df = df.drop(columns = [\"_input_hash\",\"_task_hash\",\"_is_binary\",\"tokens\",\"_view_id\",\"answer\",\"_timestamp\"])\n","df = df.reset_index().drop(columns = [\"index\"])"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-10-25T16:20:33.623989Z","iopub.status.busy":"2022-10-25T16:20:33.623695Z","iopub.status.idle":"2022-10-25T16:20:33.644963Z","shell.execute_reply":"2022-10-25T16:20:33.644081Z","shell.execute_reply.started":"2022-10-25T16:20:33.623923Z"},"id":"0kZi5qlVkuFu","outputId":"afeeaedf-ac3e-41ee-e4f9-00d2c3ba3d1c","trusted":true},"outputs":[{"data":{"text/plain":["['Appeal',\n"," 'Investigate',\n"," 'EngageInMaterialCooperation',\n"," 'Assault',\n"," 'ReduceRelations',\n"," 'ProvideAid',\n"," 'Reject',\n"," 'EngageInDiplomaticCooperation',\n"," 'ExpressIntentToCooperate',\n"," 'Yield',\n"," 'Coerce',\n"," 'Protest',\n"," 'Fight',\n"," 'MakePublicStatement',\n"," 'Disapprove',\n"," 'Demand',\n"," 'ExhibitMilitaryPosture',\n"," 'Consult']"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["#get data into needed format and create new DataFrame\n","relations = []\n","for index,row in df.iterrows():\n","    for rel in row.relations:\n","        relations.append(rel['label'])\n","\n","relations = list(set(relations))\n","relations"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2022-10-25T16:20:33.650075Z","iopub.status.busy":"2022-10-25T16:20:33.649391Z","iopub.status.idle":"2022-10-25T16:20:33.674191Z","shell.execute_reply":"2022-10-25T16:20:33.673120Z","shell.execute_reply.started":"2022-10-25T16:20:33.650037Z"},"id":"H4cXawHhkvT9","trusted":true},"outputs":[],"source":["#get data into needed format and create new DataFrame\n","data = []\n","for index,row in df.iterrows():\n","    rel_list = []\n","    for rel in row.relations:\n","        subj = row.text[rel[\"head_span\"][\"start\"]:rel[\"head_span\"][\"end\"]]\n","        obj = row.text[rel[\"child_span\"][\"start\"]:rel[\"child_span\"][\"end\"]]\n","        rel_list.append(f\"<triplet> {subj} <subj> {obj} <obj> {rel['label']}\")\n","    data.append({\"doc_id\":row.sentence_id, \"text\": row.text, \"triplets\": \" \".join(rel_list)})\n","\n","data = pd.DataFrame(data, columns = [\"doc_id\",\"text\",\"triplets\"])"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-10-25T16:20:33.677732Z","iopub.status.busy":"2022-10-25T16:20:33.676691Z","iopub.status.idle":"2022-10-25T16:20:33.729175Z","shell.execute_reply":"2022-10-25T16:20:33.728200Z","shell.execute_reply.started":"2022-10-25T16:20:33.677692Z"},"id":"BYOOvGK6kwtj","outputId":"d6de6d0f-d61c-4d6d-a437-7fbdd605dd37","trusted":true},"outputs":[{"data":{"text/plain":["DatasetDict({\n","    train: Dataset({\n","        features: ['text', 'triplets'],\n","        num_rows: 114\n","    })\n","    val: Dataset({\n","        features: ['text', 'triplets'],\n","        num_rows: 35\n","    })\n","    test: Dataset({\n","        features: ['text', 'triplets'],\n","        num_rows: 15\n","    })\n","})"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["ds = datasets.Dataset.from_pandas(data.drop(columns=[\"doc_id\"]))\n","\n","#### just random splits for testing, change later to proper splitting\n","###### and then change later again for CV\n","split = ds.train_test_split(test_size = 0.3)\n","split_val = split[\"test\"].train_test_split(test_size = 0.3)\n","ds = datasets.DatasetDict({\"train\":split[\"train\"],\"val\":split_val[\"train\"], \"test\":split_val[\"test\"]})\n","\n","#ds = ds.with_format(\"torch\")\n","ds"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2022-10-25T16:20:33.730856Z","iopub.status.busy":"2022-10-25T16:20:33.730511Z","iopub.status.idle":"2022-10-25T16:20:33.740642Z","shell.execute_reply":"2022-10-25T16:20:33.739693Z","shell.execute_reply.started":"2022-10-25T16:20:33.730816Z"},"id":"ERXMfgkhkyFS","trusted":true},"outputs":[],"source":["class conf:\n","    #general\n","    seed = 0\n","    gpus = 1\n","    \n","    #input\n","    batch_size = 8\n","    max_length = 128\n","    ignore_pad_token_for_loss = True\n","    use_fast_tokenizer = True\n","    gradient_acc_steps = 1\n","    gradient_clip_value = 10.0\n","    load_workers = 8\n","\n","    #optimizer\n","    lr = 0.00005\n","    weight_decay = 0.01\n","    beta1 = 0.9\n","    beta2 = 0.999\n","    epsilon = 0.00000001\n","    warmup_steps = 1000\n","\n","    #training\n","    max_steps = 1200000\n","    samples_interval = 1000\n","\n","    monitor_var  = \"val_loss\"\n","    monitor_var_mode = \"min\"\n","    # val_check_interval = 0.5\n","    # val_percent_check = 0.1\n","\n","    model_name = \"model1.pth\"\n","    checkpoint_path = f\"models/{model_name}\"\n","    save_top_k = 1\n","\n","    early_stopping = False\n","    patience = \"5\"\n","\n","    length_penalty = 0\n","    no_repeat_ngram_size = 0\n","    num_beams = 3\n","    precision = 16\n","    amp_level = None\n","\n","    "]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2022-10-25T16:20:33.744246Z","iopub.status.busy":"2022-10-25T16:20:33.743919Z","iopub.status.idle":"2022-10-25T16:20:33.756651Z","shell.execute_reply":"2022-10-25T16:20:33.755507Z","shell.execute_reply.started":"2022-10-25T16:20:33.744219Z"},"id":"mN1ASyHskzec","trusted":true},"outputs":[],"source":["class GetData(pl.LightningDataModule):\n","    def __init__(self, conf: conf, tokenizer: AutoTokenizer, model: AutoModelForSeq2SeqLM):\n","        \"\"\"init params from config\"\"\"\n","        super().__init__()\n","        self.tokenizer = tokenizer\n","        self.model = model\n","        self.datasets = ds\n","        \n","        # Data collator\n","        label_pad_token_id = -100                       \n","        self.data_collator = DataCollatorForSeq2Seq(self.tokenizer, self.model, label_pad_token_id=label_pad_token_id, padding = True)\n","\n","    def preprocess_function(self, data):\n","        \"\"\"tokenize, pad, truncate\"\"\"\n","        #split into input and labels\n","        inputs = data[\"text\"]       \n","        outputs = data[\"triplets\"]\n","\n","        #process input\n","        model_inputs = self.tokenizer(inputs, max_length = conf.max_length, padding = True, truncation = True)\n","\n","        #process labels\n","        with self.tokenizer.as_target_tokenizer():\n","            labels = self.tokenizer(outputs, max_length = conf.max_length, padding = True, truncation = True)\n","\n","        model_inputs[\"labels\"] = labels[\"input_ids\"]\n","        return model_inputs\n","        \n","    #apply the preprocessing and load data\n","    def train_dataloader(self, *args, **kwargs): \n","        self.train_dataset = self.datasets[\"train\"]\n","        self.train_dataset = self.train_dataset.map(self.preprocess_function, remove_columns = [\"text\", \"triplets\"], batched = True)\n","        return DataLoader(self.train_dataset, batch_size = conf.batch_size, collate_fn = self.data_collator, shuffle = True, num_workers= conf.load_workers)\n","\n","    #to check if learning works (val on train)\n","    def val_dataloader(self, *args, **kwargs): \n","        self.train_dataset = self.datasets[\"train\"]\n","        self.train_dataset = self.train_dataset.map(self.preprocess_function, remove_columns = [\"text\", \"triplets\"], batched = True)\n","        return DataLoader(self.train_dataset, batch_size = conf.batch_size, collate_fn = self.data_collator, shuffle = True, num_workers= conf.load_workers)\n","    \n","   # def val_dataloader(self, *args, **kwargs): \n","   #     self.eval_dataset = self.datasets[\"val\"]\n","   #     self.eval_dataset = self.eval_dataset.map(self.preprocess_function, remove_columns = [\"text\", \"triplets\"], batched = True)\n","   #     return DataLoader(self.eval_dataset, batch_size = conf.batch_size, collate_fn = self.data_collator, num_workers= conf.load_workers)\n","\n","    def test_dataloader(self, *args, **kwargs): \n","        self.test_dataset = self.datasets[\"test\"]\n","        self.test_dataset = self.test_dataset.map(self.preprocess_function,  remove_columns = [\"text\", \"triplets\"], batched = True)\n","        return DataLoader(self.test_dataset, batch_size = conf.batch_size, collate_fn = self.data_collator, num_workers= conf.load_workers)"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2022-10-25T16:20:33.759131Z","iopub.status.busy":"2022-10-25T16:20:33.758436Z","iopub.status.idle":"2022-10-25T16:20:33.776943Z","shell.execute_reply":"2022-10-25T16:20:33.775824Z","shell.execute_reply.started":"2022-10-25T16:20:33.759092Z"},"id":"pXLqZoiyuVNN","outputId":"999d901d-8f6b-48ff-ab53-12f7df674744","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Available objects for config:\n","     AliasManager\n","     DisplayFormatter\n","     HistoryManager\n","     IPCompleter\n","     IPKernelApp\n","     InlineBackend\n","     LoggingMagics\n","     MagicsManager\n","     OSMagics\n","     PrefilterManager\n","     ScriptMagics\n","     SqlMagic\n","     StoreMagics\n","     ZMQInteractiveShell\n"]}],"source":["config"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2022-10-25T16:20:33.778975Z","iopub.status.busy":"2022-10-25T16:20:33.778535Z","iopub.status.idle":"2022-10-25T16:20:33.793218Z","shell.execute_reply":"2022-10-25T16:20:33.792223Z","shell.execute_reply.started":"2022-10-25T16:20:33.778924Z"},"id":"Y9ELEKQHk2-f","trusted":true},"outputs":[],"source":["#from REBEL\n","from typing import Sequence\n","\n","import torch\n","from pytorch_lightning import Callback, LightningModule, Trainer\n","from torch import nn\n","import pandas as pd\n","from torch.nn.utils.rnn import pad_sequence\n","import wandb\n","\n","class GenerateTextSamplesCallback(Callback):\n","    \"\"\"\n","    PL Callback to generate triplets along training\n","    \"\"\"\n","\n","    def __init__(self, logging_batch_interval):\n","        \"\"\"\n","        Args:\n","            logging_batch_interval: How frequently to inspect/potentially plot something\n","        \"\"\"\n","        super().__init__()\n","        self.logging_batch_interval = logging_batch_interval\n","\n","    def on_train_batch_end(self,trainer: Trainer,pl_module: LightningModule, outputs: Sequence, batch: Sequence, batch_idx: int) -> None:\n","        wandb_table = wandb.Table(columns=[\"Source\", \"Pred\", \"Gold\"])\n","        # pl_module.logger.info(\"Executing translation callback\")\n","        labels = batch.pop(\"labels\")\n","        gen_kwargs = {\n","            \"max_length\": conf.max_length,\n","            \"early_stopping\": False,\n","            \"no_repeat_ngram_size\": 0,\n","            \"num_beams\": conf.num_beams\n","        }\n","        pl_module.eval()\n","\n","        decoder_inputs = torch.roll(labels, 1, 1)[:,0:2]\n","        decoder_inputs[:, 0] = 0\n","        generated_tokens = pl_module.model.generate(\n","            batch[\"input_ids\"].to(pl_module.model.device),\n","            attention_mask=batch[\"attention_mask\"].to(pl_module.model.device),\n","            decoder_input_ids=decoder_inputs.to(pl_module.model.device),\n","            **gen_kwargs,\n","        )\n","        # in case the batch is shorter than max length, the output should be padded\n","        if generated_tokens.shape[-1] < gen_kwargs[\"max_length\"]:\n","            generated_tokens = pl_module._pad_tensors_to_max_len(generated_tokens, gen_kwargs[\"max_length\"])\n","        pl_module.train()\n","        decoded_preds = pl_module.tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n","\n","        #If ignore pad token for loss == True \n","            # Replace -100 in the labels as we can't decode them.\n","        labels = torch.where(labels != -100, labels, pl_module.tokenizer.pad_token_id)\n","\n","        decoded_labels = pl_module.tokenizer.batch_decode(labels, skip_special_tokens=False)\n","        decoded_inputs = pl_module.tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=False)\n","\n","        # pl_module.logger.experiment.log_text('generated samples', '\\n'.join(decoded_preds).replace('<pad>', ''))\n","        # pl_module.logger.experiment.log_text('original samples', '\\n'.join(decoded_labels).replace('<pad>', ''))\n","        for source, translation, gold_output in zip(decoded_inputs, decoded_preds, decoded_labels):\n","            wandb_table.add_data(\n","                source.replace('<pad>', ''), translation.replace('<pad>', ''), gold_output.replace('<pad>', '')\n","            )\n","        pl_module.logger.experiment.log({\"Triplets\": wandb_table})"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2022-10-25T16:20:33.794970Z","iopub.status.busy":"2022-10-25T16:20:33.794421Z","iopub.status.idle":"2022-10-25T16:20:33.808703Z","shell.execute_reply":"2022-10-25T16:20:33.807745Z","shell.execute_reply.started":"2022-10-25T16:20:33.794915Z"},"id":"5NAqweGsk4Op","trusted":true},"outputs":[],"source":["#from REBEL\n","def shift_tokens_left(input_ids: torch.Tensor, pad_token_id: int):\n","    \"\"\"\n","    Shift input ids one token to the right.\n","    \"\"\"\n","    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n","    shifted_input_ids[:, :-1] = input_ids[:, 1:].clone()\n","    shifted_input_ids[:, -1] = pad_token_id\n","\n","    assert pad_token_id is not None, \"self.model.config.pad_token_id has to be defined.\"\n","\n","    return shifted_input_ids"]},{"cell_type":"code","execution_count":17,"metadata":{"execution":{"iopub.execute_input":"2022-10-25T16:20:33.810667Z","iopub.status.busy":"2022-10-25T16:20:33.810243Z","iopub.status.idle":"2022-10-25T16:20:33.822269Z","shell.execute_reply":"2022-10-25T16:20:33.821222Z","shell.execute_reply.started":"2022-10-25T16:20:33.810629Z"},"id":"91a0v9t_k-Hx","trusted":true},"outputs":[],"source":["#from REBEL\n","\n","##### can maybe be rewritten more effective\n","def extract_triplets(text):\n","    triplets = []\n","    relation, subject, relation, object_ = '', '', '', ''\n","    text = text.strip()\n","    current = 'x'\n","    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n","        if token == \"<triplet>\":\n","            current = 't'\n","            if relation != '':\n","                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n","                relation = ''\n","            subject = ''\n","        elif token == \"<subj>\":\n","            current = 's'\n","            if relation != '':\n","                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n","            object_ = ''\n","        elif token == \"<obj>\":\n","            current = 'o'\n","            relation = ''\n","        else:\n","            if current == 't':\n","                subject += ' ' + token\n","            elif current == 's':\n","                object_ += ' ' + token\n","            elif current == 'o':\n","                relation += ' ' + token\n","    if subject != '' and relation != '' and object_ != '':\n","        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n","    return triplets"]},{"cell_type":"code","execution_count":18,"metadata":{"execution":{"iopub.execute_input":"2022-10-25T16:20:33.824588Z","iopub.status.busy":"2022-10-25T16:20:33.824154Z","iopub.status.idle":"2022-10-25T16:20:33.863195Z","shell.execute_reply":"2022-10-25T16:20:33.862197Z","shell.execute_reply.started":"2022-10-25T16:20:33.824550Z"},"id":"MRoOQCczqp4k","trusted":true},"outputs":[],"source":["#from REBEL \n","def score(key, prediction, verbose=False):\n","    correct_by_relation = Counter()\n","    guessed_by_relation = Counter()\n","    gold_by_relation    = Counter()\n","\n","    # Loop over the data to compute a score\n","    for row in range(len(prediction)):\n","        gold = key[row]\n","        guess = prediction[row]\n","         \n","        if gold == NO_RELATION and guess == NO_RELATION:\n","            pass\n","        elif gold == NO_RELATION and guess != NO_RELATION:\n","            guessed_by_relation[guess] += 1\n","        elif gold != NO_RELATION and guess == NO_RELATION:\n","            gold_by_relation[gold] += 1\n","        elif gold != NO_RELATION and guess != NO_RELATION:\n","            guessed_by_relation[guess] += 1\n","            gold_by_relation[gold] += 1\n","            if gold == guess:\n","                correct_by_relation[guess] += 1\n","\n","    # Print verbose information\n","    if verbose:\n","        print(\"Per-relation statistics:\")\n","        relations = gold_by_relation.keys()\n","        longest_relation = 0\n","        for relation in sorted(relations):\n","            longest_relation = max(len(relation), longest_relation)\n","        for relation in sorted(relations):\n","            # (compute the score)\n","            correct = correct_by_relation[relation]\n","            guessed = guessed_by_relation[relation]\n","            gold    = gold_by_relation[relation]\n","            prec = 1.0\n","            if guessed > 0:\n","                prec = float(correct) / float(guessed)\n","            recall = 0.0\n","            if gold > 0:\n","                recall = float(correct) / float(gold)\n","            f1 = 0.0\n","            if prec + recall > 0:\n","                f1 = 2.0 * prec * recall / (prec + recall)\n","            # (print the score)\n","            sys.stdout.write((\"{:<\" + str(longest_relation) + \"}\").format(relation))\n","            sys.stdout.write(\"  P: \")\n","            if prec < 0.1: sys.stdout.write(' ')\n","            if prec < 1.0: sys.stdout.write(' ')\n","            sys.stdout.write(\"{:.2%}\".format(prec))\n","            sys.stdout.write(\"  R: \")\n","            if recall < 0.1: sys.stdout.write(' ')\n","            if recall < 1.0: sys.stdout.write(' ')\n","            sys.stdout.write(\"{:.2%}\".format(recall))\n","            sys.stdout.write(\"  F1: \")\n","            if f1 < 0.1: sys.stdout.write(' ')\n","            if f1 < 1.0: sys.stdout.write(' ')\n","            sys.stdout.write(\"{:.2%}\".format(f1))\n","            sys.stdout.write(\"  #: %d\" % gold)\n","            sys.stdout.write(\"\\n\")\n","        print(\"\")\n","\n","    # Print the aggregate score\n","    if verbose:\n","        print(\"Final Score:\")\n","    prec_micro = 1.0\n","    if sum(guessed_by_relation.values()) > 0:\n","        prec_micro   = float(sum(correct_by_relation.values())) / float(sum(guessed_by_relation.values()))\n","    recall_micro = 0.0\n","    if sum(gold_by_relation.values()) > 0:\n","        recall_micro = float(sum(correct_by_relation.values())) / float(sum(gold_by_relation.values()))\n","    f1_micro = 0.0\n","    if prec_micro + recall_micro > 0.0:\n","        f1_micro = 2.0 * prec_micro * recall_micro / (prec_micro + recall_micro)\n","    print(\"Precision (micro): {:.3%}\".format(prec_micro))\n","    print(\"   Recall (micro): {:.3%}\".format(recall_micro))\n","    print(\"       F1 (micro): {:.3%}\".format(f1_micro))\n","    return prec_micro, recall_micro, f1_micro\n","\n","'''Adapted from: https://github.com/btaille/sincere/blob/6f5472c5aeaf7ef7765edf597ede48fdf1071712/code/utils/evaluation.py'''\n","def re_score(pred_relations, gt_relations, relation_types, mode=\"boundaries\"):\n","    \"\"\"Evaluate RE predictions\n","    Args:\n","        pred_relations (list) :  list of list of predicted relations (several relations in each sentence)\n","        gt_relations (list) :    list of list of ground truth relations\n","            rel = { \"head\": (start_idx (inclusive), end_idx (exclusive)),\n","                    \"tail\": (start_idx (inclusive), end_idx (exclusive)),\n","                    \"head_type\": ent_type,\n","                    \"tail_type\": ent_type,\n","                    \"type\": rel_type}\n","        vocab (Vocab) :         dataset vocabulary\n","        mode (str) :            in 'strict' or 'boundaries' \"\"\"\n","\n","    assert mode in [\"strict\", \"boundaries\"]\n","    relation_types = [\"MakePublicStatement\",\"Appeal\",\"ExpressIntentToCooperate\",\"Consult\",\"EngageInDiplomaticCooperation\",\"EngageInMaterialCooperation\",\"ProvideAid\",\"Yield\",\"Investigate\",\"Demand\",\"Disapprove\",\"Reject\",\"Threaten\",\"ExhibitMilitaryPosture\",\"Protest\",\"ReduceRelations\",\"Coerce\",\"Assault\",\"Fight\",\"EngageInUnconventialMassViolence\"]\n","        \n","    # relation_types = [v for v in relation_types if not v == \"None\"]\n","    scores = {rel: {\"tp\": 0, \"fp\": 0, \"fn\": 0} for rel in relation_types + [\"ALL\"]}\n","\n","    # Count GT relations and Predicted relations\n","    n_sents = len(gt_relations)\n","    n_rels = sum([len([rel for rel in sent]) for sent in gt_relations])\n","    n_found = sum([len([rel for rel in sent]) for sent in pred_relations])\n","\n","    # Count TP, FP and FN per type\n","    for pred_sent, gt_sent in zip(pred_relations, gt_relations):\n","        for rel_type in relation_types:\n","            # strict mode takes argument types into account\n","            if mode == \"strict\":\n","                pred_rels = {(rel[\"head\"], rel[\"head_type\"], rel[\"tail\"], rel[\"tail_type\"]) for rel in pred_sent if\n","                             rel[\"type\"] == rel_type}\n","                gt_rels = {(rel[\"head\"], rel[\"head_type\"], rel[\"tail\"], rel[\"tail_type\"]) for rel in gt_sent if\n","                           rel[\"type\"] == rel_type}\n","\n","            # boundaries mode only takes argument spans into account\n","            elif mode == \"boundaries\":\n","                pred_rels = {(rel[\"head\"], rel[\"tail\"]) for rel in pred_sent if rel[\"type\"] == rel_type}\n","                gt_rels = {(rel[\"head\"], rel[\"tail\"]) for rel in gt_sent if rel[\"type\"] == rel_type}\n","\n","            scores[rel_type][\"tp\"] += len(pred_rels & gt_rels)\n","            scores[rel_type][\"fp\"] += len(pred_rels - gt_rels)\n","            scores[rel_type][\"fn\"] += len(gt_rels - pred_rels)\n","\n","    # Compute per relation Precision / Recall / F1\n","    for rel_type in scores.keys():\n","        if scores[rel_type][\"tp\"]:\n","            scores[rel_type][\"p\"] = 100 * scores[rel_type][\"tp\"] / (scores[rel_type][\"fp\"] + scores[rel_type][\"tp\"])\n","            scores[rel_type][\"r\"] = 100 * scores[rel_type][\"tp\"] / (scores[rel_type][\"fn\"] + scores[rel_type][\"tp\"])\n","        else:\n","            scores[rel_type][\"p\"], scores[rel_type][\"r\"] = 0, 0\n","\n","        if not scores[rel_type][\"p\"] + scores[rel_type][\"r\"] == 0:\n","            scores[rel_type][\"f1\"] = 2 * scores[rel_type][\"p\"] * scores[rel_type][\"r\"] / (\n","                    scores[rel_type][\"p\"] + scores[rel_type][\"r\"])\n","        else:\n","            scores[rel_type][\"f1\"] = 0\n","\n","    # Compute micro F1 Scores\n","    tp = sum([scores[rel_type][\"tp\"] for rel_type in relation_types])\n","    fp = sum([scores[rel_type][\"fp\"] for rel_type in relation_types])\n","    fn = sum([scores[rel_type][\"fn\"] for rel_type in relation_types])\n","\n","    if tp:\n","        precision = 100 * tp / (tp + fp)\n","        recall = 100 * tp / (tp + fn)\n","        f1 = 2 * precision * recall / (precision + recall)\n","\n","    else:\n","        precision, recall, f1 = 0, 0, 0\n","\n","    scores[\"ALL\"][\"p\"] = precision\n","    scores[\"ALL\"][\"r\"] = recall\n","    scores[\"ALL\"][\"f1\"] = f1\n","    scores[\"ALL\"][\"tp\"] = tp\n","    scores[\"ALL\"][\"fp\"] = fp\n","    scores[\"ALL\"][\"fn\"] = fn\n","\n","    # Compute Macro F1 Scores\n","    scores[\"ALL\"][\"Macro_f1\"] = np.mean([scores[ent_type][\"f1\"] for ent_type in relation_types])\n","    scores[\"ALL\"][\"Macro_p\"] = np.mean([scores[ent_type][\"p\"] for ent_type in relation_types])\n","    scores[\"ALL\"][\"Macro_r\"] = np.mean([scores[ent_type][\"r\"] for ent_type in relation_types])\n","\n","    print(f\"RE Evaluation in *** {mode.upper()} *** mode\")\n","\n","    print(\n","        \"processed {} sentences with {} relations; found: {} relations; correct: {}.\".format(n_sents, n_rels, n_found,\n","                                                                                             tp))\n","    print(\n","        \"\\tALL\\t TP: {};\\tFP: {};\\tFN: {}\".format(\n","            scores[\"ALL\"][\"tp\"],\n","            scores[\"ALL\"][\"fp\"],\n","            scores[\"ALL\"][\"fn\"]))\n","    print(\n","        \"\\t\\t(m avg): precision: {:.2f};\\trecall: {:.2f};\\tf1: {:.2f} (micro)\".format(\n","            precision,\n","            recall,\n","            f1))\n","    print(\n","        \"\\t\\t(M avg): precision: {:.2f};\\trecall: {:.2f};\\tf1: {:.2f} (Macro)\\n\".format(\n","            scores[\"ALL\"][\"Macro_p\"],\n","            scores[\"ALL\"][\"Macro_r\"],\n","            scores[\"ALL\"][\"Macro_f1\"]))\n","\n","    for rel_type in relation_types:\n","        print(\"\\t{}: \\tTP: {};\\tFP: {};\\tFN: {};\\tprecision: {:.2f};\\trecall: {:.2f};\\tf1: {:.2f};\\t{}\".format(\n","            rel_type,\n","            scores[rel_type][\"tp\"],\n","            scores[rel_type][\"fp\"],\n","            scores[rel_type][\"fn\"],\n","            scores[rel_type][\"p\"],\n","            scores[rel_type][\"r\"],\n","            scores[rel_type][\"f1\"],\n","            scores[rel_type][\"tp\"] +\n","            scores[rel_type][\n","                \"fp\"]))\n","\n","    return scores, precision, recall, f1"]},{"cell_type":"code","execution_count":19,"metadata":{"execution":{"iopub.execute_input":"2022-10-25T16:20:33.865604Z","iopub.status.busy":"2022-10-25T16:20:33.864871Z","iopub.status.idle":"2022-10-25T16:20:33.898433Z","shell.execute_reply":"2022-10-25T16:20:33.897431Z","shell.execute_reply.started":"2022-10-25T16:20:33.865565Z"},"id":"33KLRy0hk_4f","trusted":true},"outputs":[],"source":["class BaseModule(pl.LightningModule):\n","\n","    def __init__(self, conf, config: AutoConfig, tokenizer: AutoTokenizer, model: AutoModelForSeq2SeqLM):\n","        super().__init__()\n","        self.config = config\n","        self.model = model\n","        self.tokenizer = tokenizer\n","        #self.loss_fn = label_smoothed_nll_loss\n","        self.loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n","        self.num_beams = conf.num_beams\n","\n","    def forward(self, inputs, labels, *args):\n","        ##### Check later if smooth labeled loss is better \n","        outputs = self.model(**inputs, labels = labels, use_cache = False, return_dict = True, output_hidden_states = True)\n","        output_dict = {'loss': outputs['loss'], 'logits': outputs['logits']}\n","        return output_dict\n","\n","    def training_step(self, batch: dict, batch_idx: int):\n","        ##### check later if labels = batch[\"labels\"] also works\n","        labels = batch.pop(\"labels\")\n","        labels_original = labels.clone()\n","        batch[\"decoder_input_ids\"] = torch.where(labels != -100, labels, self.config.pad_token_id)\n","        labels = shift_tokens_left(labels, -100)\n","\n","        forward_output = self.forward(batch, labels)\n","        self.log('loss', forward_output['loss'])\n","\n","        batch[\"labels\"] = labels_original\n","\n","        #### ig i dont have this\n","        if 'loss_aux' in forward_output:\n","            self.log('loss_classifier', forward_output['loss_aux'])\n","            return forward_output['loss'] + forward_output['loss_aux']\n","\n","        return forward_output['loss']\n","\n","    def validation_step(self, batch: dict, batch_idx):\n","        #### pop maybe not needed?\n","        labels = batch.pop(\"labels\")\n","        batch[\"decoder_input_ids\"] = torch.where(labels != -100, labels, self.config.pad_token_id)\n","        labels = shift_tokens_left(labels, -100)\n","        \n","        with torch.no_grad():\n","            # compute loss on predict data\n","            forward_output = self.forward(batch, labels)\n","\n","        forward_output['loss'] = forward_output['loss'].mean().detach()\n","\n","        #### probably not needed ? why would i want only pred loss\n","        # if self.hparams.prediction_loss_only:\n","        #     self.log('val_loss', forward_output['loss'])\n","        #     return\n","\n","        forward_output['logits'] = forward_output['logits'].detach()\n","\n","        if labels.shape[-1] < conf.max_length:\n","            forward_output['labels'] = self._pad_tensors_to_max_len(labels, conf.max_length)\n","        else:\n","            forward_output['labels'] = labels\n","\n","        metrics = {}\n","        metrics['val_loss'] = forward_output['loss']\n","\n","        #### only 1? so why loop lmao\n","        for key in sorted(metrics.keys()):\n","            self.log(key, metrics[key])\n","\n","        outputs = {}\n","        outputs['predictions'], outputs['labels'] = self.generate_triples(batch, labels)\n","        return outputs\n","\n","\n","    def test_step(self, batch, batch_idx):\n","\n","        #### popping again\n","        labels = batch.pop(\"labels\")\n","        batch[\"decoder_input_ids\"] = torch.where(labels != -100, labels, self.config.pad_token_id)\n","        labels = shift_tokens_left(labels, -100)\n","\n","        with torch.no_grad():\n","            # compute loss on predict data\n","            forward_output = self.forward(batch, labels)\n","\n","        forward_output['loss'] = forward_output['loss'].mean().detach()\n","\n","        ##### probably not needed? would would i only want pred loss\n","        # if self.hparams.prediction_loss_only:\n","        #     self.log('test_loss', forward_output['loss'])\n","        #     return\n","\n","        forward_output['logits'] = forward_output['logits'].detach()\n","\n","        if labels.shape[-1] < conf.max_length:\n","            forward_output['labels'] = self._pad_tensors_to_max_len(labels, conf.max_length)\n","        else:\n","            forward_output['labels'] = labels\n","\n","\n","        metrics = {}\n","        metrics['test_loss'] = forward_output['loss']\n","\n","        #### dont i only have one metric anyways?\n","        for key in sorted(metrics.keys()):\n","            self.log(key, metrics[key], prog_bar=True)\n","\n","        #### what does this actually do? how does this change everything\n","        # if self.hparams.finetune:\n","        #     return {'predictions': self.forward_samples(batch, labels)}\n","        # else:\n","\n","        outputs = {}\n","        outputs['predictions'], outputs['labels'] = self.generate_triples(batch, labels)\n","        return outputs\n","\n","\n","\n","    def validation_epoch_end(self, output: dict):\n","        \n","        relations = [\"MakePublicStatement\",\"Appeal\",\"ExpressIntentToCooperate\",\"Consult\",\"EngageInDiplomaticCooperation\",\"EngageInMaterialCooperation\",\"ProvideAid\",\"Yield\",\"Investigate\",\"Demand\",\"Disapprove\",\"Reject\",\"Threaten\",\"ExhibitMilitaryPosture\",\"Protest\",\"ReduceRelations\",\"Coerce\",\"Assault\",\"Fight\",\"EngageInUnconventialMassViolence\"]\n","        scores, precision, recall, f1 = re_score([item for pred in output for item in pred['predictions']], [item for pred in output for item in pred['labels']], relations)\n","        self.log('val_prec_micro', precision)\n","        self.log('val_recall_micro', recall)\n","        self.log('val_F1_micro', f1)\n","\n","    def test_epoch_end(self, output: dict):\n","\n","        relations = [\"MakePublicStatement\",\"Appeal\",\"ExpressIntentToCooperate\",\"Consult\",\"EngageInDiplomaticCooperation\",\"EngageInMaterialCooperation\",\"ProvideAid\",\"Yield\",\"Investigate\",\"Demand\",\"Disapprove\",\"Reject\",\"Threaten\",\"ExhibitMilitaryPosture\",\"Protest\",\"ReduceRelations\",\"Coerce\",\"Assault\",\"Fight\",\"EngageInUnconventialMassViolence\"]\n","        scores, precision, recall, f1 = re_score([item for pred in output for item in pred['predictions']], [item for pred in output for item in pred['labels']], relations)\n","        self.log('test_prec_micro', precision)\n","        self.log('test_recall_micro', recall)\n","        self.log('test_F1_micro', f1)\n","\n","\n","    # additional functions called in main functions\n","\n","    def generate_triples(self, batch, labels) -> None:\n","\n","        generated_tokens = self.model.generate(\n","            batch[\"input_ids\"].to(self.model.device),\n","            attention_mask=batch[\"attention_mask\"].to(self.model.device),\n","            use_cache = True, max_length = conf.max_length, early_stopping = conf.early_stopping, length_penalty = conf.length_penalty, \n","            no_repeat_ngram_size = conf.no_repeat_ngram_size, num_beams = conf.num_beams)\n","\n","        decoded_preds = self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n","        decoded_labels = self.tokenizer.batch_decode(torch.where(labels != -100, labels, self.config.pad_token_id), skip_special_tokens=False)\n","\n","        return [extract_triplets(rel) for rel in decoded_preds], [extract_triplets(rel) for rel in decoded_labels]\n","\n","    def _pad_tensors_to_max_len(self, tensor, max_length):\n","        # If PAD token is not defined at least EOS token has to be defined\n","        pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else self.config.eos_token_id\n","\n","        if pad_token_id is None:\n","            raise ValueError(\n","                f\"Make sure that either `config.pad_token_id` or `config.eos_token_id` is defined if tensor has to be padded to `max_length`={max_length}\"\n","            )\n","\n","        padded_tensor = pad_token_id * torch.ones(\n","            (tensor.shape[0], max_length), dtype=tensor.dtype, device=tensor.device\n","        )\n","        padded_tensor[:, : tensor.shape[-1]] = tensor\n","        return padded_tensor\n","\n","    def configure_optimizers(self):\n","\n","        ##### HUH\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        optimizer_grouped_parameters = [\n","            {\n","                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": conf.weight_decay,\n","            },\n","            {\n","                \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","        optimizer = AdamW(optimizer_grouped_parameters, lr = conf.lr, betas = (conf.beta1, conf.beta2), eps = conf.epsilon, weight_decay = conf.weight_decay)\n","        #### also check warmup later\n","        factor = lambda epoch: 0.95\n","        scheduler = lr_scheduler.MultiplicativeLR(optimizer, factor)\n","        \n","        #scheduler = inverse_square_root(optimizer, num_warmup_steps= conf.warmup_steps)\n","\n","        return [optimizer], [scheduler]\n","\n","    #def compute_metrics():\n","        #looks not needed    "]},{"cell_type":"code","execution_count":20,"metadata":{"execution":{"iopub.execute_input":"2022-10-25T16:20:33.900411Z","iopub.status.busy":"2022-10-25T16:20:33.899869Z","iopub.status.idle":"2022-10-25T16:20:33.914011Z","shell.execute_reply":"2022-10-25T16:20:33.913045Z","shell.execute_reply.started":"2022-10-25T16:20:33.900375Z"},"id":"I-qhi66IlBPX","trusted":true},"outputs":[],"source":["def train(conf):\n","    pl.seed_everything(conf.seed)\n","\n","    tokenizer = transformers.AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\", use_fast = conf.use_fast_tokenizer,\n","        additional_special_tokens = [\"<obj>\", \"<subj>\", \"<triplet>\", \"<head>\", \"</head>\", \"<tail>\", \"</tail>\"])\n","    config = transformers.AutoConfig.from_pretrained(\"Babelscape/rebel-large\")\n","    model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\", config = config)\n","\n","    model.resize_token_embeddings(len(tokenizer))\n","\n","    pl_data_module = GetData(conf, tokenizer, model)\n","    pl_module = BaseModule(conf, config, tokenizer, model)\n","\n","    wandb_logger = WandbLogger(project = \"project/finetune\".split('/')[-1].replace('.py', ''), name = \"finetune\")\n","\n","    callbacks_store = []\n","\n","    if conf.early_stopping:\n","        callbacks_store.append(\n","            EarlyStopping(\n","                monitor=conf.monitor_var,\n","                mode=conf.monitor_var_mode,\n","                patience=conf.patience\n","            )\n","        )\n","\n","    # callbacks_store.append(\n","    #     ModelCheckpoint(\n","    #         monitor=conf.monitor_var,\n","    #         # monitor=None,\n","    #         dirpath=f'models/{conf.model_name}',\n","    #         save_top_k=conf.save_top_k,\n","    #         verbose=True,\n","    #         save_last=True,\n","    #         mode=conf.monitor_var_mode\n","    #     )\n","    # )\n","    callbacks_store.append(GenerateTextSamplesCallback(conf.samples_interval))\n","    callbacks_store.append(LearningRateMonitor(logging_interval='step'))\n","\n","    trainer = pl.Trainer(\n","        gpus=conf.gpus,\n","        accumulate_grad_batches=conf.gradient_acc_steps,\n","        gradient_clip_val=conf.gradient_clip_value,\n","        #val_check_interval=conf.val_check_interval,\n","        max_epochs = 10,\n","        min_epochs = 5,\n","        callbacks=callbacks_store,\n","        max_steps=conf.max_steps,\n","        # max_steps=total_steps,\n","        precision=conf.precision,\n","        amp_level=conf.amp_level,\n","        logger=wandb_logger,\n","        #resume_from_checkpoint=conf.checkpoint_path,\n","        #limit_val_batches=conf.val_percent_check\n","    )\n","\n","    # module fit\n","    trainer.fit(pl_module, datamodule=pl_data_module)"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["150a10a7600e4b7c81bae0d8f2074fb5","9d9f0941f63b4591b5dfb60b3f04b5eb","83b7f2b300774b57bc9a6d161d1ab361","369e2460fef14cf0b608ee6606b9a958","d31b8fc829d2490f8fe535b25665fd92","ff2b281cf4b2404eac573758fe98a514","82fa24fea072448fb581209b493f94af","908372568f5949778e4cdbcf4c6aef12","f366bf93bf994f53aad7570486246d0c","ba03625f26154813bc49b987ca0b0d82","574a438a44b04defa82bdd0424235a7f","8a2b47dcdc404fff8526e82712ba8ff1","e0bf74faf96c4644b2700af0e82bc3c2","f18929c9b22e42079fabcb0b81cf2555","baa2cb71da6a433da0dc27b698816fd1","cf33818563cc476983451e5b7155e32e","447b7a6abb4240a3b1493329300c84e5","d1af56d58aad44c5b7365f9b0bc53ba0","6390c74629db4f5abfb4f52c53dc6796","7232c60b8a544b7ab418cbbf3f90720e","531d5a86ef5644089d61291f92f8f639","62092f8a23944cb1a684839f61b8da4a","9b27b142777943eb98a4367fc253cc0e","2bb2f84d2127499b8ea398d27df9a5cb","5bc78be9136749578de53aed6185756e","6acdde5760364257ae70a57d5bbfbd7c","9c0784b612c742b395691825fa5971de","30f05c40bada49639644cfc5b646c0b3","0e8de96d82244c589947cffef206d26f","98f926c5ed954d818da6cf0025802907","17833a9dc3954892ade426c385e14006","eaaa70cbd0df4742a1406f3a3ebf4ec6","aa96ab2d2d3e4310bac4ac3d35c9d3e8","eb752942c88e4a2e9f0baf8f1ff88ada","e240a887867b44a4b77e3bd1d61da4d3","988be07f83dd49f5b1052d178acb265d","646b8524eb294d3aa85f90cf3b311ad5","762bfd3d61524e579db9791b0e5c822f","ffd67d27fa164c4a90b5a27695378525","676472d809674f0ea42e82ca6cc9dbec","e0982ae149b04402aafd0507b0250d78","966af13a165a4bdcab767330ad57ab79","c7225060efb14bfbad9003100ec47a63","898a55d0c4cb41879312a9baa217ab40","173944ae52604cfb8040a9727cf55367","f4d445e20cd64e40860d4ea1718ff4fd","e993024d78b6443bbe82684b314a1ce2","ec6264a5574042c3941aa1172d258935","f43018b3490141dfb36940ca58118883","3c69166d38a243d5bb92547fa524cf58","1211eb58256245cea32f856fd3a0a811","fd25fb346299476ab239f4b4a7653423","f7adc9ed848348c6acd422cb2a0b32e8","d1db33134e7944f1b1b887f7a66c5858","abfe68dc0ff2469aa4e1c226d82a6a57"]},"execution":{"iopub.execute_input":"2022-10-25T16:20:33.917692Z","iopub.status.busy":"2022-10-25T16:20:33.917424Z","iopub.status.idle":"2022-10-25T16:40:04.200996Z","shell.execute_reply":"2022-10-25T16:40:04.198570Z","shell.execute_reply.started":"2022-10-25T16:20:33.917667Z"},"id":"f1WDgwqOlD5U","outputId":"3f81075a-86c4-4521-bd64-cb74560d327d","trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"29245e1bd4834979b650cee886027ebc","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"26f88c60b83f47fd99d4ed6d7513e44d","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/780k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0b58ca16be354ce4b177c16a3f9b2921","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e030174027c2436da95cdc56f0d8bcf8","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"79195c52e32f482cb3ccf0e77de416c2","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/123 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"71f3807c4f5b4986988084959261566d","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/344 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c27d35fe0b524be0bd488f0952692590","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.39k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"eae7b555942e47438b9ec9b9337dc242","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/1.51G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"]},{"name":"stdout","output_type":"stream","text":["  ········································\n"]},{"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"]},{"data":{"text/html":["wandb version 0.13.4 is available!  To upgrade, please run:\n"," $ pip install wandb --upgrade"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Tracking run with wandb version 0.12.21"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>./wandb/run-20221025_162204-3pk4nq6f</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/valentinwerner/finetune/runs/3pk4nq6f\" target=\"_blank\">finetune</a></strong> to <a href=\"https://wandb.ai/valentinwerner/finetune\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:448: LightningDeprecationWarning: Setting `Trainer(gpus=1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=1)` instead.\n","  f\"Setting `Trainer(gpus={gpus!r})` is deprecated in v1.7 and will be removed\"\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Sanity Checking: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1fcd29208717481faf1f731f8ebdb164","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:492: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n","  category=PossibleUserWarning,\n"]},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","RE Evaluation in *** BOUNDARIES *** mode\n","processed 16 sentences with 25 relations; found: 17 relations; correct: 0.\n","\tALL\t TP: 0;\tFP: 0;\tFN: 24\n","\t\t(m avg): precision: 0.00;\trecall: 0.00;\tf1: 0.00 (micro)\n","\t\t(M avg): precision: 0.00;\trecall: 0.00;\tf1: 0.00 (Macro)\n","\n","\tMakePublicStatement: \tTP: 0;\tFP: 0;\tFN: 11;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tAppeal: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tExpressIntentToCooperate: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tConsult: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tEngageInDiplomaticCooperation: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tEngageInMaterialCooperation: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tProvideAid: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tYield: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tInvestigate: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tDemand: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tDisapprove: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tReject: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tThreaten: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tExhibitMilitaryPosture: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tProtest: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tReduceRelations: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tCoerce: \tTP: 0;\tFP: 0;\tFN: 2;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tAssault: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tFight: \tTP: 0;\tFP: 0;\tFN: 5;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tEngageInUnconventialMassViolence: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('val_prec_micro', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n","  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n","/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('val_recall_micro', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n","  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n","/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:233: UserWarning: You called `self.log('val_F1_micro', ...)` in your `validation_epoch_end` but the value needs to be floating point. Converting it to torch.float32.\n","  f\"You called `self.log({self.meta.name!r}, ...)` in your `{self.meta.fx}` but the value needs to\"\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f26525d8dd7942b9bbf71b9733db3dd3","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.7/site-packages/pytorch_lightning/trainer/trainer.py:1896: PossibleUserWarning: The number of training batches (15) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n","  category=PossibleUserWarning,\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9f9eb0b0e5fc49539bbaffd5091d346a","version_major":2,"version_minor":0},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","RE Evaluation in *** BOUNDARIES *** mode\n","processed 114 sentences with 202 relations; found: 50 relations; correct: 0.\n","\tALL\t TP: 0;\tFP: 0;\tFN: 195\n","\t\t(m avg): precision: 0.00;\trecall: 0.00;\tf1: 0.00 (micro)\n","\t\t(M avg): precision: 0.00;\trecall: 0.00;\tf1: 0.00 (Macro)\n","\n","\tMakePublicStatement: \tTP: 0;\tFP: 0;\tFN: 79;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tAppeal: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tExpressIntentToCooperate: \tTP: 0;\tFP: 0;\tFN: 5;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tConsult: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tEngageInDiplomaticCooperation: \tTP: 0;\tFP: 0;\tFN: 6;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tEngageInMaterialCooperation: \tTP: 0;\tFP: 0;\tFN: 6;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tProvideAid: \tTP: 0;\tFP: 0;\tFN: 11;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tYield: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tInvestigate: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tDemand: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tDisapprove: \tTP: 0;\tFP: 0;\tFN: 18;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tReject: \tTP: 0;\tFP: 0;\tFN: 2;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tThreaten: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tExhibitMilitaryPosture: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tProtest: \tTP: 0;\tFP: 0;\tFN: 5;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tReduceRelations: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tCoerce: \tTP: 0;\tFP: 0;\tFN: 11;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tAssault: \tTP: 0;\tFP: 0;\tFN: 10;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tFight: \tTP: 0;\tFP: 0;\tFN: 22;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tEngageInUnconventialMassViolence: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","RE Evaluation in *** BOUNDARIES *** mode\n","processed 114 sentences with 202 relations; found: 70 relations; correct: 17.\n","\tALL\t TP: 17;\tFP: 30;\tFN: 178\n","\t\t(m avg): precision: 36.17;\trecall: 8.72;\tf1: 14.05 (micro)\n","\t\t(M avg): precision: 1.81;\trecall: 1.08;\tf1: 1.35 (Macro)\n","\n","\tMakePublicStatement: \tTP: 17;\tFP: 30;\tFN: 62;\tprecision: 36.17;\trecall: 21.52;\tf1: 26.98;\t47\n","\tAppeal: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tExpressIntentToCooperate: \tTP: 0;\tFP: 0;\tFN: 5;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tConsult: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tEngageInDiplomaticCooperation: \tTP: 0;\tFP: 0;\tFN: 6;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tEngageInMaterialCooperation: \tTP: 0;\tFP: 0;\tFN: 6;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tProvideAid: \tTP: 0;\tFP: 0;\tFN: 11;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tYield: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tInvestigate: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tDemand: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tDisapprove: \tTP: 0;\tFP: 0;\tFN: 18;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tReject: \tTP: 0;\tFP: 0;\tFN: 2;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tThreaten: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tExhibitMilitaryPosture: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tProtest: \tTP: 0;\tFP: 0;\tFN: 5;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tReduceRelations: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tCoerce: \tTP: 0;\tFP: 0;\tFN: 11;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tAssault: \tTP: 0;\tFP: 0;\tFN: 10;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tFight: \tTP: 0;\tFP: 0;\tFN: 22;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tEngageInUnconventialMassViolence: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","RE Evaluation in *** BOUNDARIES *** mode\n","processed 114 sentences with 202 relations; found: 51 relations; correct: 20.\n","\tALL\t TP: 20;\tFP: 25;\tFN: 175\n","\t\t(m avg): precision: 44.44;\trecall: 10.26;\tf1: 16.67 (micro)\n","\t\t(M avg): precision: 6.06;\trecall: 1.64;\tf1: 2.41 (Macro)\n","\n","\tMakePublicStatement: \tTP: 18;\tFP: 21;\tFN: 61;\tprecision: 46.15;\trecall: 22.78;\tf1: 30.51;\t39\n","\tAppeal: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tExpressIntentToCooperate: \tTP: 0;\tFP: 0;\tFN: 5;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tConsult: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tEngageInDiplomaticCooperation: \tTP: 0;\tFP: 0;\tFN: 6;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tEngageInMaterialCooperation: \tTP: 0;\tFP: 0;\tFN: 6;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tProvideAid: \tTP: 0;\tFP: 0;\tFN: 11;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tYield: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tInvestigate: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tDemand: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tDisapprove: \tTP: 1;\tFP: 1;\tFN: 17;\tprecision: 50.00;\trecall: 5.56;\tf1: 10.00;\t2\n","\tReject: \tTP: 0;\tFP: 0;\tFN: 2;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tThreaten: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tExhibitMilitaryPosture: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tProtest: \tTP: 0;\tFP: 0;\tFN: 5;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tReduceRelations: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tCoerce: \tTP: 0;\tFP: 0;\tFN: 11;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tAssault: \tTP: 0;\tFP: 0;\tFN: 10;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tFight: \tTP: 1;\tFP: 3;\tFN: 21;\tprecision: 25.00;\trecall: 4.55;\tf1: 7.69;\t4\n","\tEngageInUnconventialMassViolence: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","RE Evaluation in *** BOUNDARIES *** mode\n","processed 114 sentences with 202 relations; found: 53 relations; correct: 32.\n","\tALL\t TP: 32;\tFP: 16;\tFN: 163\n","\t\t(m avg): precision: 66.67;\trecall: 16.41;\tf1: 26.34 (micro)\n","\t\t(M avg): precision: 8.08;\trecall: 2.90;\tf1: 4.16 (Macro)\n","\n","\tMakePublicStatement: \tTP: 27;\tFP: 11;\tFN: 52;\tprecision: 71.05;\trecall: 34.18;\tf1: 46.15;\t38\n","\tAppeal: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tExpressIntentToCooperate: \tTP: 0;\tFP: 0;\tFN: 5;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tConsult: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tEngageInDiplomaticCooperation: \tTP: 0;\tFP: 0;\tFN: 6;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tEngageInMaterialCooperation: \tTP: 0;\tFP: 0;\tFN: 6;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tProvideAid: \tTP: 0;\tFP: 0;\tFN: 11;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tYield: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tInvestigate: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tDemand: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tDisapprove: \tTP: 1;\tFP: 2;\tFN: 17;\tprecision: 33.33;\trecall: 5.56;\tf1: 9.52;\t3\n","\tReject: \tTP: 0;\tFP: 0;\tFN: 2;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tThreaten: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tExhibitMilitaryPosture: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tProtest: \tTP: 0;\tFP: 0;\tFN: 5;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tReduceRelations: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tCoerce: \tTP: 0;\tFP: 0;\tFN: 11;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tAssault: \tTP: 0;\tFP: 0;\tFN: 10;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tFight: \tTP: 4;\tFP: 3;\tFN: 18;\tprecision: 57.14;\trecall: 18.18;\tf1: 27.59;\t7\n","\tEngageInUnconventialMassViolence: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","RE Evaluation in *** BOUNDARIES *** mode\n","processed 114 sentences with 202 relations; found: 104 relations; correct: 43.\n","\tALL\t TP: 43;\tFP: 40;\tFN: 152\n","\t\t(m avg): precision: 51.81;\trecall: 22.05;\tf1: 30.94 (micro)\n","\t\t(M avg): precision: 11.70;\trecall: 5.49;\tf1: 7.19 (Macro)\n","\n","\tMakePublicStatement: \tTP: 33;\tFP: 19;\tFN: 46;\tprecision: 63.46;\trecall: 41.77;\tf1: 50.38;\t52\n","\tAppeal: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tExpressIntentToCooperate: \tTP: 0;\tFP: 0;\tFN: 5;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tConsult: \tTP: 1;\tFP: 0;\tFN: 3;\tprecision: 100.00;\trecall: 25.00;\tf1: 40.00;\t1\n","\tEngageInDiplomaticCooperation: \tTP: 0;\tFP: 3;\tFN: 6;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t3\n","\tEngageInMaterialCooperation: \tTP: 0;\tFP: 0;\tFN: 6;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tProvideAid: \tTP: 0;\tFP: 0;\tFN: 11;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tYield: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tInvestigate: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tDemand: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tDisapprove: \tTP: 2;\tFP: 10;\tFN: 16;\tprecision: 16.67;\trecall: 11.11;\tf1: 13.33;\t12\n","\tReject: \tTP: 0;\tFP: 0;\tFN: 2;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tThreaten: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tExhibitMilitaryPosture: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tProtest: \tTP: 0;\tFP: 0;\tFN: 5;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tReduceRelations: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tCoerce: \tTP: 0;\tFP: 2;\tFN: 11;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t2\n","\tAssault: \tTP: 0;\tFP: 0;\tFN: 10;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tFight: \tTP: 7;\tFP: 6;\tFN: 15;\tprecision: 53.85;\trecall: 31.82;\tf1: 40.00;\t13\n","\tEngageInUnconventialMassViolence: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","RE Evaluation in *** BOUNDARIES *** mode\n","processed 114 sentences with 202 relations; found: 107 relations; correct: 59.\n","\tALL\t TP: 59;\tFP: 30;\tFN: 136\n","\t\t(m avg): precision: 66.29;\trecall: 30.26;\tf1: 41.55 (micro)\n","\t\t(M avg): precision: 36.74;\trecall: 15.63;\tf1: 20.82 (Macro)\n","\n","\tMakePublicStatement: \tTP: 35;\tFP: 12;\tFN: 44;\tprecision: 74.47;\trecall: 44.30;\tf1: 55.56;\t47\n","\tAppeal: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tExpressIntentToCooperate: \tTP: 0;\tFP: 0;\tFN: 5;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tConsult: \tTP: 1;\tFP: 0;\tFN: 3;\tprecision: 100.00;\trecall: 25.00;\tf1: 40.00;\t1\n","\tEngageInDiplomaticCooperation: \tTP: 2;\tFP: 7;\tFN: 4;\tprecision: 22.22;\trecall: 33.33;\tf1: 26.67;\t9\n","\tEngageInMaterialCooperation: \tTP: 0;\tFP: 0;\tFN: 6;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tProvideAid: \tTP: 1;\tFP: 1;\tFN: 10;\tprecision: 50.00;\trecall: 9.09;\tf1: 15.38;\t2\n","\tYield: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tInvestigate: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tDemand: \tTP: 1;\tFP: 0;\tFN: 3;\tprecision: 100.00;\trecall: 25.00;\tf1: 40.00;\t1\n","\tDisapprove: \tTP: 4;\tFP: 4;\tFN: 14;\tprecision: 50.00;\trecall: 22.22;\tf1: 30.77;\t8\n","\tReject: \tTP: 1;\tFP: 0;\tFN: 1;\tprecision: 100.00;\trecall: 50.00;\tf1: 66.67;\t1\n","\tThreaten: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tExhibitMilitaryPosture: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tProtest: \tTP: 2;\tFP: 0;\tFN: 3;\tprecision: 100.00;\trecall: 40.00;\tf1: 57.14;\t2\n","\tReduceRelations: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tCoerce: \tTP: 2;\tFP: 1;\tFN: 9;\tprecision: 66.67;\trecall: 18.18;\tf1: 28.57;\t3\n","\tAssault: \tTP: 0;\tFP: 1;\tFN: 10;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t1\n","\tFight: \tTP: 10;\tFP: 4;\tFN: 12;\tprecision: 71.43;\trecall: 45.45;\tf1: 55.56;\t14\n","\tEngageInUnconventialMassViolence: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","RE Evaluation in *** BOUNDARIES *** mode\n","processed 114 sentences with 202 relations; found: 155 relations; correct: 87.\n","\tALL\t TP: 87;\tFP: 27;\tFN: 108\n","\t\t(m avg): precision: 76.32;\trecall: 44.62;\tf1: 56.31 (micro)\n","\t\t(M avg): precision: 47.69;\trecall: 23.68;\tf1: 29.50 (Macro)\n","\n","\tMakePublicStatement: \tTP: 45;\tFP: 8;\tFN: 34;\tprecision: 84.91;\trecall: 56.96;\tf1: 68.18;\t53\n","\tAppeal: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tExpressIntentToCooperate: \tTP: 0;\tFP: 0;\tFN: 5;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tConsult: \tTP: 1;\tFP: 0;\tFN: 3;\tprecision: 100.00;\trecall: 25.00;\tf1: 40.00;\t1\n","\tEngageInDiplomaticCooperation: \tTP: 1;\tFP: 1;\tFN: 5;\tprecision: 50.00;\trecall: 16.67;\tf1: 25.00;\t2\n","\tEngageInMaterialCooperation: \tTP: 2;\tFP: 10;\tFN: 4;\tprecision: 16.67;\trecall: 33.33;\tf1: 22.22;\t12\n","\tProvideAid: \tTP: 1;\tFP: 0;\tFN: 10;\tprecision: 100.00;\trecall: 9.09;\tf1: 16.67;\t1\n","\tYield: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tInvestigate: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tDemand: \tTP: 2;\tFP: 1;\tFN: 2;\tprecision: 66.67;\trecall: 50.00;\tf1: 57.14;\t3\n","\tDisapprove: \tTP: 8;\tFP: 2;\tFN: 10;\tprecision: 80.00;\trecall: 44.44;\tf1: 57.14;\t10\n","\tReject: \tTP: 1;\tFP: 0;\tFN: 1;\tprecision: 100.00;\trecall: 50.00;\tf1: 66.67;\t1\n","\tThreaten: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tExhibitMilitaryPosture: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tProtest: \tTP: 2;\tFP: 0;\tFN: 3;\tprecision: 100.00;\trecall: 40.00;\tf1: 57.14;\t2\n","\tReduceRelations: \tTP: 0;\tFP: 0;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tCoerce: \tTP: 5;\tFP: 2;\tFN: 6;\tprecision: 71.43;\trecall: 45.45;\tf1: 55.56;\t7\n","\tAssault: \tTP: 3;\tFP: 0;\tFN: 7;\tprecision: 100.00;\trecall: 30.00;\tf1: 46.15;\t3\n","\tFight: \tTP: 16;\tFP: 3;\tFN: 6;\tprecision: 84.21;\trecall: 72.73;\tf1: 78.05;\t19\n","\tEngageInUnconventialMassViolence: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","RE Evaluation in *** BOUNDARIES *** mode\n","processed 114 sentences with 202 relations; found: 145 relations; correct: 97.\n","\tALL\t TP: 97;\tFP: 18;\tFN: 98\n","\t\t(m avg): precision: 84.35;\trecall: 49.74;\tf1: 62.58 (micro)\n","\t\t(M avg): precision: 63.60;\trecall: 32.58;\tf1: 41.87 (Macro)\n","\n","\tMakePublicStatement: \tTP: 50;\tFP: 10;\tFN: 29;\tprecision: 83.33;\trecall: 63.29;\tf1: 71.94;\t60\n","\tAppeal: \tTP: 1;\tFP: 1;\tFN: 3;\tprecision: 50.00;\trecall: 25.00;\tf1: 33.33;\t2\n","\tExpressIntentToCooperate: \tTP: 2;\tFP: 0;\tFN: 3;\tprecision: 100.00;\trecall: 40.00;\tf1: 57.14;\t2\n","\tConsult: \tTP: 1;\tFP: 1;\tFN: 3;\tprecision: 50.00;\trecall: 25.00;\tf1: 33.33;\t2\n","\tEngageInDiplomaticCooperation: \tTP: 1;\tFP: 0;\tFN: 5;\tprecision: 100.00;\trecall: 16.67;\tf1: 28.57;\t1\n","\tEngageInMaterialCooperation: \tTP: 0;\tFP: 0;\tFN: 6;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tProvideAid: \tTP: 4;\tFP: 2;\tFN: 7;\tprecision: 66.67;\trecall: 36.36;\tf1: 47.06;\t6\n","\tYield: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tInvestigate: \tTP: 2;\tFP: 1;\tFN: 1;\tprecision: 66.67;\trecall: 66.67;\tf1: 66.67;\t3\n","\tDemand: \tTP: 2;\tFP: 0;\tFN: 2;\tprecision: 100.00;\trecall: 50.00;\tf1: 66.67;\t2\n","\tDisapprove: \tTP: 7;\tFP: 1;\tFN: 11;\tprecision: 87.50;\trecall: 38.89;\tf1: 53.85;\t8\n","\tReject: \tTP: 1;\tFP: 0;\tFN: 1;\tprecision: 100.00;\trecall: 50.00;\tf1: 66.67;\t1\n","\tThreaten: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tExhibitMilitaryPosture: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tProtest: \tTP: 3;\tFP: 0;\tFN: 2;\tprecision: 100.00;\trecall: 60.00;\tf1: 75.00;\t3\n","\tReduceRelations: \tTP: 1;\tFP: 0;\tFN: 2;\tprecision: 100.00;\trecall: 33.33;\tf1: 50.00;\t1\n","\tCoerce: \tTP: 3;\tFP: 1;\tFN: 8;\tprecision: 75.00;\trecall: 27.27;\tf1: 40.00;\t4\n","\tAssault: \tTP: 6;\tFP: 0;\tFN: 4;\tprecision: 100.00;\trecall: 60.00;\tf1: 75.00;\t6\n","\tFight: \tTP: 13;\tFP: 1;\tFN: 9;\tprecision: 92.86;\trecall: 59.09;\tf1: 72.22;\t14\n","\tEngageInUnconventialMassViolence: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","RE Evaluation in *** BOUNDARIES *** mode\n","processed 114 sentences with 202 relations; found: 156 relations; correct: 114.\n","\tALL\t TP: 114;\tFP: 22;\tFN: 81\n","\t\t(m avg): precision: 83.82;\trecall: 58.46;\tf1: 68.88 (micro)\n","\t\t(M avg): precision: 62.95;\trecall: 45.07;\tf1: 50.52 (Macro)\n","\n","\tMakePublicStatement: \tTP: 45;\tFP: 3;\tFN: 34;\tprecision: 93.75;\trecall: 56.96;\tf1: 70.87;\t48\n","\tAppeal: \tTP: 2;\tFP: 0;\tFN: 2;\tprecision: 100.00;\trecall: 50.00;\tf1: 66.67;\t2\n","\tExpressIntentToCooperate: \tTP: 2;\tFP: 2;\tFN: 3;\tprecision: 50.00;\trecall: 40.00;\tf1: 44.44;\t4\n","\tConsult: \tTP: 2;\tFP: 0;\tFN: 2;\tprecision: 100.00;\trecall: 50.00;\tf1: 66.67;\t2\n","\tEngageInDiplomaticCooperation: \tTP: 4;\tFP: 1;\tFN: 2;\tprecision: 80.00;\trecall: 66.67;\tf1: 72.73;\t5\n","\tEngageInMaterialCooperation: \tTP: 5;\tFP: 5;\tFN: 1;\tprecision: 50.00;\trecall: 83.33;\tf1: 62.50;\t10\n","\tProvideAid: \tTP: 5;\tFP: 0;\tFN: 6;\tprecision: 100.00;\trecall: 45.45;\tf1: 62.50;\t5\n","\tYield: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tInvestigate: \tTP: 1;\tFP: 0;\tFN: 2;\tprecision: 100.00;\trecall: 33.33;\tf1: 50.00;\t1\n","\tDemand: \tTP: 0;\tFP: 0;\tFN: 4;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tDisapprove: \tTP: 13;\tFP: 3;\tFN: 5;\tprecision: 81.25;\trecall: 72.22;\tf1: 76.47;\t16\n","\tReject: \tTP: 2;\tFP: 0;\tFN: 0;\tprecision: 100.00;\trecall: 100.00;\tf1: 100.00;\t2\n","\tThreaten: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tExhibitMilitaryPosture: \tTP: 0;\tFP: 0;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tProtest: \tTP: 4;\tFP: 0;\tFN: 1;\tprecision: 100.00;\trecall: 80.00;\tf1: 88.89;\t4\n","\tReduceRelations: \tTP: 1;\tFP: 1;\tFN: 2;\tprecision: 50.00;\trecall: 33.33;\tf1: 40.00;\t2\n","\tCoerce: \tTP: 9;\tFP: 3;\tFN: 2;\tprecision: 75.00;\trecall: 81.82;\tf1: 78.26;\t12\n","\tAssault: \tTP: 4;\tFP: 0;\tFN: 6;\tprecision: 100.00;\trecall: 40.00;\tf1: 57.14;\t4\n","\tFight: \tTP: 15;\tFP: 4;\tFN: 7;\tprecision: 78.95;\trecall: 68.18;\tf1: 73.17;\t19\n","\tEngageInUnconventialMassViolence: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"","version_major":2,"version_minor":0},"text/plain":["Validation: 0it [00:00, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","RE Evaluation in *** BOUNDARIES *** mode\n","processed 114 sentences with 202 relations; found: 159 relations; correct: 120.\n","\tALL\t TP: 120;\tFP: 23;\tFN: 75\n","\t\t(m avg): precision: 83.92;\trecall: 61.54;\tf1: 71.01 (micro)\n","\t\t(M avg): precision: 70.68;\trecall: 54.63;\tf1: 59.39 (Macro)\n","\n","\tMakePublicStatement: \tTP: 47;\tFP: 5;\tFN: 32;\tprecision: 90.38;\trecall: 59.49;\tf1: 71.76;\t52\n","\tAppeal: \tTP: 2;\tFP: 0;\tFN: 2;\tprecision: 100.00;\trecall: 50.00;\tf1: 66.67;\t2\n","\tExpressIntentToCooperate: \tTP: 3;\tFP: 1;\tFN: 2;\tprecision: 75.00;\trecall: 60.00;\tf1: 66.67;\t4\n","\tConsult: \tTP: 3;\tFP: 0;\tFN: 1;\tprecision: 100.00;\trecall: 75.00;\tf1: 85.71;\t3\n","\tEngageInDiplomaticCooperation: \tTP: 1;\tFP: 0;\tFN: 5;\tprecision: 100.00;\trecall: 16.67;\tf1: 28.57;\t1\n","\tEngageInMaterialCooperation: \tTP: 2;\tFP: 2;\tFN: 4;\tprecision: 50.00;\trecall: 33.33;\tf1: 40.00;\t4\n","\tProvideAid: \tTP: 5;\tFP: 0;\tFN: 6;\tprecision: 100.00;\trecall: 45.45;\tf1: 62.50;\t5\n","\tYield: \tTP: 0;\tFP: 1;\tFN: 1;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t1\n","\tInvestigate: \tTP: 0;\tFP: 1;\tFN: 3;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t1\n","\tDemand: \tTP: 3;\tFP: 0;\tFN: 1;\tprecision: 100.00;\trecall: 75.00;\tf1: 85.71;\t3\n","\tDisapprove: \tTP: 13;\tFP: 6;\tFN: 5;\tprecision: 68.42;\trecall: 72.22;\tf1: 70.27;\t19\n","\tReject: \tTP: 2;\tFP: 0;\tFN: 0;\tprecision: 100.00;\trecall: 100.00;\tf1: 100.00;\t2\n","\tThreaten: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","\tExhibitMilitaryPosture: \tTP: 1;\tFP: 0;\tFN: 0;\tprecision: 100.00;\trecall: 100.00;\tf1: 100.00;\t1\n","\tProtest: \tTP: 4;\tFP: 0;\tFN: 1;\tprecision: 100.00;\trecall: 80.00;\tf1: 88.89;\t4\n","\tReduceRelations: \tTP: 3;\tFP: 1;\tFN: 0;\tprecision: 75.00;\trecall: 100.00;\tf1: 85.71;\t4\n","\tCoerce: \tTP: 9;\tFP: 0;\tFN: 2;\tprecision: 100.00;\trecall: 81.82;\tf1: 90.00;\t9\n","\tAssault: \tTP: 8;\tFP: 5;\tFN: 2;\tprecision: 61.54;\trecall: 80.00;\tf1: 69.57;\t13\n","\tFight: \tTP: 14;\tFP: 1;\tFN: 8;\tprecision: 93.33;\trecall: 63.64;\tf1: 75.68;\t15\n","\tEngageInUnconventialMassViolence: \tTP: 0;\tFP: 0;\tFN: 0;\tprecision: 0.00;\trecall: 0.00;\tf1: 0.00;\t0\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","To disable this warning, you can either:\n","\t- Avoid using `tokenizers` before the fork if possible\n","\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"]}],"source":["train(conf)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gecF7GsHHxcP"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3.9.7 ('base': conda)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"e68333006553a7c259dbbc354d1c2bf2da12a2e4ac4c5930a431473931cc291e"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"0e8de96d82244c589947cffef206d26f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1211eb58256245cea32f856fd3a0a811":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"150a10a7600e4b7c81bae0d8f2074fb5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9d9f0941f63b4591b5dfb60b3f04b5eb","IPY_MODEL_83b7f2b300774b57bc9a6d161d1ab361","IPY_MODEL_369e2460fef14cf0b608ee6606b9a958"],"layout":"IPY_MODEL_d31b8fc829d2490f8fe535b25665fd92"}},"173944ae52604cfb8040a9727cf55367":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f4d445e20cd64e40860d4ea1718ff4fd","IPY_MODEL_e993024d78b6443bbe82684b314a1ce2","IPY_MODEL_ec6264a5574042c3941aa1172d258935"],"layout":"IPY_MODEL_f43018b3490141dfb36940ca58118883"}},"17833a9dc3954892ade426c385e14006":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2bb2f84d2127499b8ea398d27df9a5cb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_30f05c40bada49639644cfc5b646c0b3","placeholder":"​","style":"IPY_MODEL_0e8de96d82244c589947cffef206d26f","value":"  0%"}},"30f05c40bada49639644cfc5b646c0b3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"369e2460fef14cf0b608ee6606b9a958":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ba03625f26154813bc49b987ca0b0d82","placeholder":"​","style":"IPY_MODEL_574a438a44b04defa82bdd0424235a7f","value":" 2/2 [00:04&lt;00:00,  2.24s/it]"}},"3c69166d38a243d5bb92547fa524cf58":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"447b7a6abb4240a3b1493329300c84e5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"531d5a86ef5644089d61291f92f8f639":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"574a438a44b04defa82bdd0424235a7f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5bc78be9136749578de53aed6185756e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_98f926c5ed954d818da6cf0025802907","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_17833a9dc3954892ade426c385e14006","value":0}},"62092f8a23944cb1a684839f61b8da4a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6390c74629db4f5abfb4f52c53dc6796":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"646b8524eb294d3aa85f90cf3b311ad5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c7225060efb14bfbad9003100ec47a63","placeholder":"​","style":"IPY_MODEL_898a55d0c4cb41879312a9baa217ab40","value":" 36/36 [00:51&lt;00:00,  1.43s/it, loss=9.19, v_num=jbqs]"}},"676472d809674f0ea42e82ca6cc9dbec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6acdde5760364257ae70a57d5bbfbd7c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eaaa70cbd0df4742a1406f3a3ebf4ec6","placeholder":"​","style":"IPY_MODEL_aa96ab2d2d3e4310bac4ac3d35c9d3e8","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"7232c60b8a544b7ab418cbbf3f90720e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"762bfd3d61524e579db9791b0e5c822f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"82fa24fea072448fb581209b493f94af":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"83b7f2b300774b57bc9a6d161d1ab361":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_908372568f5949778e4cdbcf4c6aef12","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f366bf93bf994f53aad7570486246d0c","value":2}},"898a55d0c4cb41879312a9baa217ab40":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8a2b47dcdc404fff8526e82712ba8ff1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e0bf74faf96c4644b2700af0e82bc3c2","IPY_MODEL_f18929c9b22e42079fabcb0b81cf2555","IPY_MODEL_baa2cb71da6a433da0dc27b698816fd1"],"layout":"IPY_MODEL_cf33818563cc476983451e5b7155e32e"}},"908372568f5949778e4cdbcf4c6aef12":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"966af13a165a4bdcab767330ad57ab79":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"988be07f83dd49f5b1052d178acb265d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0982ae149b04402aafd0507b0250d78","max":36,"min":0,"orientation":"horizontal","style":"IPY_MODEL_966af13a165a4bdcab767330ad57ab79","value":36}},"98f926c5ed954d818da6cf0025802907":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b27b142777943eb98a4367fc253cc0e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2bb2f84d2127499b8ea398d27df9a5cb","IPY_MODEL_5bc78be9136749578de53aed6185756e","IPY_MODEL_6acdde5760364257ae70a57d5bbfbd7c"],"layout":"IPY_MODEL_9c0784b612c742b395691825fa5971de"}},"9c0784b612c742b395691825fa5971de":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9d9f0941f63b4591b5dfb60b3f04b5eb":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ff2b281cf4b2404eac573758fe98a514","placeholder":"​","style":"IPY_MODEL_82fa24fea072448fb581209b493f94af","value":"Sanity Checking DataLoader 0: 100%"}},"aa96ab2d2d3e4310bac4ac3d35c9d3e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"abfe68dc0ff2469aa4e1c226d82a6a57":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ba03625f26154813bc49b987ca0b0d82":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"baa2cb71da6a433da0dc27b698816fd1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_531d5a86ef5644089d61291f92f8f639","placeholder":"​","style":"IPY_MODEL_62092f8a23944cb1a684839f61b8da4a","value":" 0/1 [00:00&lt;?, ?ba/s]"}},"c7225060efb14bfbad9003100ec47a63":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cf33818563cc476983451e5b7155e32e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1af56d58aad44c5b7365f9b0bc53ba0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d1db33134e7944f1b1b887f7a66c5858":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d31b8fc829d2490f8fe535b25665fd92":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"e0982ae149b04402aafd0507b0250d78":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e0bf74faf96c4644b2700af0e82bc3c2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_447b7a6abb4240a3b1493329300c84e5","placeholder":"​","style":"IPY_MODEL_d1af56d58aad44c5b7365f9b0bc53ba0","value":"  0%"}},"e240a887867b44a4b77e3bd1d61da4d3":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ffd67d27fa164c4a90b5a27695378525","placeholder":"​","style":"IPY_MODEL_676472d809674f0ea42e82ca6cc9dbec","value":"Epoch 0: 100%"}},"e993024d78b6443bbe82684b314a1ce2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd25fb346299476ab239f4b4a7653423","max":15,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f7adc9ed848348c6acd422cb2a0b32e8","value":15}},"eaaa70cbd0df4742a1406f3a3ebf4ec6":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb752942c88e4a2e9f0baf8f1ff88ada":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e240a887867b44a4b77e3bd1d61da4d3","IPY_MODEL_988be07f83dd49f5b1052d178acb265d","IPY_MODEL_646b8524eb294d3aa85f90cf3b311ad5"],"layout":"IPY_MODEL_762bfd3d61524e579db9791b0e5c822f"}},"ec6264a5574042c3941aa1172d258935":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1db33134e7944f1b1b887f7a66c5858","placeholder":"​","style":"IPY_MODEL_abfe68dc0ff2469aa4e1c226d82a6a57","value":" 15/15 [00:15&lt;00:00,  1.05s/it]"}},"f18929c9b22e42079fabcb0b81cf2555":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_6390c74629db4f5abfb4f52c53dc6796","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7232c60b8a544b7ab418cbbf3f90720e","value":0}},"f366bf93bf994f53aad7570486246d0c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f43018b3490141dfb36940ca58118883":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"inline-flex","flex":null,"flex_flow":"row wrap","grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":"hidden","width":"100%"}},"f4d445e20cd64e40860d4ea1718ff4fd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3c69166d38a243d5bb92547fa524cf58","placeholder":"​","style":"IPY_MODEL_1211eb58256245cea32f856fd3a0a811","value":"Validation DataLoader 0: 100%"}},"f7adc9ed848348c6acd422cb2a0b32e8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fd25fb346299476ab239f4b4a7653423":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":"2","flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff2b281cf4b2404eac573758fe98a514":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ffd67d27fa164c4a90b5a27695378525":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":4}
