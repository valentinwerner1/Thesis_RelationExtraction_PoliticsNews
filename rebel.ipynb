{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Union, List, Optional\n",
    "import omegaconf\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "import datasets\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "read = pd.read_json(\"data_src/raw/out1.json\", lines = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>article_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>_input_hash</th>\n",
       "      <th>_task_hash</th>\n",
       "      <th>_is_binary</th>\n",
       "      <th>spans</th>\n",
       "      <th>tokens</th>\n",
       "      <th>_view_id</th>\n",
       "      <th>relations</th>\n",
       "      <th>answer</th>\n",
       "      <th>_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Still, the restrictions imposed on the project...</td>\n",
       "      <td>183</td>\n",
       "      <td>18315</td>\n",
       "      <td>-1595512685</td>\n",
       "      <td>351917106</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'start': 35, 'end': 46, 'token_start': 6, 't...</td>\n",
       "      <td>[{'text': 'Still', 'start': 0, 'end': 5, 'id':...</td>\n",
       "      <td>relations</td>\n",
       "      <td>[{'head': 18, 'child': 14, 'head_span': {'star...</td>\n",
       "      <td>accept</td>\n",
       "      <td>1666082270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Days later, a released fighter and others, mos...</td>\n",
       "      <td>13</td>\n",
       "      <td>1317</td>\n",
       "      <td>399796307</td>\n",
       "      <td>1377411962</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'start': 12, 'end': 30, 'token_start': 3, 't...</td>\n",
       "      <td>[{'text': 'Days', 'start': 0, 'end': 4, 'id': ...</td>\n",
       "      <td>relations</td>\n",
       "      <td>[{'head': 20, 'child': 5, 'head_span': {'start...</td>\n",
       "      <td>accept</td>\n",
       "      <td>1666082881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Are countries allowed to turn away asylum seek...</td>\n",
       "      <td>11</td>\n",
       "      <td>1111</td>\n",
       "      <td>-1470052778</td>\n",
       "      <td>-160075520</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'start': 4, 'end': 13, 'token_start': 1, 'to...</td>\n",
       "      <td>[{'text': 'Are', 'start': 0, 'end': 3, 'id': 0...</td>\n",
       "      <td>relations</td>\n",
       "      <td>[{'head': 1, 'child': 7, 'head_span': {'start'...</td>\n",
       "      <td>accept</td>\n",
       "      <td>1666082907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One man, 28, spoke to The Washington Post on t...</td>\n",
       "      <td>167</td>\n",
       "      <td>1673</td>\n",
       "      <td>-61112669</td>\n",
       "      <td>-1028177687</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'start': 0, 'end': 11, 'token_start': 0, 'to...</td>\n",
       "      <td>[{'text': 'One', 'start': 0, 'end': 3, 'id': 0...</td>\n",
       "      <td>relations</td>\n",
       "      <td>[{'head': 3, 'child': 9, 'head_span': {'start'...</td>\n",
       "      <td>accept</td>\n",
       "      <td>1666082929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A U.S. defense official said Monday that two N...</td>\n",
       "      <td>174</td>\n",
       "      <td>1749</td>\n",
       "      <td>-2118794475</td>\n",
       "      <td>-1006559555</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'start': 0, 'end': 23, 'token_start': 0, 'to...</td>\n",
       "      <td>[{'text': 'A', 'start': 0, 'end': 1, 'id': 0, ...</td>\n",
       "      <td>relations</td>\n",
       "      <td>[]</td>\n",
       "      <td>accept</td>\n",
       "      <td>1666082966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  article_id  sentence_id  \\\n",
       "0  Still, the restrictions imposed on the project...         183        18315   \n",
       "1  Days later, a released fighter and others, mos...          13         1317   \n",
       "2  Are countries allowed to turn away asylum seek...          11         1111   \n",
       "3  One man, 28, spoke to The Washington Post on t...         167         1673   \n",
       "4  A U.S. defense official said Monday that two N...         174         1749   \n",
       "\n",
       "   _input_hash  _task_hash  _is_binary  \\\n",
       "0  -1595512685   351917106       False   \n",
       "1    399796307  1377411962       False   \n",
       "2  -1470052778  -160075520       False   \n",
       "3    -61112669 -1028177687       False   \n",
       "4  -2118794475 -1006559555       False   \n",
       "\n",
       "                                               spans  \\\n",
       "0  [{'start': 35, 'end': 46, 'token_start': 6, 't...   \n",
       "1  [{'start': 12, 'end': 30, 'token_start': 3, 't...   \n",
       "2  [{'start': 4, 'end': 13, 'token_start': 1, 'to...   \n",
       "3  [{'start': 0, 'end': 11, 'token_start': 0, 'to...   \n",
       "4  [{'start': 0, 'end': 23, 'token_start': 0, 'to...   \n",
       "\n",
       "                                              tokens   _view_id  \\\n",
       "0  [{'text': 'Still', 'start': 0, 'end': 5, 'id':...  relations   \n",
       "1  [{'text': 'Days', 'start': 0, 'end': 4, 'id': ...  relations   \n",
       "2  [{'text': 'Are', 'start': 0, 'end': 3, 'id': 0...  relations   \n",
       "3  [{'text': 'One', 'start': 0, 'end': 3, 'id': 0...  relations   \n",
       "4  [{'text': 'A', 'start': 0, 'end': 1, 'id': 0, ...  relations   \n",
       "\n",
       "                                           relations  answer  _timestamp  \n",
       "0  [{'head': 18, 'child': 14, 'head_span': {'star...  accept  1666082270  \n",
       "1  [{'head': 20, 'child': 5, 'head_span': {'start...  accept  1666082881  \n",
       "2  [{'head': 1, 'child': 7, 'head_span': {'start'...  accept  1666082907  \n",
       "3  [{'head': 3, 'child': 9, 'head_span': {'start'...  accept  1666082929  \n",
       "4                                                 []  accept  1666082966  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read[read.answer == \"accept\"]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fix sentence_id mistake\n",
    "df = df.astype({\"sentence_id\":str,\"article_id\":str}) \n",
    "\n",
    "def fix_id(row):\n",
    "    return row.sentence_id[:len(row.article_id)] + \"_\" + row.sentence_id[len(row.article_id):]\n",
    "\n",
    "df[\"sentence_id\"] = df.apply(fix_id, axis = 1)\n",
    "\n",
    "#add annotator column in case more annotators will be used\n",
    "df[\"annotator\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for EDA\n",
    "df[\"relations_count\"] = df.relations.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>article_id</th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>_input_hash</th>\n",
       "      <th>_task_hash</th>\n",
       "      <th>_is_binary</th>\n",
       "      <th>spans</th>\n",
       "      <th>tokens</th>\n",
       "      <th>_view_id</th>\n",
       "      <th>relations</th>\n",
       "      <th>answer</th>\n",
       "      <th>_timestamp</th>\n",
       "      <th>annotator</th>\n",
       "      <th>relations_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Still, the restrictions imposed on the project...</td>\n",
       "      <td>183</td>\n",
       "      <td>183_15</td>\n",
       "      <td>-1595512685</td>\n",
       "      <td>351917106</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'start': 35, 'end': 46, 'token_start': 6, 't...</td>\n",
       "      <td>[{'text': 'Still', 'start': 0, 'end': 5, 'id':...</td>\n",
       "      <td>relations</td>\n",
       "      <td>[{'head': 18, 'child': 14, 'head_span': {'star...</td>\n",
       "      <td>accept</td>\n",
       "      <td>1666082270</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Days later, a released fighter and others, mos...</td>\n",
       "      <td>13</td>\n",
       "      <td>13_17</td>\n",
       "      <td>399796307</td>\n",
       "      <td>1377411962</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'start': 12, 'end': 30, 'token_start': 3, 't...</td>\n",
       "      <td>[{'text': 'Days', 'start': 0, 'end': 4, 'id': ...</td>\n",
       "      <td>relations</td>\n",
       "      <td>[{'head': 20, 'child': 5, 'head_span': {'start...</td>\n",
       "      <td>accept</td>\n",
       "      <td>1666082881</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Are countries allowed to turn away asylum seek...</td>\n",
       "      <td>11</td>\n",
       "      <td>11_11</td>\n",
       "      <td>-1470052778</td>\n",
       "      <td>-160075520</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'start': 4, 'end': 13, 'token_start': 1, 'to...</td>\n",
       "      <td>[{'text': 'Are', 'start': 0, 'end': 3, 'id': 0...</td>\n",
       "      <td>relations</td>\n",
       "      <td>[{'head': 1, 'child': 7, 'head_span': {'start'...</td>\n",
       "      <td>accept</td>\n",
       "      <td>1666082907</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>One man, 28, spoke to The Washington Post on t...</td>\n",
       "      <td>167</td>\n",
       "      <td>167_3</td>\n",
       "      <td>-61112669</td>\n",
       "      <td>-1028177687</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'start': 0, 'end': 11, 'token_start': 0, 'to...</td>\n",
       "      <td>[{'text': 'One', 'start': 0, 'end': 3, 'id': 0...</td>\n",
       "      <td>relations</td>\n",
       "      <td>[{'head': 3, 'child': 9, 'head_span': {'start'...</td>\n",
       "      <td>accept</td>\n",
       "      <td>1666082929</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A U.S. defense official said Monday that two N...</td>\n",
       "      <td>174</td>\n",
       "      <td>174_9</td>\n",
       "      <td>-2118794475</td>\n",
       "      <td>-1006559555</td>\n",
       "      <td>False</td>\n",
       "      <td>[{'start': 0, 'end': 23, 'token_start': 0, 'to...</td>\n",
       "      <td>[{'text': 'A', 'start': 0, 'end': 1, 'id': 0, ...</td>\n",
       "      <td>relations</td>\n",
       "      <td>[]</td>\n",
       "      <td>accept</td>\n",
       "      <td>1666082966</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text article_id sentence_id  \\\n",
       "0  Still, the restrictions imposed on the project...        183      183_15   \n",
       "1  Days later, a released fighter and others, mos...         13       13_17   \n",
       "2  Are countries allowed to turn away asylum seek...         11       11_11   \n",
       "3  One man, 28, spoke to The Washington Post on t...        167       167_3   \n",
       "4  A U.S. defense official said Monday that two N...        174       174_9   \n",
       "\n",
       "   _input_hash  _task_hash  _is_binary  \\\n",
       "0  -1595512685   351917106       False   \n",
       "1    399796307  1377411962       False   \n",
       "2  -1470052778  -160075520       False   \n",
       "3    -61112669 -1028177687       False   \n",
       "4  -2118794475 -1006559555       False   \n",
       "\n",
       "                                               spans  \\\n",
       "0  [{'start': 35, 'end': 46, 'token_start': 6, 't...   \n",
       "1  [{'start': 12, 'end': 30, 'token_start': 3, 't...   \n",
       "2  [{'start': 4, 'end': 13, 'token_start': 1, 'to...   \n",
       "3  [{'start': 0, 'end': 11, 'token_start': 0, 'to...   \n",
       "4  [{'start': 0, 'end': 23, 'token_start': 0, 'to...   \n",
       "\n",
       "                                              tokens   _view_id  \\\n",
       "0  [{'text': 'Still', 'start': 0, 'end': 5, 'id':...  relations   \n",
       "1  [{'text': 'Days', 'start': 0, 'end': 4, 'id': ...  relations   \n",
       "2  [{'text': 'Are', 'start': 0, 'end': 3, 'id': 0...  relations   \n",
       "3  [{'text': 'One', 'start': 0, 'end': 3, 'id': 0...  relations   \n",
       "4  [{'text': 'A', 'start': 0, 'end': 1, 'id': 0, ...  relations   \n",
       "\n",
       "                                           relations  answer  _timestamp  \\\n",
       "0  [{'head': 18, 'child': 14, 'head_span': {'star...  accept  1666082270   \n",
       "1  [{'head': 20, 'child': 5, 'head_span': {'start...  accept  1666082881   \n",
       "2  [{'head': 1, 'child': 7, 'head_span': {'start'...  accept  1666082907   \n",
       "3  [{'head': 3, 'child': 9, 'head_span': {'start'...  accept  1666082929   \n",
       "4                                                 []  accept  1666082966   \n",
       "\n",
       "   annotator  relations_count  \n",
       "0          0                1  \n",
       "1          0                2  \n",
       "2          0                1  \n",
       "3          0                1  \n",
       "4          0                0  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prune df\n",
    "df = df.drop(columns = [\"_input_hash\",\"_task_hash\",\"_is_binary\",\"tokens\",\"_view_id\",\"answer\",\"_timestamp\"])\n",
    "df = df.reset_index().drop(columns = [\"index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### check if it performs better with empty sentences or without\n",
    "#df = df[df.relations_count > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Demand',\n",
       " 'Assault',\n",
       " 'Consult',\n",
       " 'Coerce',\n",
       " 'Disapprove',\n",
       " 'ExpressIntentToCooperate',\n",
       " 'Appeal',\n",
       " 'EngageInMaterialCooperation',\n",
       " 'EngageInDiplomaticCooperation',\n",
       " 'ProvideAid',\n",
       " 'ReduceRelations',\n",
       " 'Yield',\n",
       " 'Reject',\n",
       " 'Fight',\n",
       " 'MakePublicStatement',\n",
       " 'Protest',\n",
       " 'Investigate',\n",
       " 'ExhibitMilitaryPosture']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get data into needed format and create new DataFrame\n",
    "relations = []\n",
    "for index,row in df.iterrows():\n",
    "    for rel in row.relations:\n",
    "        relations.append(rel['label'])\n",
    "\n",
    "relations = list(set(relations))\n",
    "relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get data into needed format and create new DataFrame\n",
    "data = []\n",
    "for index,row in df.iterrows():\n",
    "    rel_list = []\n",
    "    for rel in row.relations:\n",
    "        subj = row.text[rel[\"head_span\"][\"start\"]:rel[\"head_span\"][\"end\"]]\n",
    "        obj = row.text[rel[\"child_span\"][\"start\"]:rel[\"child_span\"][\"end\"]]\n",
    "        rel_list.append(f\"<triplet> {subj} <subj> {obj} <obj> {rel['label']}\")\n",
    "    data.append({\"doc_id\":row.sentence_id, \"text\": row.text, \"triplets\": \" \".join(rel_list)})\n",
    "\n",
    "data = pd.DataFrame(data, columns = [\"doc_id\",\"text\",\"triplets\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'triplets'],\n",
       "        num_rows: 131\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['text', 'triplets'],\n",
       "        num_rows: 23\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'triplets'],\n",
       "        num_rows: 10\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = datasets.Dataset.from_pandas(data.drop(columns=[\"doc_id\"]))\n",
    "\n",
    "#### just random splits for testing, change later to proper splitting\n",
    "###### and then change later again for CV\n",
    "split = ds.train_test_split(test_size = 0.2)\n",
    "split_val = split[\"test\"].train_test_split(test_size = 0.3)\n",
    "ds = datasets.DatasetDict({\"train\":split[\"train\"],\"val\":split_val[\"train\"], \"test\":split_val[\"test\"]})\n",
    "\n",
    "#ds = ds.with_format(\"torch\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conf:\n",
    "    #general\n",
    "    seed = 0\n",
    "    gpus = 0\n",
    "    \n",
    "    #input\n",
    "    batch_size = 32\n",
    "    max_length = 128\n",
    "    ignore_pad_token_for_loss = True\n",
    "    use_fast_tokenizer = True\n",
    "    gradient_acc_steps = 1\n",
    "    gradient_clip_value = 10.0\n",
    "    load_workers = 8\n",
    "\n",
    "    #optimizer\n",
    "    lr = 0.00005\n",
    "    weight_decay = 0.01\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 0.00000001\n",
    "    warmup_steps = 1000\n",
    "\n",
    "    #training\n",
    "    max_steps = 1200000\n",
    "    samples_interval = 1000\n",
    "\n",
    "    monitor_var  = \"val_loss\"\n",
    "    monitor_var_mode = \"min\"\n",
    "    # val_check_interval = 0.5\n",
    "    # val_percent_check = 0.1\n",
    "\n",
    "    model_name = \"model1.pth\"\n",
    "    checkpoint_path = f\"models/{model_name}\"\n",
    "    save_top_k = 1\n",
    "\n",
    "    early_stopping = False\n",
    "    patience = \"5\"\n",
    "\n",
    "    length_penalty = 0\n",
    "    no_repeat_ngram_size = 0\n",
    "    num_beams = 3\n",
    "    precision = 16\n",
    "    amp_level = None\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\", use_fast = conf.use_fast_tokenizer,\n",
    "        additional_special_tokens = [\"<obj>\", \"<subj>\", \"<triplet>\", \"<head>\", \"</head>\", \"<tail>\", \"</tail>\"])\n",
    "config = transformers.AutoConfig.from_pretrained(\"Babelscape/rebel-large\")\n",
    "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\", config = config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetData(pl.LightningDataModule):\n",
    "    def __init__(self, conf: conf, tokenizer: AutoTokenizer, model: AutoModelForSeq2SeqLM):\n",
    "        \"\"\"init params from config\"\"\"\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.datasets = ds\n",
    "        \n",
    "        # Data collator\n",
    "        label_pad_token_id = -100                       \n",
    "        self.data_collator = DataCollatorForSeq2Seq(self.tokenizer, self.model, label_pad_token_id=label_pad_token_id, padding = True)\n",
    "\n",
    "    def preprocess_function(self, data):\n",
    "        \"\"\"tokenize, pad, truncate\"\"\"\n",
    "        #split into input and labels\n",
    "        inputs = data[\"text\"]       \n",
    "        outputs = data[\"triplets\"]\n",
    "\n",
    "        #process input\n",
    "        model_inputs = self.tokenizer(inputs, max_length = conf.max_length, padding = True, truncation = True)\n",
    "\n",
    "        #process labels\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            labels = self.tokenizer(outputs, max_length = conf.max_length, padding = True, truncation = True)\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "        \n",
    "    #apply the preprocessing and load data\n",
    "    def train_dataloader(self, *args, **kwargs): \n",
    "        self.train_dataset = self.datasets[\"train\"]\n",
    "        self.train_dataset = self.train_dataset.map(self.preprocess_function, remove_columns = [\"text\", \"triplets\"], batched = True)\n",
    "        return DataLoader(self.train_dataset, batch_size = conf.batch_size, collate_fn = self.data_collator, shuffle = True, num_workers= conf.load_workers)\n",
    "\n",
    "    def val_dataloader(self, *args, **kwargs): \n",
    "        self.eval_dataset = self.datasets[\"val\"]\n",
    "        self.eval_dataset = self.eval_dataset.map(self.preprocess_function, remove_columns = [\"text\", \"triplets\"], batched = True)\n",
    "        return DataLoader(self.eval_dataset, batch_size = conf.batch_size, collate_fn = self.data_collator, num_workers= conf.load_workers)\n",
    "\n",
    "    def test_dataloader(self, *args, **kwargs): \n",
    "        self.test_dataset = self.datasets[\"test\"]\n",
    "        self.test_dataset = self.test_dataset.map(self.preprocess_function,  remove_columns = [\"text\", \"triplets\"], batched = True)\n",
    "        return DataLoader(self.test_dataset, batch_size = conf.batch_size, collate_fn = self.data_collator, num_workers= conf.load_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #from REBEL\n",
    "# def label_smoothed_nll_loss(lprobs, target, epsilon, ignore_index=-100):\n",
    "#     \"\"\"From fairseq\"\"\"\n",
    "#     if target.dim() == lprobs.dim() - 1:\n",
    "#         target = target.unsqueeze(-1)\n",
    "#     nll_loss = -lprobs.gather(dim=-1, index=target)\n",
    "#     smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
    "#     if ignore_index is not None:\n",
    "#         pad_mask = target.eq(ignore_index)\n",
    "#         nll_loss.masked_fill_(pad_mask, 0.0)\n",
    "#         smooth_loss.masked_fill_(pad_mask, 0.0)\n",
    "#     else:\n",
    "#         nll_loss = nll_loss.squeeze(-1)\n",
    "#         smooth_loss = smooth_loss.squeeze(-1)\n",
    "\n",
    "#     nll_loss = nll_loss.sum()  # mean()? Scared to break other math.\n",
    "#     smooth_loss = smooth_loss.sum()\n",
    "#     eps_i = epsilon / lprobs.size(-1)\n",
    "#     loss = (1.0 - epsilon) * nll_loss + eps_i * smooth_loss\n",
    "#     return loss, nll_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from REBEL\n",
    "from typing import Sequence\n",
    "\n",
    "import torch\n",
    "from pytorch_lightning import Callback, LightningModule, Trainer\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import wandb\n",
    "\n",
    "class GenerateTextSamplesCallback(Callback):\n",
    "    \"\"\"\n",
    "    PL Callback to generate triplets along training\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, logging_batch_interval):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logging_batch_interval: How frequently to inspect/potentially plot something\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.logging_batch_interval = logging_batch_interval\n",
    "\n",
    "    def on_train_batch_end(self,trainer: Trainer,pl_module: LightningModule, outputs: Sequence, batch: Sequence, batch_idx: int, dataloader_idx: int,) -> None:\n",
    "        wandb_table = wandb.Table(columns=[\"Source\", \"Pred\", \"Gold\"])\n",
    "        # pl_module.logger.info(\"Executing translation callback\")\n",
    "        if (trainer.batch_idx + 1) % self.logging_batch_interval != 0:  # type: ignore[attr-defined]\n",
    "            return\n",
    "        labels = batch.pop(\"labels\")\n",
    "        gen_kwargs = {\n",
    "            \"max_length\": conf.max_length,\n",
    "            \"early_stopping\": False,\n",
    "            \"no_repeat_ngram_size\": 0,\n",
    "            \"num_beams\": conf.num_beans\n",
    "        }\n",
    "        pl_module.eval()\n",
    "\n",
    "        decoder_inputs = torch.roll(labels, 1, 1)[:,0:2]\n",
    "        decoder_inputs[:, 0] = 0\n",
    "        generated_tokens = pl_module.model.generate(\n",
    "            batch[\"input_ids\"].to(pl_module.model.device),\n",
    "            attention_mask=batch[\"attention_mask\"].to(pl_module.model.device),\n",
    "            decoder_input_ids=decoder_inputs.to(pl_module.model.device),\n",
    "            **gen_kwargs,\n",
    "        )\n",
    "        # in case the batch is shorter than max length, the output should be padded\n",
    "        if generated_tokens.shape[-1] < gen_kwargs[\"max_length\"]:\n",
    "            generated_tokens = pl_module._pad_tensors_to_max_len(generated_tokens, gen_kwargs[\"max_length\"])\n",
    "        pl_module.train()\n",
    "        decoded_preds = pl_module.tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
    "\n",
    "        if pl_module.hparams.ignore_pad_token_for_loss:\n",
    "            # Replace -100 in the labels as we can't decode them.\n",
    "            labels = torch.where(labels != -100, labels, pl_module.tokenizer.pad_token_id)\n",
    "\n",
    "        decoded_labels = pl_module.tokenizer.batch_decode(labels, skip_special_tokens=False)\n",
    "        decoded_inputs = pl_module.tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=False)\n",
    "\n",
    "        # pl_module.logger.experiment.log_text('generated samples', '\\n'.join(decoded_preds).replace('<pad>', ''))\n",
    "        # pl_module.logger.experiment.log_text('original samples', '\\n'.join(decoded_labels).replace('<pad>', ''))\n",
    "        for source, translation, gold_output in zip(decoded_inputs, decoded_preds, decoded_labels):\n",
    "            wandb_table.add_data(\n",
    "                source.replace('<pad>', ''), translation.replace('<pad>', ''), gold_output.replace('<pad>', '')\n",
    "            )\n",
    "        pl_module.logger.experiment.log({\"Triplets\": wandb_table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from REBEL\n",
    "def shift_tokens_left(input_ids: torch.Tensor, pad_token_id: int):\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right.\n",
    "    \"\"\"\n",
    "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "    shifted_input_ids[:, :-1] = input_ids[:, 1:].clone()\n",
    "    shifted_input_ids[:, -1] = pad_token_id\n",
    "\n",
    "    assert pad_token_id is not None, \"self.model.config.pad_token_id has to be defined.\"\n",
    "\n",
    "    return shifted_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from REBEL\n",
    "\n",
    "##### can maybe be rewritten more effective\n",
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, conf, config: AutoConfig, tokenizer: AutoTokenizer, model: AutoModelForSeq2SeqLM):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        #self.loss_fn = label_smoothed_nll_loss\n",
    "        self.loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        self.num_beans = conf.num_beans\n",
    "\n",
    "    def forward(self, inputs, labels):\n",
    "        ##### Check later if smooth labeled loss is better \n",
    "        outputs = self.model(**inputs, labels = labels, use_cache = False, return_dict = True, output_hidden_states = True)\n",
    "        output_dict = {'loss': outputs['loss'], 'logits': outputs['logits']}\n",
    "        return output_dict\n",
    "\n",
    "    def training_step(self, batch: dict, batch_idx: int):\n",
    "        ##### check later if labels = batch[\"labels\"] also works\n",
    "        labels = batch.pop(\"labels\")\n",
    "        labels_original = labels.clone()\n",
    "        batch[\"decoder_input_ids\"] = torch.where(labels != -100, labels, self.config.pad_token_id)\n",
    "        labels = shift_tokens_left(labels, -100)\n",
    "\n",
    "        forward_output = self.forward(batch, labels)\n",
    "        self.log('loss', forward_output['loss'])\n",
    "\n",
    "        batch[\"labels\"] = labels_original\n",
    "\n",
    "        #### ig i dont have this\n",
    "        if 'loss_aux' in forward_output:\n",
    "            self.log('loss_classifier', forward_output['loss_aux'])\n",
    "            return forward_output['loss'] + forward_output['loss_aux']\n",
    "\n",
    "        return forward_output['loss']\n",
    "\n",
    "    def validation_step(self, batch: dict, batch_idx):\n",
    "        #### pop maybe not needed?\n",
    "        labels = batch.pop(\"labels\")\n",
    "        batch[\"decoder_input_ids\"] = torch.where(labels != -100, labels, self.config.pad_token_id)\n",
    "        labels = shift_tokens_left(labels, -100)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # compute loss on predict data\n",
    "            forward_output = self.forward(batch, labels)\n",
    "\n",
    "        forward_output['loss'] = forward_output['loss'].mean().detach()\n",
    "\n",
    "        #### probably not needed ? why would i want only pred loss\n",
    "        # if self.hparams.prediction_loss_only:\n",
    "        #     self.log('val_loss', forward_output['loss'])\n",
    "        #     return\n",
    "\n",
    "        forward_output['logits'] = forward_output['logits'].detach()\n",
    "\n",
    "        if labels.shape[-1] < conf.max_length:\n",
    "            padded_tensor = self.config.pad_token_id * torch.ones((labels.shape[0], conf.max_length), dtype=tensor.dtype, device=tensor.device)\n",
    "            padded_tensor[:, : labels.shape[-1]] = labels\n",
    "            forward_output[\"labels\"] = padded_tensor\n",
    "        else:\n",
    "            forward_output['labels'] = labels\n",
    "\n",
    "        metrics = {}\n",
    "        metrics['val_loss'] = forward_output['loss']\n",
    "\n",
    "        #### only 1? so why loop lmao\n",
    "        for key in sorted(metrics.keys()):\n",
    "            self.log(key, metrics[key])\n",
    "\n",
    "        outputs = {}\n",
    "        outputs['predictions'], outputs['labels'] = self.generate_triples(batch, labels)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        #### popping again\n",
    "        labels = batch.pop(\"labels\")\n",
    "        batch[\"decoder_input_ids\"] = torch.where(labels != -100, labels, self.config.pad_token_id)\n",
    "        labels = shift_tokens_left(labels, -100)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # compute loss on predict data\n",
    "            forward_output = self.forward(batch, labels)\n",
    "\n",
    "        forward_output['loss'] = forward_output['loss'].mean().detach()\n",
    "\n",
    "        ##### probably not needed? would would i only want pred loss\n",
    "        # if self.hparams.prediction_loss_only:\n",
    "        #     self.log('test_loss', forward_output['loss'])\n",
    "        #     return\n",
    "\n",
    "        forward_output['logits'] = generated_tokens.detach() if self.hparams.predict_with_generate else forward_output['logits'].detach()\n",
    "\n",
    "        if labels.shape[-1] < gen_kwargs[\"max_length\"]:\n",
    "            padded_tensor = self.config.pad_token_id * torch.ones((labels.shape[0], conf.max_length), dtype=tensor.dtype, device=tensor.device)\n",
    "            padded_tensor[:, : labels.shape[-1]] = labels\n",
    "            forward_output[\"labels\"] = padded_tensor\n",
    "        else:\n",
    "            forward_output['labels'] = labels\n",
    "\n",
    "\n",
    "        metrics = {}\n",
    "        metrics['test_loss'] = forward_output['loss']\n",
    "\n",
    "        #### dont i only have one metric anyways?\n",
    "        for key in sorted(metrics.keys()):\n",
    "            self.log(key, metrics[key], prog_bar=True)\n",
    "\n",
    "        #### what does this actually do? how does this change everything\n",
    "        # if self.hparams.finetune:\n",
    "        #     return {'predictions': self.forward_samples(batch, labels)}\n",
    "        # else:\n",
    "\n",
    "        outputs = {}\n",
    "        outputs['predictions'], outputs['labels'] = self.generate_triples(batch, labels)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "\n",
    "    def validation_epoch_end():\n",
    "        \n",
    "        relations = [\"MakePublicStatement\",\"Appeal\",\"ExpressIntentToCooperate\",\"Consult\",\"EngageInDiplomaticCooperation\",\"EngageInMaterialCooperation\",\"ProvideAid\",\"Yield\",\"Investigate\",\"Demand\",\"Disapprove\",\"Reject\",\"Threaten\",\"ExhibitMilitaryPosture\",\"Protest\",\"ReduceRelations\",\"Coerce\",\"Assault\",\"Fight\",\"EngageInUnconventialMassViolence\"]\n",
    "        scores, precision, recall, f1 = re_score([item for pred in output for item in pred['predictions']], [item for pred in output for item in pred['labels']], relations)\n",
    "        self.log('val_prec_micro', precision)\n",
    "        self.log('val_recall_micro', recall)\n",
    "        self.log('val_F1_micro', f1)\n",
    "\n",
    "    def test_epoch_end():\n",
    "\n",
    "        relations = [\"MakePublicStatement\",\"Appeal\",\"ExpressIntentToCooperate\",\"Consult\",\"EngageInDiplomaticCooperation\",\"EngageInMaterialCooperation\",\"ProvideAid\",\"Yield\",\"Investigate\",\"Demand\",\"Disapprove\",\"Reject\",\"Threaten\",\"ExhibitMilitaryPosture\",\"Protest\",\"ReduceRelations\",\"Coerce\",\"Assault\",\"Fight\",\"EngageInUnconventialMassViolence\"]\n",
    "        scores, precision, recall, f1 = re_score([item for pred in output for item in pred['predictions']], [item for pred in output for item in pred['labels']], relations)\n",
    "        self.log('test_prec_micro', precision)\n",
    "        self.log('test_recall_micro', recall)\n",
    "        self.log('test_F1_micro', f1)\n",
    "\n",
    "\n",
    "    # additional functions called in main functions\n",
    "\n",
    "    def generate_triples(self, batch, labels) -> None:\n",
    "\n",
    "        generated_tokens = self.model.generate(\n",
    "            batch[\"input_ids\"].to(self.model.device),\n",
    "            attention_mask=batch[\"attention_mask\"].to(self.model.device),\n",
    "            use_cache = True, max_length = conf.max_length, early_stopping = conf.early_stopping, length_penalty = conf.length_penalty, \n",
    "            no_repeat_ngram_size = conf.no_repeat_ngram_size, num_beans = conf.num_beams)\n",
    "\n",
    "        decoded_preds = self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
    "        decoded_labels = self.tokenizer.batch_decode(torch.where(labels != -100, labels, self.config.pad_token_id), skip_special_tokens=False)\n",
    "\n",
    "        return [extract_triplets(rel) for rel in decoded_preds], [extract_triplets(rel) for rel in decoded_labels]\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        ##### HUH\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": conf.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr = conf.lr, betas = (conf.beta1, conf.beta2), eps = conf.epsilon, weight_decay = conf.weight_decay)\n",
    "        #### also check warmup later\n",
    "        factor = lambda epoch: 0.95\n",
    "        scheduler = lr_scheduler.MultiplicativeLR(optimizer, factor)\n",
    "        \n",
    "        #scheduler = inverse_square_root(optimizer, num_warmup_steps= conf.warmup_steps)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    #def compute_metrics():\n",
    "        #looks not needed    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(conf):\n",
    "    pl.seed_everything(conf.seed)\n",
    "\n",
    "    tokenizer = transformers.AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\", use_fast = conf.use_fast_tokenizer,\n",
    "        additional_special_tokens = [\"<obj>\", \"<subj>\", \"<triplet>\", \"<head>\", \"</head>\", \"<tail>\", \"</tail>\"])\n",
    "    config = transformers.AutoConfig.from_pretrained(\"Babelscape/rebel-large\")\n",
    "    model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\", config = config)\n",
    "\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    pl_data_module = GetData(conf, tokenizer, model)\n",
    "    pl_module = BaseModule(conf, config, tokenizer, model)\n",
    "\n",
    "    wandb_logger = WandbLogger(project = \"project/finetune\".split('/')[-1].replace('.py', ''), name = \"finetune\")\n",
    "\n",
    "    callbacks_store = []\n",
    "\n",
    "    if conf.early_stopping:\n",
    "        callbacks_store.append(\n",
    "            EarlyStopping(\n",
    "                monitor=conf.monitor_var,\n",
    "                mode=conf.monitor_var_mode,\n",
    "                patience=conf.patience\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # callbacks_store.append(\n",
    "    #     ModelCheckpoint(\n",
    "    #         monitor=conf.monitor_var,\n",
    "    #         # monitor=None,\n",
    "    #         dirpath=f'models/{conf.model_name}',\n",
    "    #         save_top_k=conf.save_top_k,\n",
    "    #         verbose=True,\n",
    "    #         save_last=True,\n",
    "    #         mode=conf.monitor_var_mode\n",
    "    #     )\n",
    "    # )\n",
    "    callbacks_store.append(GenerateTextSamplesCallback(conf.samples_interval))\n",
    "    callbacks_store.append(LearningRateMonitor(logging_interval='step'))\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        gpus=conf.gpus,\n",
    "        accumulate_grad_batches=conf.gradient_acc_steps,\n",
    "        gradient_clip_val=conf.gradient_clip_value,\n",
    "        #val_check_interval=conf.val_check_interval,\n",
    "        max_epochs = 10,\n",
    "        min_epochs = 5,\n",
    "        callbacks=callbacks_store,\n",
    "        max_steps=conf.max_steps,\n",
    "        # max_steps=total_steps,\n",
    "        precision=conf.precision,\n",
    "        amp_level=conf.amp_level,\n",
    "        logger=wandb_logger,\n",
    "        #resume_from_checkpoint=conf.checkpoint_path,\n",
    "        #limit_val_batches=conf.val_percent_check\n",
    "    )\n",
    "\n",
    "    # module fit\n",
    "    trainer.fit(pl_module, datamodule=pl_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n",
      "Using bfloat16 Automatic Mixed Precision (AMP)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name    | Type                         | Params\n",
      "---------------------------------------------------------\n",
      "0 | model   | BartForConditionalGeneration | 406 M \n",
      "1 | loss_fn | CrossEntropyLoss             | 0     \n",
      "---------------------------------------------------------\n",
      "406 M     Trainable params\n",
      "0         Non-trainable params\n",
      "406 M     Total params\n",
      "1,625.194 Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: 0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.00ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "forward() got an unexpected keyword argument 'num_beans'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15168/1673279483.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15168/3458605139.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(conf)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[1;31m# module fit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpl_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpl_data_module\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    694\u001b[0m         \"\"\"\n\u001b[0;32m    695\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m         self._call_and_handle_interrupt(\n\u001b[0m\u001b[0;32m    697\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_impl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_dataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_dataloaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdatamodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    648\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlauncher\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlaunch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainer_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    649\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mtrainer_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    651\u001b[0m         \u001b[1;31m# TODO(awaelchli): Unify both exceptions below, where `KeyboardError` doesn't re-raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    652\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_fit_impl\u001b[1;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[0;32m    733\u001b[0m             \u001b[0mckpt_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_provided\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_connected\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlightning_module\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    734\u001b[0m         )\n\u001b[1;32m--> 735\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mckpt_path\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mckpt_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    736\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    737\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstopped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m   1164\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_checkpoint_connector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresume_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1166\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_stage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1167\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1168\u001b[0m         \u001b[0mlog\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetail\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"{self.__class__.__name__}: trainer tearing down\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1250\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredicting\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1251\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1252\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_train\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1254\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_pre_training_routine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_run_train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1272\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1273\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0misolate_rng\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1274\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_sanity_check\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1275\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m         \u001b[1;31m# enable train mode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_run_sanity_check\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1341\u001b[0m             \u001b[1;31m# run eval step\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1342\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1343\u001b[1;33m                 \u001b[0mval_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1345\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_callback_hooks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"on_sanity_check_end\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\dataloader\\evaluation_loop.py\u001b[0m in \u001b[0;36madvance\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_dataloaders\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"dataloader_idx\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataloader_idx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 155\u001b[1;33m         \u001b[0mdl_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepoch_loop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data_fetcher\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl_max_batches\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    156\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m         \u001b[1;31m# store batch level output per dataloader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\loop.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    198\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    199\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_start\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 200\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madvance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    201\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_advance_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_restarting\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\evaluation_epoch_loop.py\u001b[0m in \u001b[0;36madvance\u001b[1;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;31m# lightning module methods\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluation_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_evaluation_step_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\pytorch_lightning\\loops\\epoch\\evaluation_epoch_loop.py\u001b[0m in \u001b[0;36m_evaluation_step\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m         \"\"\"\n\u001b[0;32m    239\u001b[0m         \u001b[0mhook_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"test_step\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesting\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\"validation_step\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 240\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_strategy_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py\u001b[0m in \u001b[0;36m_call_strategy_hook\u001b[1;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1703\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"[Strategy]{self.strategy.__class__.__name__}.{hook_name}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1704\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1705\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1706\u001b[0m         \u001b[1;31m# restore current_fx when nested context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\pytorch_lightning\\strategies\\strategy.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprecision_plugin\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mval_step_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    369\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValidationStep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 370\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalidation_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    371\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    372\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtest_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mSTEP_OUTPUT\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15168/935348226.py\u001b[0m in \u001b[0;36mvalidation_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m         \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'predictions'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'labels'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_triples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15168/935348226.py\u001b[0m in \u001b[0;36mgenerate_triples\u001b[1;34m(self, batch, labels)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgenerate_triples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         generated_tokens = self.model.generate(\n\u001b[0m\u001b[0;32m    142\u001b[0m             \u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"input_ids\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"attention_mask\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, inputs, max_length, min_length, do_sample, early_stopping, num_beams, temperature, top_k, top_p, typical_p, repetition_penalty, bad_words_ids, force_words_ids, bos_token_id, pad_token_id, eos_token_id, length_penalty, no_repeat_ngram_size, encoder_no_repeat_ngram_size, num_return_sequences, max_time, max_new_tokens, decoder_start_token_id, use_cache, num_beam_groups, diversity_penalty, prefix_allowed_tokens_fn, logits_processor, renormalize_logits, stopping_criteria, constraints, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, forced_bos_token_id, forced_eos_token_id, remove_invalid_values, synced_gpus, exponential_decay_length_penalty, **model_kwargs)\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[1;31m# if model is encoder decoder encoder_outputs are created\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;31m# and added to `model_kwargs`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1181\u001b[1;33m             model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0m\u001b[0;32m   1182\u001b[0m                 \u001b[0minputs_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_input_name\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m             )\n",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\transformers\\generation_utils.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[1;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[0;32m    523\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"return_dict\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    524\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_input_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 525\u001b[1;33m         \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"encoder_outputs\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mModelOutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mencoder_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    526\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\svawe\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1102\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1103\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() got an unexpected keyword argument 'num_beans'"
     ]
    }
   ],
   "source": [
    "train(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e68333006553a7c259dbbc354d1c2bf2da12a2e4ac4c5930a431473931cc291e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
