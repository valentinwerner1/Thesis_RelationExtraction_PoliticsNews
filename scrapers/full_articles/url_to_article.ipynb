{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/werner/thesis_valentin/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package omw-1.4 to /home/werner/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "Some weights of BertModel were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#General libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os \n",
    "import re\n",
    "from dateutil.parser import parse\n",
    "import datetime \n",
    "import json\n",
    "\n",
    "#Libraries for parsing and getting text from websites\n",
    "from codecs import xmlcharrefreplace_errors\n",
    "import feedparser\n",
    "import urllib.parse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "#Libraries for coref\n",
    "import spacy\n",
    "import crosslingual_coreference\n",
    "from crosslingual_coreference import Predictor\n",
    "import en_core_web_sm\n",
    "\n",
    "#Loading extras for parsing\n",
    "ssl._create_default_https_context = ssl._create_unverified_context #avoiding SSL errors\n",
    "headers =  {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:100.0) Gecko/20100101 Firefox/100.0\"} #avoiding some bot-shields\n",
    "predictor = Predictor(language=\"en_core_web_sm\", device=-1, model_name=\"spanbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"articles_url.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couldn't scrape Police chief calls response to South Korea crowd crush \"inadequate\" from smh\n",
      "couldn't scrape ‘Starlink is the difference’: Internet connection gives Ukraine edge in drone war from f24\n",
      "couldn't scrape Russian president accused of \"death by freezing\" from smh\n",
      "couldn't scrape Police chief calls response to South Korea crowd crush \"inadequate\" from smh\n",
      "couldn't scrape Man charged with assault and attempted kidnapping after Pelosi attack from smh\n",
      "couldn't scrape Police apologise for handling of South Korea Halloween crowd crush from smh\n",
      "couldn't scrape Wave of Russian missiles target key infrastructure sites in Kyiv from smh\n",
      "couldn't scrape Sydney woman killed in Seoul crowd crush from smh\n",
      "couldn't scrape Lula da Silva victorious in Brazilian election from smh\n",
      "couldn't scrape Gallery: The best photos from around the world from smh\n",
      "couldn't scrape Russian president accused of \"death by freezing\" from smh\n",
      "couldn't scrape 'Shark Tank India' locks the sharks for its second season from it\n",
      "couldn't scrape India's PM visits site of deadly bridge collapse from cbc\n",
      "couldn't scrape Food writer behind ‘Julie & Julia,’ dead at 49 from smh\n",
      "couldn't scrape UK MP 'wrong' to go on reality show from smh\n",
      "couldn't scrape Former Israeli PM on track for astonishing comeback from smh\n",
      "couldn't scrape Forbidden Stories founder on continuing the work of silenced journalists from f24\n",
      "couldn't scrape 'Constant noise': Bitcoin mining brings misery for Niagara Falls residents from f24\n",
      "couldn't scrape White House accuses North Korea of secretly supplying Russia with ammunition from smh\n",
      "couldn't scrape US Parkland school shooter sentenced to life in prison from smh\n",
      "couldn't scrape Moments before stampede in South Korea from smh\n",
      "couldn't scrape Christmas bauble escapes display, wreak havoc on London street from smh\n",
      "couldn't scrape Ukraine's counter-offensive in Kherson: Trailing a tank unit from f24\n",
      "couldn't scrape Gunman in 'assassination' attempt on former Pakistan PM Imran Khan from smh\n",
      "couldn't scrape Bank of England makes its biggest interest rate increase in 30 years from smh\n",
      "couldn't scrape Former Israeli Prime Minister Benjamin Netanyahu has been re-elected from smh\n",
      "couldn't scrape Former Pakistan PM shot in assassination attempt from smh\n",
      "couldn't scrape ‘No more snow’: Climate change spells end for French Alps ski resort from f24\n",
      "couldn't scrape Transatlantic ‘Route du Rhum’ sailing race delayed due to unsafe weather from f24\n",
      "couldn't scrape Nile River under threat: A closer look at Egypt's water crisis from f24\n",
      "couldn't scrape What happens in Nevada will shape the US in next two years from smh\n"
     ]
    }
   ],
   "source": [
    "res = []\n",
    "for row in df[(df.paper != \"nyt\") & (df.paper != \"jt\")].iterrows():\n",
    "    #skip nyt and jt because not scrapable without java\n",
    "    response = requests.get(row[1][\"url\"], headers = headers) \n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "    if row[1][\"paper\"] == \"f24\":\n",
    "        try:\n",
    "            res.append([row[1][\"paper\"],row[1][\"url\"],soup.find(\"div\", \"t-content__body u-clearfix\").text.replace(\"\\n\",\"\")])\n",
    "        except: \n",
    "            print(f\"couldn't scrape {row[1]['title']} from {row[1]['paper']}\")\n",
    "    elif row[1][\"paper\"] == \"cbc\":\n",
    "        try:\n",
    "            res.append([row[1][\"paper\"],row[1][\"url\"],soup.find(\"div\", \"story\").text])\n",
    "        except: \n",
    "            print(f\"couldn't scrape {row[1]['title']} from {row[1]['paper']}\")\n",
    "    elif row[1][\"paper\"] == \"it\":\n",
    "        try:\n",
    "            res.append([row[1][\"paper\"],row[1][\"url\"],soup.find(\"div\", \"_3YYSt clearfix\").text.replace(\"\\n\",\"\")])\n",
    "        except: \n",
    "            print(f\"couldn't scrape {row[1]['title']} from {row[1]['paper']}\")\n",
    "    elif row[1][\"paper\"] == \"smh\":\n",
    "        try:\n",
    "            res.append([row[1][\"paper\"],row[1][\"url\"],soup.find(\"div\", \"_1665V _2q-Vk\").text.replace(\"\\n\",\"\")])\n",
    "        except: \n",
    "            print(f\"couldn't scrape {row[1]['title']} from {row[1]['paper']}\")\n",
    "    elif row[1][\"paper\"] == \"bbc\":\n",
    "        try:\n",
    "            content = soup.find_all(\"div\", \"ssrcss-11r1m41-RichTextComponentWrapper ep2nwvo0\")\n",
    "            text = \"\"\n",
    "            for para in content:\n",
    "                text = text + \" \" + para.text\n",
    "            res.append([row[1][\"paper\"],row[1][\"url\"],text.replace(\"\\n\",\"\")])\n",
    "        except: \n",
    "            print(f\"couldn't scrape {row[1]['title']} from {row[1]['paper']}\")\n",
    "    elif row[1][\"paper\"] == \"spiegel\":\n",
    "        try:\n",
    "            content = soup.find_all(\"div\", \"RichText lg:w-8/12 md:w-10/12 lg:mx-auto md:mx-auto lg:px-24 md:px-24 sm:px-16 break-words word-wrap\")\n",
    "            text = \"\"\n",
    "            for para in content:\n",
    "                text = text + \" \" + para.text\n",
    "            res.append([row[1][\"paper\"],row[1][\"url\"],text.replace(\"\\n\", \" \")])\n",
    "        except: \n",
    "            print(f\"couldn't scrape {row[1]['title']} from {row[1]['paper']}\")\n",
    "    elif row[1][\"paper\"] == \"tass\":\n",
    "        try:\n",
    "            res.append([row[1][\"paper\"],row[1][\"url\"],soup.find(\"div\", \"text-block\").text.replace(\"\\n\",\"\")])\n",
    "        except: \n",
    "            print(f\"couldn't scrape {row[1]['title']} from {row[1]['paper']}\")\n",
    "    elif row[1][\"paper\"] == \"folha\":\n",
    "        try:\n",
    "            res.append([row[1][\"paper\"],row[1][\"url\"],soup.find(\"div\", \"c-news__content\").text.replace(\"\\n\",\"\")])\n",
    "        except: \n",
    "            print(f\"couldn't scrape {row[1]['title']} from {row[1]['paper']}\")\n",
    "    elif row[1][\"paper\"] == \"bat\":\n",
    "        try:\n",
    "            res.append([row[1][\"paper\"],row[1][\"url\"],soup.find(\"div\", \"col-12 px-4\").text.replace(\"\\n\",\"\")])\n",
    "        except: \n",
    "            print(f\"couldn't scrape {row[1]['title']} from {row[1]['paper']}\")\n",
    "    elif row[1][\"paper\"] == \"independent\":\n",
    "        try:\n",
    "            content = soup.find_all(\"p\", \"paragraph inline-placeholder\")\n",
    "            text = \"\"\n",
    "            for para in content:\n",
    "                text = text + \" \" + para.text\n",
    "            res.append([row[1][\"paper\"],row[1][\"url\"],text.replace(\"\\n\", \" \")])\n",
    "        except: \n",
    "            print(f\"couldn't scrape {row[1]['title']} from {row[1]['paper']}\")\n",
    "    elif row[1][\"paper\"] == \"ewn\":\n",
    "        try:\n",
    "            res.append([row[1][\"paper\"],row[1][\"url\"],\". \".join(soup.find(\"div\", \"medium-12 columns\").text.replace(\"\\n\",\"\").split(\".\"))])\n",
    "        except: \n",
    "            print(f\"couldn't scrape {row[1]['title']} from {row[1]['paper']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_coref(new_df):\n",
    "    \"\"\"applies coreference resolution to the new scraped articles\"\"\"\n",
    "    #load spanbert model, as it is currently one of the state of the art models and achieved best performance on the data\n",
    "    predictor = Predictor(language=\"en_core_web_sm\", device=-1, model_name=\"spanbert\")\n",
    "    #apply coreference resolution\n",
    "    new_df[\"coref_text\"] = new_df.full_text.apply(lambda x: predictor.predict(str(x))[\"resolved_text\"])\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = pd.DataFrame(res, columns = [\"paper\", \"link\", \"full_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "predictor = Predictor(language=\"en_core_web_sm\", device=-1, model_name=\"spanbert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"intermediate_full_articles-pre26-11.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "coref = []\n",
    "for row in df.iterrows():\n",
    "    try: coref.append(predictor.predict(row[1][\"full_text\"])[\"resolved_text\"])\n",
    "    except: coref.append(row[1][\"full_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[\"text\"] = coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df[[\"paper\",\"link\",\"text\"]].to_csv(\"articles_url_coref.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('thesis_valentin': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a1350fe81f24607af7015099983099ac829ebf1f4acb969c2cf14b7230b10f2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
