{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[1]:\n",
    "#General libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os \n",
    "import re\n",
    "from dateutil.parser import parse\n",
    "import datetime \n",
    "import json\n",
    "\n",
    "#Libraries for parsing and getting text from websites\n",
    "from codecs import xmlcharrefreplace_errors\n",
    "import feedparser\n",
    "import urllib.parse\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "\n",
    "#Loading extras for parsing\n",
    "ssl._create_default_https_context = ssl._create_unverified_context #avoiding SSL errors\n",
    "headers =  {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:100.0) Gecko/20100101 Firefox/100.0\"} #avoiding some bot-shields\n",
    "\n",
    "os.chdir(\"..\") #puts directory at main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selection of newspaper: 3-4 per continent, 1 per country (countries chosen based on GDP on continent & political influence (in case of russia)); \n",
    "                        ## factors: - how many readers / circulation\n",
    "                        ##          - political orientation (as central as possible)\n",
    "                        ##          - must publish in english \n",
    "\n",
    "rss_dict = {\n",
    "    \"bbc\": \"http://feeds.bbci.co.uk/news/world/rss.xml#\",\n",
    "    #\"guardian\": \"https://www.theguardian.com/world/rss\",\n",
    "    \"spiegel\": \"https://www.spiegel.de/international/index.rss\",\n",
    "    \"f24\" : \"https://www.france24.com/en/rss\",\n",
    "    \"tass\": \"http://tass.com/rss/v2.xml\",\n",
    "\n",
    "    #\"post\": \"https://feeds.washingtonpost.com/rss/world\",\n",
    "    \"nyt\": \"https://rss.nytimes.com/services/xml/rss/nyt/World.xml\",\n",
    "    \"cbc\": \"https://www.cbc.ca/cmlink/rss-world\",\n",
    "    #\"ctv\": \"http://www.ctvnews.ca/rss/World\",\n",
    "\n",
    "    \"folha\" : \"https://feeds.folha.uol.com.br/internacional/en/world/rss091.xml\",\n",
    "    \"bat\" : \"https://www.batimes.com.ar/feed\",\n",
    "\n",
    "    #\"cd\" : \"http://www.chinadaily.com.cn/rss/world_rss.xml\",\n",
    "    \"jt\" : \"https://www.japantimes.co.jp/news_category/world/feed/\",\n",
    "    \"it\" : \"https://timesofindia.indiatimes.com/rssfeeds/296589292.cms\",\n",
    "\n",
    "    \"independent\" : \"https://www.egyptindependent.com/feed/\",\n",
    "    \"ewn\" : \"http://ewn.co.za/RSS%20Feeds/Latest%20News?category=World\",\n",
    "\n",
    "    \"smh\" : \"https://www.smh.com.au/rss/world.xml\",\n",
    "    #...?    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(entry, rss, df):\n",
    "    \"\"\"get relevant data from rss feeds\"\"\"\n",
    "    data = []   #collects new articles\n",
    "    dups = 0    #counts duplicates that were not collected again\n",
    "    for article in entry:\n",
    "        try: #if any main part is missing, the article is skipped instead of the pipeline breaking\n",
    "            text = article[\"title\"] \n",
    "            if text in df.title.to_list():\n",
    "                dups += 1\n",
    "            else: \n",
    "                desc = article[\"summary\"]\n",
    "                desc = BeautifulSoup(desc).get_text()   #strips html tags which some papers are using in the description\n",
    "                date = article[\"published\"]\n",
    "                try:\n",
    "                    authors = \"\"\n",
    "                    if rss == \"cd\": #china daily uses authorname instead of authors\n",
    "                        for name in article[\"authorname\"]:\n",
    "                            if authors == \"\": authors += name[\"name\"]\n",
    "                            else: authors = authors + \"; \" + name[\"name\"]\n",
    "                    else:\n",
    "                        for name in article[\"authors\"]:\n",
    "                            if authors == \"\": authors += name[\"name\"]\n",
    "                            else: authors = authors + \"; \" + name[\"name\"]\n",
    "                except:\n",
    "                    authors = \"not mentioned in feed\" #many feeds don't include authors in the feed\n",
    "                data.append([rss, text, desc, authors, date])\n",
    "        except:\n",
    "            print(f\"couldn't scrape {article['title']} in {rss}\")\n",
    "    print(f\"found {dups} duplicates\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_feeds(rss_dict, df):\n",
    "    articles = []\n",
    "    for rss in rss_dict:\n",
    "        feed = feedparser.parse(rss_dict[f\"{rss}\"])\n",
    "        entry = feed.entries\n",
    "        if rss == \"tass\": #because tass has no dedicated world news feed, we extract only entries that link to world articles\n",
    "            new_entry = [article for article in entry if \"tass.com/world/\" in article[\"link\"]]\n",
    "            data = get_data(new_entry, rss, df)\n",
    "            for article in data:\n",
    "                articles.append(article)\n",
    "            print(f\"done with {rss}\")\n",
    "        else:\n",
    "            data = get_data(entry, rss, df)\n",
    "            for article in data:\n",
    "                articles.append(article)\n",
    "            print(f\"done with {rss}\")\n",
    "    print(f\"found {len(articles)} new articles\")\n",
    "    return articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(articles, df):\n",
    "    \"\"\"create and transform dataframe: generate main text, article id and process time, drop duplicates and unneeded columns\"\"\"\n",
    "    new_df = pd.DataFrame(articles, columns = [\"paper\", \"title\", \"summ\",\"authors\", \"DateTime\"])\n",
    "\n",
    "    #concat title and summary \n",
    "    new_df[\"full_text\"] = new_df[\"title\"] + \". \" + new_df[\"summ\"]\n",
    "\n",
    "    #time post processing\n",
    "\n",
    "    #nyt == GMT, bbc == GMT, spiegel == GMT +2, f24 == GMT, tass == GMT +3, \n",
    "    #cbc == GMT -4, folha == GMT -3, bat == GMT, jt == GMT +9, it == GMT +5.5, \n",
    "    #independent == GMT, ewn == GMT, smh == GMT + 11\n",
    "    new_df[\"date\"] = new_df.DateTime.apply(lambda x: parse(x).date())\n",
    "\n",
    "    tzinfos = {\"EDT\": -14400, \"EST\": -14400} #because cbc uses timezone instead of offset\n",
    "    new_df[\"date_time\"] = new_df.DateTime.apply(lambda x: (parse(x) - parse(x, tzinfos = tzinfos).utcoffset()).replace(tzinfo = None))\n",
    "\n",
    "    #drop old DateTime row that has been resolved\n",
    "    new_df = new_df.drop(columns = \"DateTime\")\n",
    "\n",
    "    #remove duplicated summaries and titles\n",
    "    new_df = new_df[~new_df.duplicated([\"full_text\"])]\n",
    "\n",
    "    #generate article_id\n",
    "    if df.shape[0] == 0: new_df[\"article_id\"] = new_df.index\n",
    "    else: new_df[\"article_id\"] = max(df.article_id) + 1 + new_df.index\n",
    "\n",
    "    #fill possible NaNs with empty string to avoid errors in jsonl file\n",
    "    if new_df.full_text.isna().any():\n",
    "        new_df[\"full_text\"].fillna(\"\", inplace = True)\n",
    "    \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_jsonl(new_df):\n",
    "    \"\"\"append new dataframe to jsonl file which is input for labeling\"\"\"\n",
    "    \n",
    "    jsonl = []  #list to collect rows in jsonl format\n",
    "    for index in new_df.index:\n",
    "        jsonl.append({\"text\":new_df.full_text.iloc[index], \"article_id\":int(new_df.article_id.iloc[index])})\n",
    "\n",
    "    #append to jsonl file\n",
    "    with open(\"data_src/raw/in_label/all_articles.jsonl\", 'a') as f: \n",
    "        for item in jsonl:\n",
    "            f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 26 duplicates\n",
      "done with bbc\n",
      "found 20 duplicates\n",
      "done with spiegel\n",
      "found 21 duplicates\n",
      "done with f24\n",
      "found 21 duplicates\n",
      "done with tass\n",
      "found 54 duplicates\n",
      "done with nyt\n",
      "found 20 duplicates\n",
      "done with cbc\n",
      "found 15 duplicates\n",
      "done with folha\n",
      "found 100 duplicates\n",
      "done with bat\n",
      "found 30 duplicates\n",
      "done with jt\n",
      "found 20 duplicates\n",
      "done with it\n",
      "found 10 duplicates\n",
      "done with independent\n",
      "found 14 duplicates\n",
      "done with ewn\n",
      "found 20 duplicates\n",
      "done with smh\n",
      "found 0 new articles\n",
      "0 articles have been added\n"
     ]
    }
   ],
   "source": [
    "# Load existing DataFrame of all articles\n",
    "df = pd.read_csv(\"data_src/raw/all_articles.csv\", index_col = 0)\n",
    "\n",
    "#Getting data from rss feeds\n",
    "new_df = post_process(parse_feeds(rss_dict, df), df)\n",
    "\n",
    "#Append new data to existing jsonl file\n",
    "append_jsonl(new_df)\n",
    "\n",
    "#Concat old and new data and safe as csv of all articles\n",
    "df = pd.concat([df, new_df], ignore_index = True)\n",
    "df.to_csv(\"data_src/raw/all_articles.csv\")\n",
    "\n",
    "print(f\"{new_df.shape[0]} articles have been added\")\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e68333006553a7c259dbbc354d1c2bf2da12a2e4ac4c5930a431473931cc291e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
