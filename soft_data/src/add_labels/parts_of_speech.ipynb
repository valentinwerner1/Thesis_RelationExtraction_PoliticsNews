{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/werner/thesis_valentin/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import xml.etree.ElementTree as ET\n",
    "from spacy.symbols import nsubj, dobj, pobj, iobj, neg, xcomp, VERB\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "nlp=spacy.load('en_core_web_lg')\n",
    "\n",
    "def merge_trip(df):\n",
    "    if df.shape[0] > 1:\n",
    "        return [df.iloc[0].noun, df.iloc[0].verb, df.iloc[1].noun]\n",
    "\n",
    "## make sure to allign relation name between this data and own data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/werner/thesis_valentin/lib/python3.8/site-packages/spacy/glossary.py:19: UserWarning: [W118] Term 'ner' not found in glossary. It may however be explained in documentation for the corpora used to train the language. Please check `nlp.meta[\"sources\"]` for any relevant links.\n",
      "  warnings.warn(Warnings.W118.format(term=term))\n"
     ]
    }
   ],
   "source": [
    "spacy.explain(\"ner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verb_code_dict(pico_path, verb_path):\n",
    "    \"\"\"reads coding ontology and verb lists, \n",
    "    directly matches verbs to their CAMEO codes and returns this verbs:codes dictionairy.\n",
    "    verb with codes that cannot be read are printed out as full line of the file\"\"\"\n",
    "    #read PETRARCH Internal Coding Ontology (= pico)\n",
    "    pico_path = os.path.join(os.getcwd(), pico_path)\n",
    "    pico_file = open(pico_path, 'r')\n",
    "    pico_lines = pico_file.readlines()\n",
    "\n",
    "    #get all 20 codes with their respective code\n",
    "    main_codes = {}                             #we run one iteration for all the main codes, only main codes contain relation name\n",
    "    for line in pico_lines:\n",
    "        line = line.split('#')\n",
    "        if line[0] == \"\" or line[0] == \"\\n\":    #only intro comments and empty lines\n",
    "            continue\n",
    "        else: \n",
    "            code_split = line[0].split(\":\")     #splits into CAMEO code and related hex\n",
    "            if len(line) > 1 and code_split[0][2] == \"0\":      #only main categories have 0 in 3rd idx, [cat_num 0] -> [010]\n",
    "                main_codes[code_split[0][:2]] = line[-1].replace(\"\\n\",\"\")\n",
    "    \n",
    "    #map code to code we want to use in the training\n",
    "    map_codes = {\"DiplomaticCoop\" : \"Engage In Diplomatic Cooperation\", \n",
    "                \"MaterialCoop\" : \"Engage In Material Cooperation\",\n",
    "                \"ProvideAid\" : \"Provide Aid\",\n",
    "                \"Exhibit Force Posture\": \"Exhibit Military Posture\",\n",
    "                \"Use Unconventional Mass Violence\" : \"Engage In Unconventional Mass Violence\"}\n",
    "    main_codes = {k: (map_codes[v] if v in map_codes else v) for k, v in main_codes.items()}\n",
    "    \n",
    "    #read verbs and match their code to the relation extracted in main_codes\n",
    "    verb_path = os.path.join(os.getcwd(), verb_path)\n",
    "    verb_file = open(verb_path, 'r')\n",
    "    verb_lines = verb_file.readlines()\n",
    "    \n",
    "    verb_dict = {}\n",
    "    for line in verb_lines:\n",
    "        if line[0] == \"#\":\n",
    "            continue\n",
    "        elif line.startswith(\"---\"):    #main verbs have a lead code, which is applied to all very in the section\n",
    "                                        #unless a separate code is specified for a specific verb in section\n",
    "            try: cur_main_code = re.split(\"\\[|\\]|---\", line)[2].replace(\":\",\"\")[:2]  #we only need main codes which are first two numbers\n",
    "                                                                                #sometimes code starts with \":\", e.g.: ---  OFFEND   [:110]  ---\n",
    "                                                                                #we just remove those to get the main code\n",
    "            except:                     #depending on chosen verb dictionairy, there may be main verbs without lead codes\n",
    "                print(\"couldn't finde code in: \", line.replace(\"\\n\",\"\")) \n",
    "                cur_main_code == \"--\"\n",
    "            if cur_main_code == \"\": cur_main_code = \"--\"\n",
    "        elif line == \"\\n\":              #skip empty lines\n",
    "            continue\n",
    "        elif line[0] == \"-\" or line[0] == \"~\" or line[0] == \"+\" or line[0] == \"&\": #removes all special structures we cannot use\n",
    "            continue\n",
    "        else:\n",
    "            if len(re.split(\"\\[|\\]\", line)) > 1:    #verbs with their own code, e.g.: AFFIRM [051] \n",
    "                code = re.split(\"\\[|\\]\", line)[1].replace(\":\",\"\")[:2]\n",
    "                if code != \"--\":\n",
    "                    if \"{\" in line:         #conjugated verbs, e.g. \"APPLY {APPLYING APPLIED APPLIES } [020]\"\n",
    "                        line_s = re.split(\"\\{|\\}\", line)    #split at { and }\n",
    "                        verb_dict[line_s[0]] = main_codes[code] \n",
    "                        for word in line_s[1].split():\n",
    "                            verb_dict[word.lower()] = main_codes[code]\n",
    "                    else:\n",
    "                        word = re.split(\"\\[|\\]\", line)[0]\n",
    "                        verb_dict[word.lower()] = main_codes[code]\n",
    "            else:\n",
    "                if cur_main_code != \"--\":\n",
    "                    if \"{\" in line:         #e.g. \"HURRY {HURRIES HURRYING HURRIED }\" \n",
    "                        line_s = re.split(\"\\{|\\}\", line)    #split at { and }\n",
    "                        verb_dict[line_s[0]] = main_codes[cur_main_code]\n",
    "                        for word in line_s[1].split():\n",
    "                            verb_dict[word.lower()] = main_codes[cur_main_code]\n",
    "                    else:                   #only single words with sometimes comments, e.g.: CENSURE  # JON 5/17/95\n",
    "                        word = line.split(\"#\")[0].rstrip()    #gets part before \"#\", removes all whitespaces to the right\n",
    "                        verb_dict[word.lower()] = main_codes[cur_main_code]\n",
    "\n",
    "    return verb_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#version1\n",
    "\n",
    "# doc = nlp(text2)\n",
    "# verbs = []\n",
    "# dict = {}\n",
    "# for possible_verb in doc:\n",
    "#     if possible_verb.pos == VERB:\n",
    "#         if neg in [child.dep for child in possible_verb.children]: continue\n",
    "#         else: \n",
    "#             for chunk in doc.noun_chunks:\n",
    "#                 if chunk.root.head.idx == possible_verb.idx:\n",
    "#                     verbs.append([possible_verb.idx, possible_verb, chunk.text, chunk.root.dep_])\n",
    "#                     if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "#                     else: dict[possible_verb.idx] = 1\n",
    "        \n",
    "\n",
    "# trip_idx = [key for key in dict if dict[key] > 1]\n",
    "# verbs, trip_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#version2 - for text2: technically want join is an xcomp so the child entity of want should be treated as child entity of join\n",
    "def get_triples(sentence, verb_dict):\n",
    "    \"\"\"create triplet structure for training from text input, \n",
    "    verb_dict needs to be loaded before,\n",
    "    spacy model needs to be initialized before \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    verbs = []\n",
    "    dict = {}\n",
    "\n",
    "    for possible_verb in doc:\n",
    "        if possible_verb.pos == VERB:\n",
    "            if neg in [child.dep for child in possible_verb.children]: continue\n",
    "            else: \n",
    "                for possible_subject in possible_verb.children: \n",
    "                    if possible_subject.dep == xcomp:   #subj / obj of composed verb should also be subj / obj of main verb\n",
    "                        main_verb = possible_subject\n",
    "                        main_idx = possible_subject.idx\n",
    "                        for chunk in doc.noun_chunks:\n",
    "                            if chunk.root.head.idx == possible_verb.idx:\n",
    "                                verbs.append([main_idx, main_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "                                if main_idx in dict.keys(): dict[main_idx] += 1\n",
    "                                else: dict[main_idx] = 1\n",
    "\n",
    "                for chunk in doc.noun_chunks:\n",
    "                    if chunk.root.head.idx == possible_verb.idx:\n",
    "                        verbs.append([possible_verb.idx, possible_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "                        if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "                        else: dict[possible_verb.idx] = 1\n",
    "    \n",
    "    trip_idx = [key for key in dict if dict[key] > 1]\n",
    "\n",
    "    #priority for subj-relation-obj triplets\n",
    "    mapper = {\"nsubj\":1,\"dobj\":2, \"pobj\":2, \"iobj\":2}\n",
    "\n",
    "    #create df from verbs extracted \n",
    "    df = pd.DataFrame(verbs, columns = [\"idx\", \"verb\", \"noun\", \"noun_type\"])\n",
    "    df[\"noun_type\"] = df.noun_type.map(mapper)  #turn noun_types into priority \n",
    "\n",
    "    #create groups that resolve around same word\n",
    "    gb = df.groupby('idx')    \n",
    "    #only keep groups if verb idx was identified as potential triplet before, sort by priority for structure\n",
    "    df_l = [gb.get_group(x).sort_values(\"noun_type\") for x in gb.groups if gb.get_group(x).idx.iloc[0] in dict]\n",
    "    matches = [merge_trip(group) for group in df_l if not merge_trip(group) == None] #get groups into triplet structure\n",
    "    \n",
    "    #turn matches into triples by only keeping those with coded verbs, return code instead of verb\n",
    "    triples = [[f\"<triplet>{match[0]}<subj>{match[2]}<obj>{verb_dict[match[1]]}\"] for match in matches if match[1].lower() in verb_dict]\n",
    "\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"According to a poll by Kyodo news agency released Saturday, 78 of 100 people surveyed opposed the military action in Iraq.\"\n",
    "text2 = \"I want to join together the feelings of each of us as individuals who oppose the war.\"\n",
    "text3 = \"I am getting hold of you\"\n",
    "text4 = \"Russia ends ties with EU, US, Australia\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couldn't finde code in:  --- DEFEND  ###\n",
      "couldn't finde code in:  --- REVOKE_   ###\n",
      "couldn't finde code in:  --- SEND   ###\n",
      "couldn't finde code in:  --- COLLAPSE  ###\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1522"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_dict = verb_code_dict(\"dictionaries/PETR.Internal.Coding.Ontology.txt\", \"dictionaries/newdict.txt\")\n",
    "\n",
    "#CAMEO.2.0.txt = 1451 words with relations\n",
    "#CAMEO.2.0_unsorted.txt = 1452 words with relations\n",
    "#CAMEO.verbpatterns.150430.txt = 1514 words with relations\n",
    "#newdict.txt = 1522 words with relations\n",
    "len(verb_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<triplet>78 of 100 people<subj>the military action<obj>Disapprove']]\n",
      "[['<triplet>I<subj>the feelings<obj>Consult'], ['<triplet>who<subj>the war<obj>Disapprove']]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "for sent in [text, text2, text3, text4]:\n",
    "    print(get_triples(sent, verb_dict = verb_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\svawe\\\\Thesis_RelationExtraction_PoliticsNews\\\\soft_data\\\\src\\\\add_labels'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"spans\" style=\"line-height: 2.5; direction: ltr\">The \n",
       "<span style=\"font-weight: bold; display: inline-block; position: relative; height: 60px;\">\n",
       "    Berlin\n",
       "    \n",
       "<span style=\"background: #6c6c6c; top: 40px; height: 4px; left: -1px; width: calc(100% + 2px); position: absolute;\">\n",
       "</span>\n",
       "\n",
       "    \n",
       "<span style=\"background: #6c6c6c; top: 40px; height: 4px; border-top-left-radius: 3px; border-bottom-left-radius: 3px; left: -1px; width: calc(100% + 2px); position: absolute;\">\n",
       "    <span style=\"background: #6c6c6c; z-index: 10; color: #000; top: -0.5em; padding: 2px 3px; position: absolute; font-size: 0.6em; font-weight: bold; line-height: 1; border-radius: 3px\">\n",
       "        subj\n",
       "    </span>\n",
       "</span>\n",
       "\n",
       "\n",
       "</span>\n",
       "\n",
       "<span style=\"font-weight: bold; display: inline-block; position: relative; height: 60px;\">\n",
       "    School\n",
       "    \n",
       "<span style=\"background: #6c6c6c; top: 40px; height: 4px; left: -1px; width: calc(100% + 2px); position: absolute;\">\n",
       "</span>\n",
       "\n",
       "    \n",
       "</span>\n",
       "\n",
       "<span style=\"font-weight: bold; display: inline-block; position: relative; height: 60px;\">\n",
       "    of\n",
       "    \n",
       "<span style=\"background: #6c6c6c; top: 40px; height: 4px; left: -1px; width: calc(100% + 2px); position: absolute;\">\n",
       "</span>\n",
       "\n",
       "    \n",
       "</span>\n",
       "\n",
       "<span style=\"font-weight: bold; display: inline-block; position: relative; height: 60px;\">\n",
       "    Economics\n",
       "    \n",
       "<span style=\"background: #6c6c6c; top: 40px; height: 4px; left: -1px; width: calc(100% + 2px); position: absolute;\">\n",
       "</span>\n",
       "\n",
       "    \n",
       "</span>\n",
       "\n",
       "<span style=\"font-weight: bold; display: inline-block; position: relative; height: 60px;\">\n",
       "    and\n",
       "    \n",
       "<span style=\"background: #6c6c6c; top: 40px; height: 4px; left: -1px; width: calc(100% + 2px); position: absolute;\">\n",
       "</span>\n",
       "\n",
       "    \n",
       "</span>\n",
       "\n",
       "<span style=\"font-weight: bold; display: inline-block; position: relative; height: 60px;\">\n",
       "    Law\n",
       "    \n",
       "<span style=\"background: #6c6c6c; top: 40px; height: 4px; left: -1px; width: calc(100% + 2px); position: absolute;\">\n",
       "</span>\n",
       "\n",
       "    \n",
       "</span>\n",
       "is situated in Berlin \n",
       "<span style=\"font-weight: bold; display: inline-block; position: relative; height: 60px;\">\n",
       "    Schöneberg\n",
       "    \n",
       "<span style=\"background: #6c6c6c; top: 40px; height: 4px; left: -1px; width: calc(100% + 2px); position: absolute;\">\n",
       "</span>\n",
       "\n",
       "    \n",
       "<span style=\"background: #6c6c6c; top: 40px; height: 4px; border-top-left-radius: 3px; border-bottom-left-radius: 3px; left: -1px; width: calc(100% + 2px); position: absolute;\">\n",
       "    <span style=\"background: #6c6c6c; z-index: 10; color: #000; top: -0.5em; padding: 2px 3px; position: absolute; font-size: 0.6em; font-weight: bold; line-height: 1; border-radius: 3px\">\n",
       "        obj\n",
       "    </span>\n",
       "</span>\n",
       "\n",
       "\n",
       "</span>\n",
       "</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'span' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "text6 = \"The Berlin School of Economics and Law is situated in Berlin Schöneberg\"\n",
    "from spacy import displacy\n",
    "from spacy.tokens import Span\n",
    "from pathlib import Path\n",
    "doc = nlp(text6)\n",
    "\n",
    "doc.spans[\"sc\"] = [\n",
    "    Span(doc, 1, 7, \"subj\"), \n",
    "    Span(doc, 11, 12, \"obj\"),\n",
    "]\n",
    "\n",
    "svg = displacy.serve(doc,style = \"span\", options = {\"colors\" : {\"subj\" :\"#6c6c6c\", \"obj\": \"#6c6c6c\"}})\n",
    "\n",
    "#svg = displacy.render(nlp(text5),style = \"dep\", jupyter=True, minify = True)\n",
    "# output_path = Path(r\"C:\\Users\\svawe\\Thesis_RelationExtraction_PoliticsNews\\docs\\generated_img\\spans.svg\")\n",
    "# output_path.open(\"w\", encoding=\"utf-8\").write(svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    The Berlin School of Economics and Law \n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">subj</span>\n",
       "</mark>\n",
       "is situated in \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Berlin Schöneberg\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">obj</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(ents, style = \"ent\", manual = True, jupyter = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ents = {\n",
    "    \"text\": \"The Berlin School of Economics and Law is situated in Berlin Schöneberg\",\n",
    "    \"ents\": [{\"start\": 0, \"end\": 39, \"label\": \"subj\"},{\"start\": 54, \"end\": 71, \"label\": \"obj\"}],\n",
    "    \"title\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"239ca8cf4c4b492fa819bf7be74f914d-0\" class=\"displacy\" width=\"700\" height=\"187.0\" direction=\"ltr\" style=\"max-width: none; height: 187.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"97.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"200\">The Berlin School of Economics and Law</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"200\">SUBJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"97.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"300\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"300\"> </tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"97.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">situated</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\"> </tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"97.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\"> </tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"97.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"600\">Berlin Schöneberg</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"600\">OBJ</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-239ca8cf4c4b492fa819bf7be74f914d-0-0\" stroke-width=\"2px\" d=\"M220,52.0 C220,2.0 600.0,2.0 600.0,52.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-239ca8cf4c4b492fa819bf7be74f914d-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">situated_in</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M600.0,54.0 L608.0,42.0 592.0,42.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "displacy.render(rels, style = \"dep\", manual = True, jupyter = True, options = {\"offset_x\":200, \"distance\":100})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "rels = {\n",
    "    \"words\": [\n",
    "        {\"text\": \"The Berlin School of Economics and Law\", \"tag\": \"SUBJ\"},\n",
    "        {\"text\": \"is\", \"tag\": \" \"},\n",
    "        {\"text\": \"situated\", \"tag\": \" \"},\n",
    "        {\"text\": \"in\", \"tag\": \" \"},\n",
    "        {\"text\": \"Berlin Schöneberg\", \"tag\": \"OBJ\"}\n",
    "    ],\n",
    "    \"arcs\": [\n",
    "        {\"start\": 0, \"end\": 4, \"label\": \"situated_in\", \"dir\": \"right\"},\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "went to xcomp on  trying\n",
      "new main verb is  trying\n",
      "began\n",
      "a Revolutionary Court\n",
      "nsubj\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text5)\n",
    "verbs = []\n",
    "dict = {}\n",
    "\n",
    "\n",
    "for possible_verb in doc:\n",
    "    if possible_verb.pos == VERB:\n",
    "        if neg in [child.dep for child in possible_verb.children]: continue\n",
    "        else: \n",
    "            for possible_subject in possible_verb.children: \n",
    "                if possible_subject.dep == xcomp:   #subj / obj of composed verb should also be subj / obj of main verb\n",
    "                    print(\"went to xcomp on \", possible_subject.text)\n",
    "                    main_verb = possible_subject\n",
    "                    main_idx = possible_subject.idx\n",
    "                    print(\"new main verb is \", main_verb)\n",
    "                    for token in doc.ents:\n",
    "                        if token.label_ in [\"GPE\", \"NORP\", \"EVENTS\", \"FAC\", \"LAW\", \"ORG\", \"PERSON\"]:\n",
    "                            if token.root.dep_ == \"poss\":\n",
    "                                if token.root.head.head.idx == possible_verb.idx:\n",
    "                                    verbs.append([main_idx, main_verb.lemma_, token.text, token.root.head.dep_])\n",
    "                                    if main_idx in dict.keys(): dict[main_idx] += 1\n",
    "                                    else: dict[main_idx] = 1\n",
    "                            else:\n",
    "                                if token.root.head.idx == possible_verb.idx:\n",
    "                                    print(token.root.head.text)\n",
    "                                    print(token.text)\n",
    "                                    print(token.root.dep_)\n",
    "                                    verbs.append([possible_verb.idx, possible_verb.lemma_, token.text, token.root.dep_])\n",
    "                                    if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "                                    else: dict[possible_verb.idx] = 1\n",
    "\n",
    "            for token in doc.ents:\n",
    "                if token.label_ in [\"GPE\", \"NORP\", \"EVENTS\", \"FAC\", \"LAW\", \"ORG\", \"PERSON\"]:\n",
    "                    if token.root.dep_ == \"poss\":\n",
    "                        if token.root.head.head.idx == possible_verb.idx:\n",
    "                            verbs.append([possible_verb.idx, possible_verb.lemma_, token.text, token.root.head.dep_])\n",
    "                            if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "                            else: dict[possible_verb.idx] = 1\n",
    "                    else:\n",
    "                        if token.root.head.idx == possible_verb.idx:\n",
    "                            verbs.append([possible_verb.idx, possible_verb.lemma_, token.text, token.root.dep_])\n",
    "                            if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "                            else: dict[possible_verb.idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text5)\n",
    "verbs = []\n",
    "dict = {}\n",
    "\n",
    "\n",
    "for possible_verb in doc:\n",
    "    if possible_verb.pos == VERB:\n",
    "        if neg in [child.dep for child in possible_verb.children]: continue\n",
    "        else: \n",
    "            for possible_subject in possible_verb.children: \n",
    "                if possible_subject.dep == xcomp:   #subj / obj of composed verb should also be subj / obj of main verb\n",
    "                    main_verb = possible_subject\n",
    "                    main_idx = possible_subject.idx\n",
    "                    \n",
    "                    for chunk in doc.noun_chunks:\n",
    "                        if chunk.root.dep_ == \"poss\":\n",
    "                            if chunk.root.head.head.idx == possible_verb.idx:\n",
    "                                verbs.append([main_idx, main_verb.lemma_, chunk.text, chunk.root.head.dep_])\n",
    "                                if main_idx in dict.keys(): dict[main_idx] += 1\n",
    "                                else: dict[main_idx] = 1\n",
    "                        else:\n",
    "                            if chunk.root.head.idx == possible_verb.idx:\n",
    "                                verbs.append([main_idx, main_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "                                if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "                                else: dict[possible_verb.idx] = 1\n",
    "\n",
    "            for chunk in doc.noun_chunks:       #for normal verbs, check chunks directly\n",
    "    #                 if chunk.root.head.idx == possible_verb.idx:\n",
    "                if chunk.root.head.dep_ == \"poss\":\n",
    "                    if chunk.root.head.head.idx == possible_verb.idx:\n",
    "                        verbs.append([possible_verb.idx, possible_verb.lemma_, chunk.text, chunk.root.head.dep_])\n",
    "                        if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "                        else: dict[possible_verb.idx] = 1\n",
    "                else:\n",
    "                    if chunk.root.head.idx == possible_verb.idx:\n",
    "                        verbs.append([possible_verb.idx, possible_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "                        if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "                        else: dict[possible_verb.idx] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[49, 'try', 'a Revolutionary Court', 'nsubj'],\n",
       " [43, 'begin', 'a Revolutionary Court', 'nsubj'],\n",
       " [49, 'try', 'five people', 'dobj'],\n",
       " [84, 'carry', 'that', 'nsubj'],\n",
       " [84, 'carry', 'the death penalty', 'dobj'],\n",
       " [132, 'report', 'state news agency Irna', 'nsubj']]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot instantiate typing.Union",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m Union(chunk_types[\u001b[39m0\u001b[39;49m], [\u001b[39m\"\u001b[39;49m\u001b[39mGPE\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mNORP\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mEVENTS\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mFAC\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mLAW\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mORG\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mPERSON\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m/usr/lib/python3.8/typing.py:339\u001b[0m, in \u001b[0;36m_SpecialForm.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds):\n\u001b[0;32m--> 339\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot instantiate \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m!r}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot instantiate typing.Union"
     ]
    }
   ],
   "source": [
    "Union(chunk_types[0], [\"GPE\", \"NORP\", \"EVENTS\", \"FAC\", \"LAW\", \"ORG\", \"PERSON\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection = set(chunk_types[3]) & set([\"GPE\", \"NORP\", \"EVENTS\", \"FAC\", \"LAW\", \"ORG\", \"PERSON\"])\n",
    "intersection == set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Olaf Scholz\n",
      "Beijing\n",
      "German chancellor\n",
      "the German economy’s unsustainable dependence\n",
      "China\n",
      "Olaf Scholz\n",
      "German executives\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(text5)\n",
    "chunk_types = []\n",
    "for chunk in doc.noun_chunks:\n",
    "    if  set([word.ent_type_ for word in chunk]) & set([\"GPE\", \"NORP\", \"EVENTS\", \"FAC\", \"LAW\", \"ORG\", \"PERSON\"]) != set():\n",
    "        print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['DATE', 'DATE'],\n",
       " ['PERSON', 'PERSON'],\n",
       " ['GPE'],\n",
       " ['', 'ORDINAL', ''],\n",
       " ['NORP', ''],\n",
       " ['', '', ''],\n",
       " ['', 'NORP', '', '', '', ''],\n",
       " ['GPE'],\n",
       " ['PERSON', 'PERSON'],\n",
       " ['', '', ''],\n",
       " ['NORP', '']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"04da2e0e4514495a968229f147ee779e-0\" class=\"displacy\" width=\"680\" height=\"227.0\" direction=\"ltr\" style=\"max-width: none; height: 227.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Germany</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">takes</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">part</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">in</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">Russia-</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">Ukraine</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"137.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">Conflict</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-04da2e0e4514495a968229f147ee779e-0-0\" stroke-width=\"2px\" d=\"M70,92.0 C70,47.0 135.0,47.0 135.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-04da2e0e4514495a968229f147ee779e-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,94.0 L62,82.0 78,82.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-04da2e0e4514495a968229f147ee779e-0-1\" stroke-width=\"2px\" d=\"M160,92.0 C160,47.0 225.0,47.0 225.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-04da2e0e4514495a968229f147ee779e-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M225.0,94.0 L233.0,82.0 217.0,82.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-04da2e0e4514495a968229f147ee779e-0-2\" stroke-width=\"2px\" d=\"M160,92.0 C160,2.0 320.0,2.0 320.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-04da2e0e4514495a968229f147ee779e-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M320.0,94.0 L328.0,82.0 312.0,82.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-04da2e0e4514495a968229f147ee779e-0-3\" stroke-width=\"2px\" d=\"M430,92.0 C430,47.0 495.0,47.0 495.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-04da2e0e4514495a968229f147ee779e-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M430,94.0 L422,82.0 438,82.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-04da2e0e4514495a968229f147ee779e-0-4\" stroke-width=\"2px\" d=\"M520,92.0 C520,47.0 585.0,47.0 585.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-04da2e0e4514495a968229f147ee779e-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M520,94.0 L512,82.0 528,82.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-04da2e0e4514495a968229f147ee779e-0-5\" stroke-width=\"2px\" d=\"M340,92.0 C340,2.0 590.0,2.0 590.0,92.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-04da2e0e4514495a968229f147ee779e-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M590.0,94.0 L598.0,82.0 582.0,82.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Germany nsubj takes ROOT\n",
      "part dobj takes ROOT\n",
      "Russia-Ukraine Conflict pobj takes prep\n"
     ]
    }
   ],
   "source": [
    "text5 = \"Germany takes part in Russia-Ukraine Conflict\"\n",
    "from spacy import displacy\n",
    "svg = displacy.render(nlp(text5),style = \"dep\", jupyter=True, options = {\"distance\": 90}, minify = False)\n",
    "for chunk in nlp(text5).noun_chunks:\n",
    "    print(chunk.text, chunk.root.dep_, chunk.root.head.head.text, chunk.root.head.dep_)\n",
    "\n",
    "#svg = displacy.render(nlp(text5),style = \"dep\", jupyter=True, minify = True)\n",
    "# output_path = Path(r\"C:\\Users\\svawe\\Thesis_RelationExtraction_PoliticsNews\\docs\\generated_img\\dep_tree.svg\")\n",
    "# output_path.open(\"w\", encoding=\"utf-8\").write(svg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monday pobj prep began\n",
      "a Revolutionary Court nsubj ccomp reported\n",
      "Tehran pobj prep Court\n",
      "five people dobj xcomp began\n",
      "charges pobj prep trying\n",
      "that nsubj relcl charges\n",
      "the death penalty dobj relcl charges\n",
      "state news agency Irna nsubj ROOT reported\n"
     ]
    }
   ],
   "source": [
    "for chunk in nlp(text5).noun_chunks:\n",
    "    print(chunk.text, chunk.root.dep_, chunk.root.head.dep_, chunk.root.head.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monday DATE\n",
      "a Revolutionary Court ORG\n",
      "Tehran GPE\n",
      "five CARDINAL\n",
      "Irna ORG\n"
     ]
    }
   ],
   "source": [
    "for token in doc.ents:\n",
    "    print(token, token.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('thesis_valentin': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a1350fe81f24607af7015099983099ac829ebf1f4acb969c2cf14b7230b10f2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
