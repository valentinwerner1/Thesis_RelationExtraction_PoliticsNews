{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import xml.etree.ElementTree as ET\n",
    "from spacy.symbols import nsubj, dobj, pobj, iobj, neg, xcomp, VERB\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def read_lines(inputparsed):    \n",
    "    \"\"\"takes input from CoreNLP sentence parsed file and returns sentences\"\"\"\n",
    "    #parse all lines from CoreNLP sentence split\n",
    "    parsed = open(inputparsed, encoding = \"utf-8\")\n",
    "    parsedfile = parsed.readlines()\n",
    "    parsedlines = []\n",
    "\n",
    "    #Only keep those lines which have Sentence #n in the line before\n",
    "    for idx, text in enumerate(parsedfile):\n",
    "        if text.startswith(\"Sentence #\"):\n",
    "            parsedlines.append(parsedfile[idx+1].replace('\\n','').strip())\n",
    "    \n",
    "    return parsedlines\n",
    "\n",
    "def gen_poss(line, verb_match, pre_dict):\n",
    "    \"\"\"generates all possibilities of patterns that a multi word line implies,\n",
    "    by extracting partial patterns and resolving placeholder words\"\"\"\n",
    "    poss = []\n",
    "\n",
    "    #replace special tokens in text that are clear at this point\n",
    "    line = line.replace(\"*\", verb_match)\n",
    "    line = line.replace(\"- \", \"\")\n",
    "    line = line.replace(\"+\",\"\")\n",
    "    line = line.replace(\"%\",\"\")\n",
    "    line = line.replace(\"^\",\"\")\n",
    "    line = line.replace(\"$\",\"\")\n",
    "\n",
    "    #split line by possibility indicators and code (always ends possibility)\n",
    "    #example.: \"- $ * (P ON KILLING (P OF + [010] #  COMMENT <ELH 07 May 2008>\"\n",
    "    poss_split = re.split(\"\\(P |\\[.*]\",line) \n",
    "\n",
    "    if len(poss_split) > 2: #2 is if no (P in the line\n",
    "        #only combining the first (P, as they share the same code \n",
    "        #and the longer version will never be contained in a text if the shorter isnt\n",
    "        poss.append(strip_multiple_whitespaces(\" \".join(poss_split[:2])).lower().rstrip().lstrip())\n",
    "    else: \n",
    "        poss.append(strip_multiple_whitespaces(poss_split[0].lower().rstrip().lstrip()))\n",
    "\n",
    "    cleaned = []\n",
    "    for text in list(set(poss)):\n",
    "        c = 0\n",
    "        for tag in list(pre_dict.keys()):\n",
    "            if tag in text:\n",
    "                for replacement in pre_dict[tag]:\n",
    "                    cleaned.append(text.replace(tag, replacement))\n",
    "                    c += 1\n",
    "        if c == 0:\n",
    "            cleaned.append(text)\n",
    "\n",
    "    return cleaned\n",
    "    \n",
    "\n",
    "def verb_code_dict(pico_path, verb_path):\n",
    "    \"\"\"reads coding ontology and verb lists, \n",
    "    directly matches verbs to their CAMEO codes and returns this verbs:codes dictionairy.\n",
    "    verb with codes that cannot be read are printed out as full line of the file\"\"\"\n",
    "    #read PETRARCH Internal Coding Ontology (= pico)\n",
    "    pico_path = os.path.join(os.getcwd(), pico_path)\n",
    "    pico_file = open(pico_path, 'r')\n",
    "    pico_lines = pico_file.readlines()\n",
    "\n",
    "    #get all 20 codes with their respective code\n",
    "    main_codes = {}                             #we run one iteration for all the main codes, only main codes contain relation name\n",
    "    for line in pico_lines:\n",
    "        line = line.split('#')\n",
    "        if line[0] == \"\" or line[0] == \"\\n\":    #only intro comments and empty lines\n",
    "            continue\n",
    "        else: \n",
    "            code_split = line[0].split(\":\")     #splits into CAMEO code and related hex\n",
    "            if len(line) > 1 and code_split[0][2] == \"0\":      #only main categories have 0 in 3rd idx, [cat_num 0] -> [010]\n",
    "                main_codes[code_split[0][:2]] = line[-1].replace(\"\\n\",\"\")\n",
    "    \n",
    "    #map code to code we want to use in the training\n",
    "    map_codes = {\"DiplomaticCoop\" : \"Engage In Diplomatic Cooperation\", \n",
    "                \"MaterialCoop\" : \"Engage In Material Cooperation\",\n",
    "                \"ProvideAid\" : \"Provide Aid\",\n",
    "                \"Exhibit Force Posture\": \"Exhibit Military Posture\",\n",
    "                \"Use Unconventional Mass Violence\" : \"Engage In Unconventional Mass Violence\"}\n",
    "    main_codes = {k: (map_codes[v] if v in map_codes else v) for k, v in main_codes.items()}\n",
    "    \n",
    "    #read single word patterns and match their code to the relation extracted in main_codes\n",
    "    verb_path = os.path.join(os.getcwd(), verb_path)\n",
    "    verb_file = open(verb_path, 'r')\n",
    "    verb_lines = verb_file.readlines()\n",
    "    \n",
    "    verb_dict = {}\n",
    "    for line in verb_lines:\n",
    "        if line[0] == \"#\":\n",
    "            continue\n",
    "        elif line.startswith(\"---\"):    #main verbs have a lead code, which is applied to all very in the section\n",
    "                                        #unless a separate code is specified for a specific verb in section\n",
    "            try: cur_main_code = re.split(\"\\[|\\]|---\", line)[2].replace(\":\",\"\")[:2]  #we only need main codes which are first two numbers\n",
    "                                                                                #sometimes code starts with \":\", e.g.: ---  OFFEND   [:110]  ---\n",
    "                                                                                #we just remove those to get the main code\n",
    "            except:                     #depending on chosen verb dictionairy, there may be main verbs without lead codes\n",
    "                print(\"couldn't finde code in: \", line.replace(\"\\n\",\"\")) \n",
    "                cur_main_code == \"--\"\n",
    "            if cur_main_code == \"\": cur_main_code = \"--\"\n",
    "        elif line == \"\\n\":              #skip empty lines\n",
    "            continue\n",
    "        elif line[0] == \"-\" or line[0] == \"~\" or line[0] == \"+\" or line[0] == \"&\": #removes all special structures we cannot use\n",
    "            continue\n",
    "        else:\n",
    "            if len(re.split(\"\\[|\\]\", line)) > 1:    #verbs with their own code, e.g.: AFFIRM [051] \n",
    "                code = re.split(\"\\[|\\]\", line)[1].replace(\":\",\"\")[:2]\n",
    "                if code != \"--\":\n",
    "                    if \"{\" in line:         #conjugated verbs, e.g. \"APPLY {APPLYING APPLIED APPLIES } [020]\"\n",
    "                        line_s = re.split(\"\\{|\\}\", line)    #split at { and }\n",
    "                        verb_dict[line_s[0].lower()] = main_codes[code] \n",
    "                        for word in line_s[1].split():\n",
    "                            verb_dict[word.lower()] = main_codes[code]\n",
    "                    else:\n",
    "                        word = re.split(\"\\[|\\]\", line)[0]\n",
    "                        verb_dict[word.lower()] = main_codes[code]\n",
    "            else:\n",
    "                if cur_main_code != \"--\":\n",
    "                    if \"{\" in line:         #e.g. \"HURRY {HURRIES HURRYING HURRIED }\" \n",
    "                        line_s = re.split(\"\\{|\\}\", line)    #split at { and }\n",
    "                        verb_dict[line_s[0].lower()] = main_codes[cur_main_code]\n",
    "                        for word in line_s[1].split():\n",
    "                            verb_dict[word.lower()] = main_codes[cur_main_code]\n",
    "                    else:                   #only single words with sometimes comments, e.g.: CENSURE  # JON 5/17/95\n",
    "                        word = line.split(\"#\")[0].rstrip()    #gets part before \"#\", removes all whitespaces to the right\n",
    "                        verb_dict[word.lower()] = main_codes[cur_main_code]\n",
    "\n",
    "    #read multi word patterns and create a dictionary for their code\n",
    "\n",
    "    #get filler words that occur in multi word patterns\n",
    "    verb_file = open(verb_path, 'r')\n",
    "    verb_lines = verb_file.readlines()\n",
    "\n",
    "    pre_dict = {}\n",
    "    filter_list = []\n",
    "    for line in verb_lines:\n",
    "        if line.startswith(\"&\"):\n",
    "            cur_filter = line.rstrip()\n",
    "        elif line.startswith(\"\\n\") and \"cur_filter\" in locals():\n",
    "            pre_dict[cur_filter.lower()] = filter_list\n",
    "            cur_filter = \"\"\n",
    "            filter_list = []\n",
    "        elif line.startswith(\"+\") and cur_filter != \"\":\n",
    "            filter_list.append(line.rstrip()[1:].replace(\"_\", \"\").lower())\n",
    "    del pre_dict[\"\"]\n",
    "\n",
    "    #generate dictionaries for multi word patterns\n",
    "    verb_file = open(verb_path, 'r')\n",
    "    verb_lines = verb_file.readlines()\n",
    "\n",
    "    spec_dict = {}\n",
    "    spec_code = {}\n",
    "\n",
    "    count = 0\n",
    "    for line in verb_lines:\n",
    "        if line.startswith(\"- \"):\n",
    "            #get main verb as dict key\n",
    "            try: \n",
    "                verb_match = re.search(\"# *\\w+\", line).group()\n",
    "                verb_match = re.search(\"\\w+\", verb_match).group()\n",
    "                verb_match = verb_match.replace(\"_\", \" \").lower()\n",
    "            except: \n",
    "                count += 1\n",
    "\n",
    "            #get code for line\n",
    "            try:\n",
    "                code = re.search(\"\\[.*]\", line).group()[1:3]\n",
    "                if code != \"--\":\n",
    "                    #get all possibility that the line indicates\n",
    "                    poss = gen_poss(line, verb_match, pre_dict)\n",
    "                    for pattern in poss:\n",
    "                        spec_code[pattern] = main_codes[code]\n",
    "                    spec_dict[verb_match] = poss\n",
    "            except:\n",
    "                count += 1\n",
    "\n",
    "    print(f\"{count} patterns could not be loaded\")        \n",
    "\n",
    "    return verb_dict, spec_dict, spec_code\n",
    "\n",
    "\n",
    "def get_triples(sentence, verb_dict, spec_dict, spec_code, nlp):\n",
    "    \"\"\"create triplet structure for training from text input, \n",
    "    verb_dict needs to be loaded before,\n",
    "    spacy model needs to be initialized before \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    verbs = []\n",
    "    dict = {}\n",
    "\n",
    "\n",
    "    for possible_verb in doc:\n",
    "        if possible_verb.pos == VERB:\n",
    "            if neg in [child.dep for child in possible_verb.children]: continue\n",
    "            else: \n",
    "                for possible_subject in possible_verb.children: \n",
    "                    if possible_subject.dep == xcomp:   #subj / obj of composed verb should also be subj / obj of main verb\n",
    "                        main_verb = possible_subject\n",
    "                        main_idx = possible_subject.idx\n",
    "                        for token in doc.ents:\n",
    "                            if token.label_ in [\"GPE\", \"NORP\", \"EVENTS\", \"FAC\", \"LAW\", \"ORG\", \"PERSON\"]:\n",
    "                                if token.root.dep_ == \"poss\":\n",
    "                                    if token.root.head.head.idx == possible_verb.idx:\n",
    "                                        verbs.append([main_idx, main_verb.lemma_, token.text, token.root.head.dep_])\n",
    "                                        if main_idx in dict.keys(): dict[main_idx] += 1\n",
    "                                        else: dict[main_idx] = 1\n",
    "                                else:\n",
    "                                    if token.root.head.idx == possible_verb.idx:\n",
    "                                        verbs.append([main_idx, main_verb.lemma_, token.text, token.root.dep_])\n",
    "                                        if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "                                        else: dict[possible_verb.idx] = 1\n",
    "\n",
    "                for token in doc.ents:\n",
    "                    if token.label_ in [\"GPE\", \"NORP\", \"EVENTS\", \"FAC\", \"LAW\", \"ORG\", \"PERSON\"]:\n",
    "                        if token.root.dep_ == \"poss\":\n",
    "                            if token.root.head.head.idx == possible_verb.idx:\n",
    "                                verbs.append([possible_verb.idx, possible_verb.lemma_, token.text, token.root.head.dep_])\n",
    "                                if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "                                else: dict[possible_verb.idx] = 1\n",
    "                        else:\n",
    "                            if token.root.head.idx == possible_verb.idx:\n",
    "                                verbs.append([possible_verb.idx, possible_verb.lemma_, token.text, token.root.dep_])\n",
    "                                if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "                                else: dict[possible_verb.idx] = 1\n",
    "\n",
    "    trip_idx = [key for key in dict if dict[key] > 1]\n",
    "\n",
    "    # doc = nlp(sentence)\n",
    "    # verbs = []\n",
    "    # dict = {}\n",
    "\n",
    "    # for possible_verb in doc:           #parses through all words in sentence\n",
    "    #     if possible_verb.pos == VERB:   #we only care about verbs\n",
    "    #         if neg in [child.dep for child in possible_verb.children]: continue #we exclude all negated verbs\n",
    "    #         else: \n",
    "    #             for candidate in possible_verb.children: #for composed verbs of verb (e.g. \"want to join\" -> \"want join\")\n",
    "    #                 if candidate.dep == xcomp:   #subj / obj of composed verb should also be subj / obj of main verb\n",
    "    #                     main_verb = candidate    \n",
    "    #                     main_idx = candidate.idx\n",
    "    #                     for chunk in doc.noun_chunks:   #chunks are noun-groups (e.g.: \"78 out of 100 people\" instead of \"people\")\n",
    "    #                         if chunk.root.head.idx == possible_verb.idx:    #if chunk applies to xcomp (want),\n",
    "    #                                                                         #treat it like it aplles to main verb (\"join\")\n",
    "    #                             verbs.append([main_idx, main_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "    #                             if main_idx in dict.keys(): dict[main_idx] += 1 #count how often verb is used\n",
    "    #                             else: dict[main_idx] = 1\n",
    "\n",
    "    #             for chunk in doc.noun_chunks:       #for normal verbs, check chunks directly\n",
    "    #                 if chunk.root.head.idx == possible_verb.idx:\n",
    "    #                     verbs.append([possible_verb.idx, possible_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "    #                     if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "    #                     else: dict[possible_verb.idx] = 1\n",
    "    \n",
    "    # trip_idx = [key for key in dict if dict[key] > 1]   #if verbs used more than once, its candidate for triplet\n",
    "\n",
    "    #priority for subj-relation-obj triplets\n",
    "    mapper = {\"nsubj\":1,\"dobj\":2, \"pobj\":2, \"iobj\":2}\n",
    "\n",
    "    #create df from verbs extracted \n",
    "    df = pd.DataFrame(verbs, columns = [\"idx\", \"verb\", \"noun\", \"noun_type\"])\n",
    "    df[\"noun_map\"] = df.noun_type.map(mapper)  #turn noun_types into priority \n",
    "    return df\n",
    "\n",
    "    # #create groups that resolve around same word\n",
    "    # gb = df.groupby('idx')    \n",
    "    # #only keep groups if verb idx was identified as potential triplet before, sort by priority for structure\n",
    "    # df_l = [gb.get_group(x).sort_values(\"noun_map\") for x in gb.groups if gb.get_group(x).idx.iloc[0] in dict]\n",
    "    # matches = [merge_trip(group) for group in df_l if not merge_trip(group) == None] #get groups into triplet structure\n",
    "    \n",
    "    # #turn matches into triples by only keeping those with coded verbs, return code instead of verb\n",
    "    # triples = []\n",
    "    # for match in matches:\n",
    "    #     if match[1].lower() in spec_dict:\n",
    "    #         for poss_pattern in spec_dict[match[1].lower()]:\n",
    "    #             if set(poss_pattern.split()).intersection(sentence.split()) == set(poss_pattern.split()):\n",
    "    #                 triples.append(f\"<triplet> {match[0]} <subj> {match[2]} <obj> {spec_code[poss_pattern]}\")\n",
    "                    \n",
    "    #     elif match[1].lower() in verb_dict:\n",
    "    #         triples.append(f\"<triplet> {match[0]} <subj> {match[2]} <obj> {verb_dict[match[1].lower()]}\")\n",
    "    #     else: print(f\"couldn't match {match[1].lower()}\")\n",
    "\n",
    "    # #triples = [f\"<triplet> {match[0]} <subj> {match[2]} <obj> {verb_dict[match[1].lower()]}\" for match in matches if match[1].lower() in verb_dict]\n",
    "\n",
    "    # return triples\n",
    "\n",
    "def merge_trip(df):\n",
    "    \"\"\"helper function to turn two rows of a pandas groupby into subj, verb, obj\"\"\"\n",
    "    if df.shape[0] == 2:\n",
    "        if df.noun_type.iloc[0] != df.noun_type.iloc[1]:\n",
    "            return [df.iloc[0].noun, df.iloc[0].verb, df.iloc[1].noun]\n",
    "    elif df.shape[0] > 2:\n",
    "        for i in range(df.shape[0] - 1):\n",
    "            if df.noun_type.iloc[i] != df.noun_type.iloc[i+1]:\n",
    "                return [df.iloc[0].noun, df.iloc[0].verb, df.iloc[1].noun]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "pico_path = r\"C:/Users/svawe/Thesis_RelationExtraction_PoliticsNews/soft_data/src/add_labels/dictionaries/PETR.Internal.Coding.Ontology.txt\"\n",
    "verb_path = r\"C:/Users/svawe/Thesis_RelationExtraction_PoliticsNews/soft_data/src/add_labels/dictionaries/newdict.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "read = read_lines(\"data/out_data/articles_url_coref2.csv.xml.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couldn't finde code in:  --- DEFEND  ###\n",
      "couldn't finde code in:  --- REVOKE_   ###\n",
      "couldn't finde code in:  --- SEND   ###\n",
      "couldn't finde code in:  --- COLLAPSE  ###\n",
      "22 patterns could not be loaded\n"
     ]
    }
   ],
   "source": [
    "verb_dict, spec_dict, spec_code = verb_code_dict(pico_path, verb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [99], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dfs_2 \u001b[39m=\u001b[39m [[sentence, get_triples(sentence, verb_dict, spec_dict, spec_code, nlp)] \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m read]\n",
      "Cell \u001b[1;32mIn [99], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dfs_2 \u001b[39m=\u001b[39m [[sentence, get_triples(sentence, verb_dict, spec_dict, spec_code, nlp)] \u001b[39mfor\u001b[39;00m sentence \u001b[39min\u001b[39;00m read]\n",
      "Cell \u001b[1;32mIn [95], line 193\u001b[0m, in \u001b[0;36mget_triples\u001b[1;34m(sentence, verb_dict, spec_dict, spec_code, nlp)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_triples\u001b[39m(sentence, verb_dict, spec_dict, spec_code, nlp):\n\u001b[0;32m    190\u001b[0m     \u001b[39m\"\"\"create triplet structure for training from text input, \u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[39m    verb_dict needs to be loaded before,\u001b[39;00m\n\u001b[0;32m    192\u001b[0m \u001b[39m    spacy model needs to be initialized before \"\"\"\u001b[39;00m\n\u001b[1;32m--> 193\u001b[0m     doc \u001b[39m=\u001b[39m nlp(sentence)\n\u001b[0;32m    194\u001b[0m     verbs \u001b[39m=\u001b[39m []\n\u001b[0;32m    195\u001b[0m     \u001b[39mdict\u001b[39m \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\spacy\\language.py:1020\u001b[0m, in \u001b[0;36mLanguage.__call__\u001b[1;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     error_handler \u001b[39m=\u001b[39m proc\u001b[39m.\u001b[39mget_error_handler()\n\u001b[0;32m   1019\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1020\u001b[0m     doc \u001b[39m=\u001b[39m proc(doc, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcomponent_cfg\u001b[39m.\u001b[39mget(name, {}))  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m   1021\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1022\u001b[0m     \u001b[39m# This typically happens if a component is not initialized\u001b[39;00m\n\u001b[0;32m   1023\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE109\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\spacy\\pipeline\\trainable_pipe.pyx:52\u001b[0m, in \u001b[0;36mspacy.pipeline.trainable_pipe.TrainablePipe.__call__\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\spacy\\pipeline\\tok2vec.py:125\u001b[0m, in \u001b[0;36mTok2Vec.predict\u001b[1;34m(self, docs)\u001b[0m\n\u001b[0;32m    123\u001b[0m     width \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mget_dim(\u001b[39m\"\u001b[39m\u001b[39mnO\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    124\u001b[0m     \u001b[39mreturn\u001b[39;00m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39malloc((\u001b[39m0\u001b[39m, width)) \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs]\n\u001b[1;32m--> 125\u001b[0m tokvecs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(docs)\n\u001b[0;32m    126\u001b[0m batch_id \u001b[39m=\u001b[39m Tok2VecListener\u001b[39m.\u001b[39mget_batch_id(docs)\n\u001b[0;32m    127\u001b[0m \u001b[39mfor\u001b[39;00m listener \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlisteners:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\thinc\\model.py:315\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X: InT) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m OutT:\n\u001b[0;32m    312\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function with `is_train=False`, and return\u001b[39;00m\n\u001b[0;32m    313\u001b[0m \u001b[39m    only the output, instead of the `(output, callback)` tuple.\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 315\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\thinc\\layers\\chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[0;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[0;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\thinc\\model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\thinc\\layers\\chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[0;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[0;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\thinc\\model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\thinc\\layers\\concatenate.py:44\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(model: Model[InT, OutT], X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m---> 44\u001b[0m     Ys, callbacks \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[layer(X, is_train\u001b[39m=\u001b[39mis_train) \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers])\n\u001b[0;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(Ys[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m):\n\u001b[0;32m     46\u001b[0m         data_l, backprop \u001b[39m=\u001b[39m _list_forward(model, X, Ys, callbacks, is_train)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\thinc\\layers\\concatenate.py:44\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(model: Model[InT, OutT], X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[1;32m---> 44\u001b[0m     Ys, callbacks \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39m[layer(X, is_train\u001b[39m=\u001b[39;49mis_train) \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers])\n\u001b[0;32m     45\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(Ys[\u001b[39m0\u001b[39m], \u001b[39mlist\u001b[39m):\n\u001b[0;32m     46\u001b[0m         data_l, backprop \u001b[39m=\u001b[39m _list_forward(model, X, Ys, callbacks, is_train)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\thinc\\model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\thinc\\layers\\chain.py:55\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     53\u001b[0m callbacks \u001b[39m=\u001b[39m []\n\u001b[0;32m     54\u001b[0m \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mlayers:\n\u001b[1;32m---> 55\u001b[0m     Y, inc_layer_grad \u001b[39m=\u001b[39m layer(X, is_train\u001b[39m=\u001b[39;49mis_train)\n\u001b[0;32m     56\u001b[0m     callbacks\u001b[39m.\u001b[39mappend(inc_layer_grad)\n\u001b[0;32m     57\u001b[0m     X \u001b[39m=\u001b[39m Y\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\thinc\\model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\thinc\\layers\\with_array.py:32\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, Xseq, is_train)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[0;32m     29\u001b[0m     model: Model[SeqT, SeqT], Xseq: SeqT, is_train: \u001b[39mbool\u001b[39m\n\u001b[0;32m     30\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[SeqT, Callable]:\n\u001b[0;32m     31\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(Xseq, Ragged):\n\u001b[1;32m---> 32\u001b[0m         \u001b[39mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], _ragged_forward(model, Xseq, is_train))\n\u001b[0;32m     33\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(Xseq, Padded):\n\u001b[0;32m     34\u001b[0m         \u001b[39mreturn\u001b[39;00m cast(Tuple[SeqT, Callable], _padded_forward(model, Xseq, is_train))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\thinc\\layers\\with_array.py:87\u001b[0m, in \u001b[0;36m_ragged_forward\u001b[1;34m(model, Xr, is_train)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_ragged_forward\u001b[39m(\n\u001b[0;32m     84\u001b[0m     model: Model[SeqT, SeqT], Xr: Ragged, is_train: \u001b[39mbool\u001b[39m\n\u001b[0;32m     85\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Ragged, Callable]:\n\u001b[0;32m     86\u001b[0m     layer: Model[ArrayXd, ArrayXd] \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m]\n\u001b[1;32m---> 87\u001b[0m     Y, get_dX \u001b[39m=\u001b[39m layer(Xr\u001b[39m.\u001b[39;49mdataXd, is_train)\n\u001b[0;32m     89\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(dYr: Ragged) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Ragged:\n\u001b[0;32m     90\u001b[0m         \u001b[39mreturn\u001b[39;00m Ragged(get_dX(dYr\u001b[39m.\u001b[39mdataXd), dYr\u001b[39m.\u001b[39mlengths)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\thinc\\model.py:291\u001b[0m, in \u001b[0;36mModel.__call__\u001b[1;34m(self, X, is_train)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, X: InT, is_train: \u001b[39mbool\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[OutT, Callable]:\n\u001b[0;32m    289\u001b[0m     \u001b[39m\"\"\"Call the model's `forward` function, returning the output and a\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[39m    callback to compute the gradients via backpropagation.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 291\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_func(\u001b[39mself\u001b[39;49m, X, is_train\u001b[39m=\u001b[39;49mis_train)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\thinc\\layers\\concatenate.py:52\u001b[0m, in \u001b[0;36mforward\u001b[1;34m(model, X, is_train)\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(OutT, data_r), backprop\n\u001b[0;32m     51\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m     data_a, backprop \u001b[39m=\u001b[39m _array_forward(model, X, Ys, callbacks, is_train)\n\u001b[0;32m     53\u001b[0m     \u001b[39mreturn\u001b[39;00m cast(OutT, data_a), backprop\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\thinc\\layers\\concatenate.py:60\u001b[0m, in \u001b[0;36m_array_forward\u001b[1;34m(model, X, Ys, callbacks, is_train)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_array_forward\u001b[39m(\n\u001b[0;32m     57\u001b[0m     model: Model[InT, OutT], X, Ys: List, callbacks, is_train: \u001b[39mbool\u001b[39m\n\u001b[0;32m     58\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Array2d, Callable]:\n\u001b[0;32m     59\u001b[0m     widths \u001b[39m=\u001b[39m [Y\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m Y \u001b[39min\u001b[39;00m Ys]\n\u001b[1;32m---> 60\u001b[0m     output \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mops\u001b[39m.\u001b[39;49mxp\u001b[39m.\u001b[39;49mhstack(Ys)\n\u001b[0;32m     62\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mbackprop\u001b[39m(d_output: Array2d) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m InT:\n\u001b[0;32m     63\u001b[0m         dY \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mops\u001b[39m.\u001b[39mas_contig(d_output[:, : widths[\u001b[39m0\u001b[39m]])\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\core\\shape_base.py:345\u001b[0m, in \u001b[0;36mhstack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    343\u001b[0m     \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39mconcatenate(arrs, \u001b[39m0\u001b[39m)\n\u001b[0;32m    344\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 345\u001b[0m     \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39;49mconcatenate(arrs, \u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "dfs_2 = [[sentence, get_triples(sentence, verb_dict, spec_dict, spec_code, nlp)] for sentence in read]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21395"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfs_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['AdvertisingRead moreThis live page is no longer being updated.',\n",
       "  Empty DataFrame\n",
       "  Columns: [idx, verb, noun, noun_type, noun_map]\n",
       "  Index: []],\n",
       " ['For more on\\xa0our coverage of the war in Ukraine, click here.11:44pm:\\xa0UN grain coordinator\\xa0expects loaded ships to depart Ukraine on ThursdayThe UN coordinator for the Ukraine Black Sea grain deal said UN grain coordinator expects loaded ships to depart Ukrainian ports on ThursdayThe.',\n",
       "     idx    verb     noun noun_type  noun_map\n",
       "  0  113  depart  Ukraine      dobj         2],\n",
       " ['“Exports of grain and foodstuffs from Ukraine need to continue.',\n",
       "  Empty DataFrame\n",
       "  Columns: [idx, verb, noun, noun_type, noun_map]\n",
       "  Index: []],\n",
       " ['Although no movements of vessels are planned for 2 November under the #BlackSeaGrainInitiative, we expect loaded ships to sail on ThursdayThe,” UN coordinator Amir Abdulla posted on Twitter.',\n",
       "     idx  verb          noun noun_type  noun_map\n",
       "  0  172  post  Amir Abdulla     nsubj         1],\n",
       " ['Exports of grain and foodstuffs from Ukraine need to continue.',\n",
       "  Empty DataFrame\n",
       "  Columns: [idx, verb, noun, noun_type, noun_map]\n",
       "  Index: []]]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The UN Secretariat at coordination centreThere reports that the Ukrainian, Turkish and UN delegations agreed not to plan any movement of vessels in the Black Sea Grain Initiative for 2 November,\" \"The UN Secretariat at the Joint Coordination Centre said Tuesday, referring to the July deal brokered by Turkey and the UN.6:15pm:\\xa0Russian President Vladimir Putin tells Erdogan Russian President Vladimir Putin wants \\'real guarantees\\' from Kyiv on grain deal, says Russian President Vladimir Putin told Erdogan Tuesday that Russian President Vladimir Putin wanted \"real guarantees\" from Kyiv before Kyiv potentially rejoined grain deal.',\n",
       "    idx    verb                noun noun_type  noun_map\n",
       " 0   47  report  The UN Secretariat     nsubj         1\n",
       " 1  361    tell      Vladimir Putin     nsubj         1\n",
       " 2  408    want      Vladimir Putin     nsubj         1\n",
       " 3  495    tell      Vladimir Putin     nsubj         1\n",
       " 4  495    tell             Erdogan      dobj         2\n",
       " 5  554    want      Vladimir Putin     nsubj         1\n",
       " 6  613  rejoin                Kyiv     nsubj         1]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_2[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidats_2 = [df for df in dfs_2 if df[1].shape[0] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6454"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidats_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     47\n",
       "1    361\n",
       "2    408\n",
       "3    495\n",
       "4    495\n",
       "5    554\n",
       "6    613\n",
       "Name: idx, dtype: int64"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_2[13][1][\"idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_2 = []\n",
    "for can in candidats_2:\n",
    "    for idx in can[1][\"idx\"]:\n",
    "        if can[1][\"idx\"].to_list().count(idx) > 1:\n",
    "            trips_2.append([can[0], can[1]])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2395"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trips_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_2 = []\n",
    "for idx, df in enumerate(trips_2):\n",
    "    #create groups that resolve around same word\n",
    "    gb = df[1].groupby('idx')  \n",
    "    #only keep groups if verb idx was identified as potential triplet before, sort by priority for structure\n",
    "    for x in gb.groups:\n",
    "        group = gb.get_group(x).sort_values(\"noun_map\")\n",
    "        if group.shape[0] == 2:\n",
    "            if group.noun_type.iloc[0] != group.noun_type.iloc[1]:\n",
    "                matches_2.append([df[0], group.iloc[0].noun, group.iloc[0].verb, group.iloc[1].noun])\n",
    "        elif group.shape[0] > 2:\n",
    "            for i in range(group.shape[0] - 1):\n",
    "                if group.noun_type.iloc[i] != group.noun_type.iloc[i+1]:\n",
    "                    matches_2.append([df[0], group.iloc[0].noun, group.iloc[0].verb, group.iloc[1].noun])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couldn't match evaluate\n",
      "couldn't match co\n",
      "couldn't match roll\n",
      "couldn't match love\n",
      "couldn't match feel\n",
      "couldn't match have\n",
      "couldn't match convince\n",
      "couldn't match teach\n",
      "couldn't match soak\n",
      "couldn't match think\n",
      "couldn't match feel\n",
      "couldn't match bind\n",
      "couldn't match recruit\n",
      "couldn't match classify\n",
      "couldn't match leak\n",
      "couldn't match begrudge\n",
      "couldn't match lose\n",
      "couldn't match become\n",
      "couldn't match have\n",
      "couldn't match have\n",
      "couldn't match renew\n",
      "couldn't match shift\n",
      "couldn't match stump\n",
      "couldn't match modify\n",
      "couldn't match exert\n",
      "couldn't match position\n",
      "couldn't match know\n",
      "couldn't match suspend\n",
      "couldn't match pack\n",
      "couldn't match know\n",
      "couldn't match have\n",
      "couldn't match exhaust\n",
      "couldn't match turn\n",
      "couldn't match have\n",
      "couldn't match type\n",
      "couldn't match post\n",
      "couldn't match upload\n",
      "couldn't match pack\n",
      "couldn't match turn\n",
      "couldn't match spare\n",
      "couldn't match to\n",
      "couldn't match position\n",
      "couldn't match do\n",
      "couldn't match evaluate\n",
      "couldn't match download\n",
      "couldn't match repopulate\n",
      "couldn't match discredit\n",
      "couldn't match suspend\n",
      "couldn't match suspend\n",
      "couldn't match intensify\n",
      "couldn't match mishandle\n",
      "couldn't match nominate\n",
      "couldn't match do\n",
      "couldn't match overtake\n",
      "couldn't match tap\n",
      "couldn't match be\n",
      "couldn't match nudge\n",
      "couldn't match post\n",
      "couldn't match update\n",
      "couldn't match shake\n",
      "couldn't match know\n",
      "couldn't match lower\n",
      "couldn't match stoke\n",
      "couldn't match delegitimise\n",
      "couldn't match tweet\n",
      "couldn't match have\n",
      "couldn't match isolate\n",
      "couldn't match love\n",
      "couldn't match feel\n",
      "couldn't match remember\n",
      "couldn't match have\n",
      "couldn't match remind\n",
      "couldn't match gloss\n",
      "couldn't match reflect\n",
      "couldn't match have\n",
      "couldn't match earn\n",
      "couldn't match swear\n",
      "couldn't match convince\n",
      "couldn't match near\n",
      "couldn't match macribookpara\n",
      "couldn't match be\n",
      "couldn't match love\n",
      "couldn't match thrill\n",
      "couldn't match like\n",
      "couldn't match become\n",
      "couldn't match crown\n",
      "couldn't match stoke\n",
      "couldn't match owe\n",
      "couldn't match catch\n",
      "couldn't match update\n",
      "couldn't match have\n",
      "couldn't match finish\n",
      "couldn't match like\n",
      "couldn't match know\n",
      "couldn't match downplay\n",
      "couldn't match downgrade\n",
      "couldn't match sack\n",
      "couldn't match have\n",
      "couldn't match play\n",
      "couldn't match feed\n",
      "couldn't match have\n",
      "couldn't match recycle\n",
      "couldn't match paint\n",
      "couldn't match wean\n",
      "couldn't match ramp\n",
      "couldn't match intensify\n",
      "couldn't match mishandle\n",
      "couldn't match suspend\n",
      "couldn't match sideline\n",
      "couldn't match lose\n",
      "couldn't match bolster\n",
      "couldn't match slow\n",
      "couldn't match become\n",
      "couldn't match become\n",
      "couldn't match write\n",
      "couldn't match suspend\n",
      "couldn't match revert\n",
      "couldn't match suspend\n",
      "couldn't match intensify\n",
      "couldn't match hang\n",
      "couldn't match overtake\n",
      "couldn't match content\n",
      "couldn't match trail\n",
      "couldn't match have\n",
      "couldn't match evaluate\n",
      "couldn't match lose\n",
      "couldn't match have\n",
      "couldn't match alter\n",
      "couldn't match lose\n",
      "couldn't match focus\n",
      "couldn't match evaluate\n",
      "couldn't match do\n",
      "couldn't match suspend\n",
      "couldn't match suspend\n",
      "couldn't match survey\n",
      "couldn't match have\n",
      "couldn't match write\n",
      "couldn't match read\n",
      "couldn't match like\n",
      "couldn't match turn\n",
      "couldn't match listen\n",
      "couldn't match treat\n",
      "couldn't match barnstorm\n",
      "couldn't match focus\n",
      "couldn't match have\n",
      "couldn't match rethink\n",
      "couldn't match position\n",
      "couldn't match have\n",
      "couldn't match overhaul\n",
      "couldn't match overcome\n",
      "couldn't match do\n",
      "couldn't match evaluate\n",
      "couldn't match secure\n",
      "couldn't match finish\n",
      "couldn't match write\n",
      "couldn't match have\n",
      "couldn't match do\n",
      "couldn't match evaluate\n",
      "couldn't match acquire\n",
      "couldn't match diversify\n",
      "couldn't match reflect\n",
      "couldn't match copy\n",
      "couldn't match exhaust\n",
      "couldn't match turn\n",
      "couldn't match shrink\n",
      "couldn't match backtrack\n",
      "couldn't match restart\n",
      "couldn't match suspend\n",
      "couldn't match sack\n",
      "couldn't match suspend\n",
      "couldn't match suspend\n",
      "couldn't match suspend\n",
      "couldn't match eat\n",
      "couldn't match retake\n",
      "couldn't match download\n",
      "couldn't match do\n",
      "couldn't match portray\n",
      "couldn't match play\n",
      "couldn't match don\n",
      "couldn't match singe\n",
      "couldn't match suspend\n",
      "couldn't match singe\n",
      "couldn't match widen\n",
      "couldn't match finish\n",
      "couldn't match treat\n",
      "couldn't match birth\n",
      "couldn't match have\n",
      "couldn't match lose\n",
      "couldn't match suspend\n",
      "couldn't match suspend\n",
      "couldn't match suspend\n",
      "couldn't match have\n",
      "couldn't match interrupt\n",
      "couldn't match do\n",
      "couldn't match have\n",
      "couldn't match cover\n",
      "couldn't match do\n",
      "couldn't match hug\n",
      "couldn't match permit\n",
      "couldn't match reinstate\n",
      "couldn't match reinstate\n",
      "couldn't match answer\n",
      "couldn't match reflect\n",
      "couldn't match exploit\n",
      "couldn't match suspend\n",
      "couldn't match suspend\n",
      "couldn't match suspend\n",
      "couldn't match alter\n",
      "couldn't match hug\n",
      "couldn't match do\n",
      "couldn't match turn\n",
      "couldn't match court\n",
      "couldn't match be\n",
      "couldn't match differentiate\n",
      "couldn't match have\n",
      "couldn't match shrink\n",
      "couldn't match backtrack\n",
      "couldn't match diversify\n",
      "couldn't match distract\n",
      "couldn't match outrun\n",
      "couldn't match single\n",
      "couldn't match ramp\n",
      "couldn't match discard\n",
      "couldn't match suspend\n",
      "couldn't match suspend\n",
      "couldn't match suspend\n",
      "couldn't match download\n",
      "couldn't match turn\n",
      "couldn't match inherit\n",
      "couldn't match single\n",
      "couldn't match single\n",
      "couldn't match overcome\n",
      "couldn't match outsmart\n",
      "couldn't match turn\n",
      "couldn't match rail\n",
      "couldn't match become\n",
      "couldn't match finish\n",
      "couldn't match disqualify\n",
      "couldn't match sleep\n",
      "couldn't match star\n",
      "couldn't match play\n",
      "couldn't match rename\n",
      "couldn't match exploit\n",
      "couldn't match have\n",
      "couldn't match sink\n",
      "couldn't match run\n",
      "couldn't match ramp\n",
      "couldn't match discard\n",
      "couldn't match intoto\n",
      "couldn't match reporters.1:59pm\n",
      "couldn't match suspend\n",
      "couldn't match pitch\n",
      "couldn't match carve\n",
      "couldn't match plunk\n",
      "couldn't match have\n",
      "couldn't match lose\n",
      "couldn't match disqualify\n",
      "couldn't match have\n",
      "couldn't match sink\n",
      "couldn't match intoto\n",
      "couldn't match reporters.1:59pm\n",
      "couldn't match suspend\n",
      "couldn't match lose\n",
      "couldn't match ramp\n",
      "couldn't match ramp\n",
      "couldn't match miss\n",
      "couldn't match suspend\n",
      "couldn't match play\n",
      "couldn't match finish\n",
      "couldn't match chip\n",
      "couldn't match know\n",
      "couldn't match bury\n",
      "couldn't match feel\n",
      "couldn't match intoto\n",
      "couldn't match reporters.1:59pm\n",
      "couldn't match suspend\n",
      "couldn't match download\n",
      "couldn't match have\n",
      "couldn't match reassess\n",
      "couldn't match work\n",
      "couldn't match cement\n",
      "couldn't match feel\n",
      "couldn't match do\n",
      "couldn't match consolidate\n",
      "couldn't match intoto\n",
      "couldn't match reporters.1:59pm\n",
      "couldn't match suspend\n",
      "couldn't match tweet\n",
      "couldn't match distract\n",
      "couldn't match energize\n",
      "couldn't match convince\n",
      "couldn't match climb\n",
      "couldn't match cement\n",
      "couldn't match elevate\n",
      "couldn't match outperform\n",
      "couldn't match edge\n",
      "couldn't match outstrip\n",
      "couldn't match have\n",
      "couldn't match suspend\n",
      "couldn't match prolong\n",
      "couldn't match consolidate\n",
      "couldn't match download\n",
      "couldn't match renew\n",
      "couldn't match ramp\n",
      "couldn't match intoto\n",
      "couldn't match reporters.1:59pm\n",
      "couldn't match suspend\n",
      "couldn't match download\n",
      "couldn't match operate\n",
      "couldn't match have\n",
      "couldn't match balance\n",
      "couldn't match balance\n",
      "couldn't match hawk\n",
      "couldn't match hawk\n",
      "couldn't match hawk\n",
      "couldn't match feel\n",
      "couldn't match alienate\n",
      "couldn't match convince\n",
      "couldn't match climb\n",
      "couldn't match miss\n",
      "couldn't match cement\n",
      "couldn't match uninvite\n",
      "couldn't match have\n",
      "couldn't match suspend\n",
      "couldn't match prolong\n",
      "couldn't match rapt\n",
      "couldn't match practice\n",
      "couldn't match download\n",
      "couldn't match download\n",
      "couldn't match ram\n",
      "couldn't match suspend\n",
      "couldn't match dial\n",
      "couldn't match suspend\n",
      "couldn't match reconstitute\n",
      "couldn't match post\n",
      "couldn't match suspend\n",
      "couldn't match prolong\n",
      "couldn't match ram\n",
      "couldn't match download\n",
      "couldn't match download\n",
      "couldn't match suspend\n",
      "couldn't match suspend\n",
      "couldn't match pause\n",
      "couldn't match Press)In\n",
      "couldn't match lose\n",
      "couldn't match reclaim\n",
      "couldn't match peg\n",
      "couldn't match stoke\n",
      "couldn't match have\n",
      "couldn't match accompany\n",
      "couldn't match suspend\n",
      "couldn't match justify\n",
      "couldn't match prevail\n",
      "couldn't match post\n",
      "couldn't match destabilise\n",
      "couldn't match front\n",
      "couldn't match like\n",
      "couldn't match transform\n",
      "couldn't match label\n",
      "couldn't match assail\n",
      "couldn't match seal\n",
      "couldn't match por\n",
      "couldn't match number\n",
      "couldn't match respond\n",
      "couldn't match have\n",
      "couldn't match feel\n",
      "couldn't match be\n",
      "couldn't match have\n",
      "couldn't match run\n",
      "couldn't match portray\n",
      "couldn't match front\n",
      "couldn't match become\n",
      "couldn't match work\n",
      "couldn't match lull\n",
      "couldn't match liberalize\n",
      "couldn't match prevail\n",
      "couldn't match compare\n",
      "couldn't match wean\n",
      "couldn't match lose\n",
      "couldn't match download\n",
      "couldn't match front\n",
      "couldn't match like\n",
      "couldn't match transform\n",
      "couldn't match label\n",
      "couldn't match accompany\n",
      "couldn't match loom\n",
      "couldn't match reference\n",
      "couldn't match have\n",
      "couldn't match lose\n",
      "couldn't match decrease\n",
      "couldn't match work\n",
      "couldn't match run\n",
      "couldn't match become\n"
     ]
    }
   ],
   "source": [
    "triples_2 = []\n",
    "ma_df = pd.DataFrame(matches_2, columns = [\"text\",\"subj\", \"verb\",\"obj\"])\n",
    "for row in ma_df.iterrows():\n",
    "    if row[1][\"verb\"] in spec_dict:\n",
    "        for poss_pattern in spec_dict[row[1][\"verb\"]]:\n",
    "            if set(poss_pattern.split()).intersection(row[1][\"text\"].split()) == set(poss_pattern.split()):\n",
    "                triples_2.append([row[1][\"text\"], row[1]['subj'], row[1]['obj'] , spec_code[poss_pattern]])\n",
    "    elif row[1][\"verb\"] in verb_dict:\n",
    "            triples_2.append([row[1][\"text\"], row[1]['subj'], row[1]['obj'], verb_dict[row[1]['verb']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(triples_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "probs_l = []\n",
    "for row in huh.iterrows():\n",
    "    premise = row[1][\"text\"]\n",
    "    subj = row[1][\"subj\"]\n",
    "    rel = row[1][\"label\"]\n",
    "    obj =  row[1][\"obj\"]\n",
    "\n",
    "    hypothesis = f'{subj} does {rel} towards {obj}.'\n",
    "\n",
    "    # run through model pre-trained on MNLI\n",
    "    x = tokenizer.encode(premise, hypothesis, return_tensors='pt',\n",
    "                        truncation_strategy='only_first')\n",
    "    logits = nli_model(x)[0]\n",
    "\n",
    "    # we throw away \"neutral\" (dim 1) and take the probability of\n",
    "    # \"entailment\" (2) as the probability of the label being true \n",
    "    entail_contradiction_logits = logits[:,[0,2]]\n",
    "    probs = entail_contradiction_logits.softmax(dim=1)\n",
    "    prob_label_is_true = probs[:,1]\n",
    "\n",
    "    probs_l.append([row[0], prob_label_is_true.item()])\n",
    "    if row[0] % 100 == 0: print(row[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrial with chunks instead of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import xml.etree.ElementTree as ET\n",
    "from spacy.symbols import nsubj, dobj, pobj, iobj, neg, xcomp, VERB\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def read_lines(inputparsed):    \n",
    "    \"\"\"takes input from CoreNLP sentence parsed file and returns sentences\"\"\"\n",
    "    #parse all lines from CoreNLP sentence split\n",
    "    parsed = open(inputparsed, encoding = \"utf-8\")\n",
    "    parsedfile = parsed.readlines()\n",
    "    parsedlines = []\n",
    "\n",
    "    #Only keep those lines which have Sentence #n in the line before\n",
    "    for idx, text in enumerate(parsedfile):\n",
    "        if text.startswith(\"Sentence #\"):\n",
    "            parsedlines.append(parsedfile[idx+1].replace('\\n','').strip())\n",
    "    \n",
    "    return parsedlines\n",
    "\n",
    "def gen_poss(line, verb_match, pre_dict):\n",
    "    \"\"\"generates all possibilities of patterns that a multi word line implies,\n",
    "    by extracting partial patterns and resolving placeholder words\"\"\"\n",
    "    poss = []\n",
    "\n",
    "    #replace special tokens in text that are clear at this point\n",
    "    line = line.replace(\"*\", verb_match)\n",
    "    line = line.replace(\"- \", \"\")\n",
    "    line = line.replace(\"+\",\"\")\n",
    "    line = line.replace(\"%\",\"\")\n",
    "    line = line.replace(\"^\",\"\")\n",
    "    line = line.replace(\"$\",\"\")\n",
    "\n",
    "    #split line by possibility indicators and code (always ends possibility)\n",
    "    #example.: \"- $ * (P ON KILLING (P OF + [010] #  COMMENT <ELH 07 May 2008>\"\n",
    "    poss_split = re.split(\"\\(P |\\[.*]\",line) \n",
    "\n",
    "    if len(poss_split) > 2: #2 is if no (P in the line\n",
    "        #only combining the first (P, as they share the same code \n",
    "        #and the longer version will never be contained in a text if the shorter isnt\n",
    "        poss.append(strip_multiple_whitespaces(\" \".join(poss_split[:2])).lower().rstrip().lstrip())\n",
    "    else: \n",
    "        poss.append(strip_multiple_whitespaces(poss_split[0].lower().rstrip().lstrip()))\n",
    "\n",
    "    cleaned = []\n",
    "    for text in list(set(poss)):\n",
    "        c = 0\n",
    "        for tag in list(pre_dict.keys()):\n",
    "            if tag in text:\n",
    "                for replacement in pre_dict[tag]:\n",
    "                    cleaned.append(text.replace(tag, replacement))\n",
    "                    c += 1\n",
    "        if c == 0:\n",
    "            cleaned.append(text)\n",
    "\n",
    "    return cleaned\n",
    "    \n",
    "\n",
    "def verb_code_dict(pico_path, verb_path):\n",
    "    \"\"\"reads coding ontology and verb lists, \n",
    "    directly matches verbs to their CAMEO codes and returns this verbs:codes dictionairy.\n",
    "    verb with codes that cannot be read are printed out as full line of the file\"\"\"\n",
    "    #read PETRARCH Internal Coding Ontology (= pico)\n",
    "    pico_path = os.path.join(os.getcwd(), pico_path)\n",
    "    pico_file = open(pico_path, 'r')\n",
    "    pico_lines = pico_file.readlines()\n",
    "\n",
    "    #get all 20 codes with their respective code\n",
    "    main_codes = {}                             #we run one iteration for all the main codes, only main codes contain relation name\n",
    "    for line in pico_lines:\n",
    "        line = line.split('#')\n",
    "        if line[0] == \"\" or line[0] == \"\\n\":    #only intro comments and empty lines\n",
    "            continue\n",
    "        else: \n",
    "            code_split = line[0].split(\":\")     #splits into CAMEO code and related hex\n",
    "            if len(line) > 1 and code_split[0][2] == \"0\":      #only main categories have 0 in 3rd idx, [cat_num 0] -> [010]\n",
    "                main_codes[code_split[0][:2]] = line[-1].replace(\"\\n\",\"\")\n",
    "    \n",
    "    #map code to code we want to use in the training\n",
    "    map_codes = {\"DiplomaticCoop\" : \"Engage In Diplomatic Cooperation\", \n",
    "                \"MaterialCoop\" : \"Engage In Material Cooperation\",\n",
    "                \"ProvideAid\" : \"Provide Aid\",\n",
    "                \"Exhibit Force Posture\": \"Exhibit Military Posture\",\n",
    "                \"Use Unconventional Mass Violence\" : \"Engage In Unconventional Mass Violence\"}\n",
    "    main_codes = {k: (map_codes[v] if v in map_codes else v) for k, v in main_codes.items()}\n",
    "    \n",
    "    #read single word patterns and match their code to the relation extracted in main_codes\n",
    "    verb_path = os.path.join(os.getcwd(), verb_path)\n",
    "    verb_file = open(verb_path, 'r')\n",
    "    verb_lines = verb_file.readlines()\n",
    "    \n",
    "    verb_dict = {}\n",
    "    for line in verb_lines:\n",
    "        if line[0] == \"#\":\n",
    "            continue\n",
    "        elif line.startswith(\"---\"):    #main verbs have a lead code, which is applied to all very in the section\n",
    "                                        #unless a separate code is specified for a specific verb in section\n",
    "            try: cur_main_code = re.split(\"\\[|\\]|---\", line)[2].replace(\":\",\"\")[:2]  #we only need main codes which are first two numbers\n",
    "                                                                                #sometimes code starts with \":\", e.g.: ---  OFFEND   [:110]  ---\n",
    "                                                                                #we just remove those to get the main code\n",
    "            except:                     #depending on chosen verb dictionairy, there may be main verbs without lead codes\n",
    "                print(\"couldn't finde code in: \", line.replace(\"\\n\",\"\")) \n",
    "                cur_main_code == \"--\"\n",
    "            if cur_main_code == \"\": cur_main_code = \"--\"\n",
    "        elif line == \"\\n\":              #skip empty lines\n",
    "            continue\n",
    "        elif line[0] == \"-\" or line[0] == \"~\" or line[0] == \"+\" or line[0] == \"&\": #removes all special structures we cannot use\n",
    "            continue\n",
    "        else:\n",
    "            if len(re.split(\"\\[|\\]\", line)) > 1:    #verbs with their own code, e.g.: AFFIRM [051] \n",
    "                code = re.split(\"\\[|\\]\", line)[1].replace(\":\",\"\")[:2]\n",
    "                if code != \"--\":\n",
    "                    if \"{\" in line:         #conjugated verbs, e.g. \"APPLY {APPLYING APPLIED APPLIES } [020]\"\n",
    "                        line_s = re.split(\"\\{|\\}\", line)    #split at { and }\n",
    "                        verb_dict[line_s[0].lower()] = main_codes[code] \n",
    "                        for word in line_s[1].split():\n",
    "                            verb_dict[word.lower()] = main_codes[code]\n",
    "                    else:\n",
    "                        word = re.split(\"\\[|\\]\", line)[0]\n",
    "                        verb_dict[word.lower()] = main_codes[code]\n",
    "            else:\n",
    "                if cur_main_code != \"--\":\n",
    "                    if \"{\" in line:         #e.g. \"HURRY {HURRIES HURRYING HURRIED }\" \n",
    "                        line_s = re.split(\"\\{|\\}\", line)    #split at { and }\n",
    "                        verb_dict[line_s[0].lower()] = main_codes[cur_main_code]\n",
    "                        for word in line_s[1].split():\n",
    "                            verb_dict[word.lower()] = main_codes[cur_main_code]\n",
    "                    else:                   #only single words with sometimes comments, e.g.: CENSURE  # JON 5/17/95\n",
    "                        word = line.split(\"#\")[0].rstrip()    #gets part before \"#\", removes all whitespaces to the right\n",
    "                        verb_dict[word.lower()] = main_codes[cur_main_code]\n",
    "\n",
    "    #read multi word patterns and create a dictionary for their code\n",
    "\n",
    "    #get filler words that occur in multi word patterns\n",
    "    verb_file = open(verb_path, 'r')\n",
    "    verb_lines = verb_file.readlines()\n",
    "\n",
    "    pre_dict = {}\n",
    "    filter_list = []\n",
    "    for line in verb_lines:\n",
    "        if line.startswith(\"&\"):\n",
    "            cur_filter = line.rstrip()\n",
    "        elif line.startswith(\"\\n\") and \"cur_filter\" in locals():\n",
    "            pre_dict[cur_filter.lower()] = filter_list\n",
    "            cur_filter = \"\"\n",
    "            filter_list = []\n",
    "        elif line.startswith(\"+\") and cur_filter != \"\":\n",
    "            filter_list.append(line.rstrip()[1:].replace(\"_\", \"\").lower())\n",
    "    del pre_dict[\"\"]\n",
    "\n",
    "    #generate dictionaries for multi word patterns\n",
    "    verb_file = open(verb_path, 'r')\n",
    "    verb_lines = verb_file.readlines()\n",
    "\n",
    "    spec_dict = {}\n",
    "    spec_code = {}\n",
    "\n",
    "    count = 0\n",
    "    for line in verb_lines:\n",
    "        if line.startswith(\"- \"):\n",
    "            #get main verb as dict key\n",
    "            try: \n",
    "                verb_match = re.search(\"# *\\w+\", line).group()\n",
    "                verb_match = re.search(\"\\w+\", verb_match).group()\n",
    "                verb_match = verb_match.replace(\"_\", \" \").lower()\n",
    "            except: \n",
    "                count += 1\n",
    "\n",
    "            #get code for line\n",
    "            try:\n",
    "                code = re.search(\"\\[.*]\", line).group()[1:3]\n",
    "                if code != \"--\":\n",
    "                    #get all possibility that the line indicates\n",
    "                    poss = gen_poss(line, verb_match, pre_dict)\n",
    "                    for pattern in poss:\n",
    "                        spec_code[pattern] = main_codes[code]\n",
    "                    spec_dict[verb_match] = poss\n",
    "            except:\n",
    "                count += 1\n",
    "\n",
    "    print(f\"{count} patterns could not be loaded\")        \n",
    "\n",
    "    return verb_dict, spec_dict, spec_code\n",
    "\n",
    "\n",
    "def get_triples(sentence, verb_dict, spec_dict, spec_code, nlp):\n",
    "    \"\"\"create triplet structure for training from text input, \n",
    "    verb_dict needs to be loaded before,\n",
    "    spacy model needs to be initialized before \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    verbs = []\n",
    "    dict = {}\n",
    "\n",
    "\n",
    "    for possible_verb in doc:\n",
    "        if possible_verb.pos == VERB:\n",
    "            if neg in [child.dep for child in possible_verb.children]: continue\n",
    "            else: \n",
    "                for possible_subject in possible_verb.children: \n",
    "                    if possible_subject.dep == xcomp:   #subj / obj of composed verb should also be subj / obj of main verb\n",
    "                        main_verb = possible_subject\n",
    "                        main_idx = possible_subject.idx\n",
    "                        \n",
    "                        for chunk in doc.noun_chunks:\n",
    "                            if chunk.root.dep_ == \"poss\":\n",
    "                                if chunk.root.head.head.idx == possible_verb.idx:\n",
    "                                    verbs.append([main_idx, main_verb.lemma_, chunk.text, chunk.root.head.dep_])\n",
    "                                    if main_idx in dict.keys(): dict[main_idx] += 1\n",
    "                                    else: dict[main_idx] = 1\n",
    "                            else:\n",
    "                                if chunk.root.head.idx == possible_verb.idx:\n",
    "                                    verbs.append([main_idx, main_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "                                    if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "                                    else: dict[possible_verb.idx] = 1\n",
    "\n",
    "                for chunk in doc.noun_chunks:       #for normal verbs, check chunks directly\n",
    "        #                 if chunk.root.head.idx == possible_verb.idx:\n",
    "                    if chunk.root.head.dep_ == \"poss\":\n",
    "                        if chunk.root.head.head.idx == possible_verb.idx:\n",
    "                            verbs.append([possible_verb.idx, possible_verb.lemma_, chunk.text, chunk.root.head.dep_])\n",
    "                            if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "                            else: dict[possible_verb.idx] = 1\n",
    "                    else:\n",
    "                        if chunk.root.head.idx == possible_verb.idx:\n",
    "                            verbs.append([possible_verb.idx, possible_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "                            if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "                            else: dict[possible_verb.idx] = 1\n",
    "\n",
    "    trip_idx = [key for key in dict if dict[key] > 1]\n",
    "\n",
    "    # doc = nlp(sentence)\n",
    "    # verbs = []\n",
    "    # dict = {}\n",
    "\n",
    "    # for possible_verb in doc:           #parses through all words in sentence\n",
    "    #     if possible_verb.pos == VERB:   #we only care about verbs\n",
    "    #         if neg in [child.dep for child in possible_verb.children]: continue #we exclude all negated verbs\n",
    "    #         else: \n",
    "    #             for candidate in possible_verb.children: #for composed verbs of verb (e.g. \"want to join\" -> \"want join\")\n",
    "    #                 if candidate.dep == xcomp:   #subj / obj of composed verb should also be subj / obj of main verb\n",
    "    #                     main_verb = candidate    \n",
    "    #                     main_idx = candidate.idx\n",
    "    #                     for chunk in doc.noun_chunks:   #chunks are noun-groups (e.g.: \"78 out of 100 people\" instead of \"people\")\n",
    "    #                         if chunk.root.head.idx == possible_verb.idx:    #if chunk applies to xcomp (want),\n",
    "    #                                                                         #treat it like it aplles to main verb (\"join\")\n",
    "    #                             verbs.append([main_idx, main_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "    #                             if main_idx in dict.keys(): dict[main_idx] += 1 #count how often verb is used\n",
    "    #                             else: dict[main_idx] = 1\n",
    "\n",
    "    #             for chunk in doc.noun_chunks:       #for normal verbs, check chunks directly\n",
    "    #                 if chunk.root.head.idx == possible_verb.idx:\n",
    "    #                     verbs.append([possible_verb.idx, possible_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "    #                     if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "    #                     else: dict[possible_verb.idx] = 1\n",
    "    \n",
    "    # trip_idx = [key for key in dict if dict[key] > 1]   #if verbs used more than once, its candidate for triplet\n",
    "\n",
    "    #priority for subj-relation-obj triplets\n",
    "    mapper = {\"nsubj\":1,\"dobj\":2, \"pobj\":2, \"iobj\":2}\n",
    "\n",
    "    #create df from verbs extracted \n",
    "    df = pd.DataFrame(verbs, columns = [\"idx\", \"verb\", \"noun\", \"noun_type\"])\n",
    "    df[\"noun_map\"] = df.noun_type.map(mapper)  #turn noun_types into priority \n",
    "    return df\n",
    "\n",
    "    # #create groups that resolve around same word\n",
    "    # gb = df.groupby('idx')    \n",
    "    # #only keep groups if verb idx was identified as potential triplet before, sort by priority for structure\n",
    "    # df_l = [gb.get_group(x).sort_values(\"noun_map\") for x in gb.groups if gb.get_group(x).idx.iloc[0] in dict]\n",
    "    # matches = [merge_trip(group) for group in df_l if not merge_trip(group) == None] #get groups into triplet structure\n",
    "    \n",
    "    # #turn matches into triples by only keeping those with coded verbs, return code instead of verb\n",
    "    # triples = []\n",
    "    # for match in matches:\n",
    "    #     if match[1].lower() in spec_dict:\n",
    "    #         for poss_pattern in spec_dict[match[1].lower()]:\n",
    "    #             if set(poss_pattern.split()).intersection(sentence.split()) == set(poss_pattern.split()):\n",
    "    #                 triples.append(f\"<triplet> {match[0]} <subj> {match[2]} <obj> {spec_code[poss_pattern]}\")\n",
    "                    \n",
    "    #     elif match[1].lower() in verb_dict:\n",
    "    #         triples.append(f\"<triplet> {match[0]} <subj> {match[2]} <obj> {verb_dict[match[1].lower()]}\")\n",
    "    #     else: print(f\"couldn't match {match[1].lower()}\")\n",
    "\n",
    "    # #triples = [f\"<triplet> {match[0]} <subj> {match[2]} <obj> {verb_dict[match[1].lower()]}\" for match in matches if match[1].lower() in verb_dict]\n",
    "\n",
    "    # return triples\n",
    "\n",
    "def merge_trip(df):\n",
    "    \"\"\"helper function to turn two rows of a pandas groupby into subj, verb, obj\"\"\"\n",
    "    if df.shape[0] == 2:\n",
    "        if df.noun_type.iloc[0] != df.noun_type.iloc[1]:\n",
    "            return [df.iloc[0].noun, df.iloc[0].verb, df.iloc[1].noun]\n",
    "    elif df.shape[0] > 2:\n",
    "        for i in range(df.shape[0] - 1):\n",
    "            if df.noun_type.iloc[i] != df.noun_type.iloc[i+1]:\n",
    "                return [df.iloc[0].noun, df.iloc[0].verb, df.iloc[1].noun]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [[sentence, get_triples(sentence, verb_dict, spec_dict, spec_code, nlp)] for sentence in read]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21395"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidats = [df for df in dfs if df[1].shape[0] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16590"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = []\n",
    "for can in candidats:\n",
    "    for idx in can[1][\"idx\"]:\n",
    "        if can[1][\"idx\"].to_list().count(idx) > 1:\n",
    "            trips.append([can[0], can[1]])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12594"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trips)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = []\n",
    "for idx, df in enumerate(trips):\n",
    "    #create groups that resolve around same word\n",
    "    gb = df[1].groupby('idx')    \n",
    "    #only keep groups if verb idx was identified as potential triplet before, sort by priority for structure\n",
    "    for x in gb.groups:\n",
    "        if gb.get_group(x).shape[0] == 2:\n",
    "            if gb.get_group(x).noun_type.iloc[0] != gb.get_group(x).noun_type.iloc[1]:\n",
    "                matches.append([df[0], gb.get_group(x).iloc[0].noun, gb.get_group(x).iloc[0].verb, gb.get_group(x).iloc[1].noun])\n",
    "        elif gb.get_group(x).shape[0] > 2:\n",
    "            for i in range(gb.get_group(x).shape[0] - 1):\n",
    "                if gb.get_group(x).noun_type.iloc[i] != gb.get_group(x).noun_type.iloc[i+1]:\n",
    "                    matches.append([df[0], gb.get_group(x).iloc[0].noun, gb.get_group(x).iloc[0].verb, gb.get_group(x).iloc[1].noun])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20489"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triples = []\n",
    "# ma_df = pd.DataFrame(matches, columns = [\"text\",\"subj\", \"verb\",\"obj\"])\n",
    "# for row in ma_df.iterrows():\n",
    "#     if row[1][\"verb\"] in spec_dict:\n",
    "#         for poss_pattern in spec_dict[row[1][\"verb\"]]:\n",
    "#             if set(poss_pattern.split()).intersection(row[1][\"text\"].split()) == set(poss_pattern.split()):\n",
    "#                 triples.append([row[1][\"text\"], f\"<triplet> {row[1]['subj']} <subj> {row[1]['obj']} <obj> {spec_code[poss_pattern]}\"])\n",
    "#     elif row[1][\"verb\"] in verb_dict:\n",
    "#             triples.append([row[1][\"text\"], f\"<triplet> {row[1]['subj']} <subj> {row[1]['obj']} <obj> {verb_dict[row[1]['verb']]}\"])\n",
    "\n",
    "triples = []\n",
    "ma_df = pd.DataFrame(matches, columns = [\"text\",\"subj\", \"verb\",\"obj\"])\n",
    "for row in ma_df.iterrows():\n",
    "    if row[1][\"verb\"] in spec_dict:\n",
    "        for poss_pattern in spec_dict[row[1][\"verb\"]]:\n",
    "            if set(poss_pattern.split()).intersection(row[1][\"text\"].split()) == set(poss_pattern.split()):\n",
    "                triples.append([row[1][\"text\"], row[1]['subj'], row[1]['obj'] , spec_code[poss_pattern]])\n",
    "    elif row[1][\"verb\"] in verb_dict:\n",
    "            triples.append([row[1][\"text\"], row[1]['subj'], row[1]['obj'], verb_dict[row[1]['verb']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_l = [key for key in spec_code.keys() if \"deport\" in key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['decide deport',\n",
       " 'stop deport',\n",
       " 'deport terrorist to',\n",
       " 'living deport',\n",
       " 'deport militant to',\n",
       " 'rounded up and deport',\n",
       " 'deport &preposit1',\n",
       " 'deport linked',\n",
       " 'deport alert',\n",
       " 'deport despite',\n",
       " 'deport from',\n",
       " 'deport by',\n",
       " 'deport to',\n",
       " 'plan to deport',\n",
       " 'arrive home after deport',\n",
       " 'resume deportation']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coerce\n",
      "Yield\n",
      "Coerce\n",
      "Coerce\n",
      "Coerce\n",
      "Coerce\n",
      "Coerce\n",
      "Coerce\n",
      "Coerce\n",
      "Coerce\n",
      "Coerce\n",
      "Coerce\n",
      "Engage In Material Cooperation\n",
      "Threaten\n",
      "Coerce\n",
      "Coerce\n"
     ]
    }
   ],
   "source": [
    "for key in key_l:\n",
    "    print(spec_code[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Coerce'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_code['arrive home after deport']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "huh = pd.DataFrame(triples, columns = [\"text\", \"subj\", \"obj\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1249"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huh.text.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subj</th>\n",
       "      <th>obj</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For more on our coverage of the war in Ukraine...</td>\n",
       "      <td>loaded ships</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>Consult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For more on our coverage of the war in Ukraine...</td>\n",
       "      <td>loaded ships</td>\n",
       "      <td>Ukrainian ports</td>\n",
       "      <td>Consult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Russia must clearly understand that Russia wi...</td>\n",
       "      <td>Russia</td>\n",
       "      <td>a harsh global response</td>\n",
       "      <td>Consult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Russia must clearly understand that Russia wi...</td>\n",
       "      <td>70,000 people</td>\n",
       "      <td>the Kherson region</td>\n",
       "      <td>Yield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Russian President Vladimir Putin told Erdogan ...</td>\n",
       "      <td>President Emmanuel Macron</td>\n",
       "      <td>Ukraine's defence needs</td>\n",
       "      <td>Reject</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  For more on our coverage of the war in Ukraine...   \n",
       "1  For more on our coverage of the war in Ukraine...   \n",
       "2  \"Russia must clearly understand that Russia wi...   \n",
       "3  \"Russia must clearly understand that Russia wi...   \n",
       "4  Russian President Vladimir Putin told Erdogan ...   \n",
       "\n",
       "                        subj                      obj    label  \n",
       "0               loaded ships                  Ukraine  Consult  \n",
       "1               loaded ships          Ukrainian ports  Consult  \n",
       "2                     Russia  a harsh global response  Consult  \n",
       "3              70,000 people       the Kherson region    Yield  \n",
       "4  President Emmanuel Macron  Ukraine's defence needs   Reject  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\svawe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:2339: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n"
     ]
    }
   ],
   "source": [
    "probs_l = []\n",
    "for row in huh.iterrows():\n",
    "    premise = row[1][\"text\"]\n",
    "    subj = row[1][\"subj\"]\n",
    "    rel = row[1][\"label\"]\n",
    "    obj =  row[1][\"obj\"]\n",
    "\n",
    "    hypothesis = f'{subj} does {rel} towards {obj}.'\n",
    "\n",
    "    # run through model pre-trained on MNLI\n",
    "    x = tokenizer.encode(premise, hypothesis, return_tensors='pt',\n",
    "                        truncation_strategy='only_first')\n",
    "    logits = nli_model(x)[0]\n",
    "\n",
    "    # we throw away \"neutral\" (dim 1) and take the probability of\n",
    "    # \"entailment\" (2) as the probability of the label being true \n",
    "    entail_contradiction_logits = logits[:,[0,2]]\n",
    "    probs = entail_contradiction_logits.softmax(dim=1)\n",
    "    prob_label_is_true = probs[:,1]\n",
    "\n",
    "    probs_l.append([row[0], prob_label_is_true.item()])\n",
    "    if row[0] % 100 == 0: print(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "huh = pd.merge(huh.reset_index(), pd.DataFrame(probs_l, columns = [\"index\", \"prob\"]), on = \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = huh[huh.prob < 0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = huh[huh.prob > 0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Appeal                               31\n",
       "Assault                              10\n",
       "Coerce                               36\n",
       "Consult                             128\n",
       "Disapprove                           28\n",
       "Engage In Diplomatic Cooperation     29\n",
       "Engage In Material Cooperation       59\n",
       "Fight                                 6\n",
       "Intend                               25\n",
       "Investigate                          20\n",
       "Make Public Statement                98\n",
       "Protest                               2\n",
       "Provide Aid                          24\n",
       "Reduce Relations                     10\n",
       "Reject                                2\n",
       "Threaten                             13\n",
       "Yield                                73\n",
       "Name: index, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep.groupby(\"label\").index.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Appeal                                     20\n",
       "Assault                                    62\n",
       "Coerce                                     66\n",
       "Consult                                    92\n",
       "Demand                                      1\n",
       "Disapprove                                 46\n",
       "Engage In Diplomatic Cooperation          118\n",
       "Engage In Material Cooperation            113\n",
       "Engage In Unconventional Mass Violence      1\n",
       "Exhibit Military Posture                    6\n",
       "Fight                                      38\n",
       "Intend                                     46\n",
       "Investigate                                 4\n",
       "Make Public Statement                     118\n",
       "Protest                                     9\n",
       "Provide Aid                                65\n",
       "Reduce Relations                           53\n",
       "Reject                                     47\n",
       "Threaten                                    6\n",
       "Yield                                     128\n",
       "Name: index, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec.groupby(\"label\").index.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>subj</th>\n",
       "      <th>obj</th>\n",
       "      <th>label</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>For more on our coverage of the war in Ukraine...</td>\n",
       "      <td>loaded ships</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.937298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"Russia must clearly understand that Russia wi...</td>\n",
       "      <td>70,000 people</td>\n",
       "      <td>the Kherson region</td>\n",
       "      <td>Yield</td>\n",
       "      <td>0.810604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>12:04pm Kremlin accuses UK of ‘directing and c...</td>\n",
       "      <td>coordinating</td>\n",
       "      <td>Nord Stream blastsThe Kremlin</td>\n",
       "      <td>Engage In Material Cooperation</td>\n",
       "      <td>0.898151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Swedish, Finnish NATO bidsFinland's Prime Mini...</td>\n",
       "      <td>Finland</td>\n",
       "      <td>the NATO defence alliance</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.716120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Swedish, Finnish NATO bidsFinland's Prime Mini...</td>\n",
       "      <td>Finland</td>\n",
       "      <td>the NATO defence alliance</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.716120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>1621</td>\n",
       "      <td>As a result, Lesotho's has been run by coaliti...</td>\n",
       "      <td>no prime minister</td>\n",
       "      <td>a full five-year term</td>\n",
       "      <td>Engage In Material Cooperation</td>\n",
       "      <td>0.912791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623</th>\n",
       "      <td>1623</td>\n",
       "      <td>The UK delegation will also include Foreign Se...</td>\n",
       "      <td>The UK delegation</td>\n",
       "      <td>Foreign Secretary James Cleverly</td>\n",
       "      <td>Yield</td>\n",
       "      <td>0.746475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>1626</td>\n",
       "      <td>\" a spokesperson for the State Department also...</td>\n",
       "      <td>a spokesperson</td>\n",
       "      <td>remarks</td>\n",
       "      <td>Make Public Statement</td>\n",
       "      <td>0.964414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1628</th>\n",
       "      <td>1628</td>\n",
       "      <td>MUMBAI: Mass layoffs at Twitter and also other...</td>\n",
       "      <td>Mass layoffs</td>\n",
       "      <td>also other companies</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.930462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1632</th>\n",
       "      <td>1632</td>\n",
       "      <td>“It seems Joe Biden's positions change dependi...</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>a lesson</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.903089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>594 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                               text  \\\n",
       "0         0  For more on our coverage of the war in Ukraine...   \n",
       "3         3  \"Russia must clearly understand that Russia wi...   \n",
       "5         5  12:04pm Kremlin accuses UK of ‘directing and c...   \n",
       "7         7  Swedish, Finnish NATO bidsFinland's Prime Mini...   \n",
       "8         8  Swedish, Finnish NATO bidsFinland's Prime Mini...   \n",
       "...     ...                                                ...   \n",
       "1621   1621  As a result, Lesotho's has been run by coaliti...   \n",
       "1623   1623  The UK delegation will also include Foreign Se...   \n",
       "1626   1626  \" a spokesperson for the State Department also...   \n",
       "1628   1628  MUMBAI: Mass layoffs at Twitter and also other...   \n",
       "1632   1632  “It seems Joe Biden's positions change dependi...   \n",
       "\n",
       "                   subj                               obj  \\\n",
       "0          loaded ships                           Ukraine   \n",
       "3         70,000 people                the Kherson region   \n",
       "5          coordinating     Nord Stream blastsThe Kremlin   \n",
       "7               Finland         the NATO defence alliance   \n",
       "8               Finland         the NATO defence alliance   \n",
       "...                 ...                               ...   \n",
       "1621  no prime minister             a full five-year term   \n",
       "1623  The UK delegation  Foreign Secretary James Cleverly   \n",
       "1626     a spokesperson                           remarks   \n",
       "1628       Mass layoffs              also other companies   \n",
       "1632          Joe Biden                          a lesson   \n",
       "\n",
       "                               label      prob  \n",
       "0                            Consult  0.937298  \n",
       "3                              Yield  0.810604  \n",
       "5     Engage In Material Cooperation  0.898151  \n",
       "7                            Consult  0.716120  \n",
       "8                            Consult  0.716120  \n",
       "...                              ...       ...  \n",
       "1621  Engage In Material Cooperation  0.912791  \n",
       "1623                           Yield  0.746475  \n",
       "1626           Make Public Statement  0.964414  \n",
       "1628                         Consult  0.930462  \n",
       "1632                         Consult  0.903089  \n",
       "\n",
       "[594 rows x 6 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the nine people arrested include two managers, two ticket clerks, two contractors and three security guards.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec.iloc[10].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                                   16\n",
       "text     the nine people arrested include two managers,...\n",
       "subj                                       the nine people\n",
       "obj                                           two managers\n",
       "label                                                Yield\n",
       "prob                                              0.064997\n",
       "Name: 16, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec.iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8217246532440186"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premise = dec.iloc[6][\"text\"]\n",
    "subj = dec.iloc[6][\"subj\"]\n",
    "rel = \"Coerce\"\n",
    "obj =  dec.iloc[6][\"obj\"]\n",
    "\n",
    "hypothesis = f'{subj} does {rel} towards {obj} .'\n",
    "\n",
    "# run through model pre-trained on MNLI\n",
    "x = tokenizer.encode(premise, hypothesis, return_tensors='pt',\n",
    "                    truncation_strategy='only_first')\n",
    "logits = nli_model(x)[0]\n",
    "\n",
    "entail_contradiction_logits = logits[:,[0,2]]\n",
    "probs = entail_contradiction_logits.softmax(dim=1)\n",
    "prob_label_is_true = probs[:,1]\n",
    "\n",
    "prob_label_is_true.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb6e93dfa86b7f97c60c39b0c0c403458d25a4deb04fc60caf4cf2dd07f32ebe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
