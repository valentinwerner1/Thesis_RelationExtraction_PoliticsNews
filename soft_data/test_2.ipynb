{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\svawe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import xml.etree.ElementTree as ET\n",
    "from spacy.symbols import nsubj, dobj, pobj, iobj, neg, xcomp, VERB\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def read_lines(inputparsed):    \n",
    "    \"\"\"takes input from CoreNLP sentence parsed file and returns sentences\"\"\"\n",
    "    #parse all lines from CoreNLP sentence split\n",
    "    parsed = open(inputparsed, encoding = \"utf-8\")\n",
    "    parsedfile = parsed.readlines()\n",
    "    parsedlines = []\n",
    "\n",
    "    #Only keep those lines which have Sentence #n in the line before\n",
    "    for idx, text in enumerate(parsedfile):\n",
    "        if text.startswith(\"Sentence #\"):\n",
    "            parsedlines.append(parsedfile[idx+1].replace('\\n','').strip())\n",
    "    \n",
    "    return parsedlines\n",
    "\n",
    "def gen_poss(line, verb_match, pre_dict):\n",
    "    \"\"\"generates all possibilities of patterns that a multi word line implies,\n",
    "    by extracting partial patterns and resolving placeholder words\"\"\"\n",
    "    poss = []\n",
    "\n",
    "    #replace special tokens in text that are clear at this point\n",
    "    line = line.replace(\"*\", verb_match)\n",
    "    line = line.replace(\"- \", \"\")\n",
    "    line = line.replace(\"+\",\"\")\n",
    "    line = line.replace(\"%\",\"\")\n",
    "    line = line.replace(\"^\",\"\")\n",
    "    line = line.replace(\"$\",\"\")\n",
    "\n",
    "    #split line by possibility indicators and code (always ends possibility)\n",
    "    #example.: \"- $ * (P ON KILLING (P OF + [010] #  COMMENT <ELH 07 May 2008>\"\n",
    "    poss_split = re.split(\"\\(P |\\[.*]\",line) \n",
    "\n",
    "    if len(poss_split) > 2: #2 is if no (P in the line\n",
    "        #only combining the first (P, as they share the same code \n",
    "        #and the longer version will never be contained in a text if the shorter isnt\n",
    "        poss.append(strip_multiple_whitespaces(\" \".join(poss_split[:2])).lower().rstrip().lstrip())\n",
    "    else: \n",
    "        poss.append(strip_multiple_whitespaces(poss_split[0].lower().rstrip().lstrip()))\n",
    "\n",
    "    cleaned = []\n",
    "    for text in list(set(poss)):\n",
    "        c = 0\n",
    "        for tag in list(pre_dict.keys()):\n",
    "            if tag in text:\n",
    "                for replacement in pre_dict[tag]:\n",
    "                    cleaned.append(text.replace(tag, replacement))\n",
    "                    c += 1\n",
    "        if c == 0:\n",
    "            cleaned.append(text)\n",
    "\n",
    "    return cleaned\n",
    "    \n",
    "\n",
    "def verb_code_dict(pico_path, verb_path):\n",
    "    \"\"\"reads coding ontology and verb lists, \n",
    "    directly matches verbs to their CAMEO codes and returns this verbs:codes dictionairy.\n",
    "    verb with codes that cannot be read are printed out as full line of the file\"\"\"\n",
    "    #read PETRARCH Internal Coding Ontology (= pico)\n",
    "    pico_path = os.path.join(os.getcwd(), pico_path)\n",
    "    pico_file = open(pico_path, 'r')\n",
    "    pico_lines = pico_file.readlines()\n",
    "\n",
    "    #get all 20 codes with their respective code\n",
    "    main_codes = {}                             #we run one iteration for all the main codes, only main codes contain relation name\n",
    "    for line in pico_lines:\n",
    "        line = line.split('#')\n",
    "        if line[0] == \"\" or line[0] == \"\\n\":    #only intro comments and empty lines\n",
    "            continue\n",
    "        else: \n",
    "            code_split = line[0].split(\":\")     #splits into CAMEO code and related hex\n",
    "            if len(line) > 1 and code_split[0][2] == \"0\":      #only main categories have 0 in 3rd idx, [cat_num 0] -> [010]\n",
    "                main_codes[code_split[0][:2]] = line[-1].replace(\"\\n\",\"\")\n",
    "    \n",
    "    #map code to code we want to use in the training\n",
    "    map_codes = {\"DiplomaticCoop\" : \"Engage In Diplomatic Cooperation\", \n",
    "                \"MaterialCoop\" : \"Engage In Material Cooperation\",\n",
    "                \"ProvideAid\" : \"Provide Aid\",\n",
    "                \"Exhibit Force Posture\": \"Exhibit Military Posture\",\n",
    "                \"Use Unconventional Mass Violence\" : \"Engage In Unconventional Mass Violence\"}\n",
    "    main_codes = {k: (map_codes[v] if v in map_codes else v) for k, v in main_codes.items()}\n",
    "    \n",
    "    #read single word patterns and match their code to the relation extracted in main_codes\n",
    "    verb_path = os.path.join(os.getcwd(), verb_path)\n",
    "    verb_file = open(verb_path, 'r')\n",
    "    verb_lines = verb_file.readlines()\n",
    "    \n",
    "    verb_dict = {}\n",
    "    for line in verb_lines:\n",
    "        if line[0] == \"#\":\n",
    "            continue\n",
    "        elif line.startswith(\"---\"):    #main verbs have a lead code, which is applied to all very in the section\n",
    "                                        #unless a separate code is specified for a specific verb in section\n",
    "            try: cur_main_code = re.split(\"\\[|\\]|---\", line)[2].replace(\":\",\"\")[:2]  #we only need main codes which are first two numbers\n",
    "                                                                                #sometimes code starts with \":\", e.g.: ---  OFFEND   [:110]  ---\n",
    "                                                                                #we just remove those to get the main code\n",
    "            except:                     #depending on chosen verb dictionairy, there may be main verbs without lead codes\n",
    "                print(\"couldn't finde code in: \", line.replace(\"\\n\",\"\")) \n",
    "                cur_main_code == \"--\"\n",
    "            if cur_main_code == \"\": cur_main_code = \"--\"\n",
    "        elif line == \"\\n\":              #skip empty lines\n",
    "            continue\n",
    "        elif line[0] == \"-\" or line[0] == \"~\" or line[0] == \"+\" or line[0] == \"&\": #removes all special structures we cannot use\n",
    "            continue\n",
    "        else:\n",
    "            if len(re.split(\"\\[|\\]\", line)) > 1:    #verbs with their own code, e.g.: AFFIRM [051] \n",
    "                code = re.split(\"\\[|\\]\", line)[1].replace(\":\",\"\")[:2]\n",
    "                if code != \"--\":\n",
    "                    if \"{\" in line:         #conjugated verbs, e.g. \"APPLY {APPLYING APPLIED APPLIES } [020]\"\n",
    "                        line_s = re.split(\"\\{|\\}\", line)    #split at { and }\n",
    "                        verb_dict[line_s[0].lower()] = main_codes[code] \n",
    "                        for word in line_s[1].split():\n",
    "                            verb_dict[word.lower()] = main_codes[code]\n",
    "                    else:\n",
    "                        word = re.split(\"\\[|\\]\", line)[0]\n",
    "                        verb_dict[word.lower()] = main_codes[code]\n",
    "            else:\n",
    "                if cur_main_code != \"--\":\n",
    "                    if \"{\" in line:         #e.g. \"HURRY {HURRIES HURRYING HURRIED }\" \n",
    "                        line_s = re.split(\"\\{|\\}\", line)    #split at { and }\n",
    "                        verb_dict[line_s[0].lower()] = main_codes[cur_main_code]\n",
    "                        for word in line_s[1].split():\n",
    "                            verb_dict[word.lower()] = main_codes[cur_main_code]\n",
    "                    else:                   #only single words with sometimes comments, e.g.: CENSURE  # JON 5/17/95\n",
    "                        word = line.split(\"#\")[0].rstrip()    #gets part before \"#\", removes all whitespaces to the right\n",
    "                        verb_dict[word.lower()] = main_codes[cur_main_code]\n",
    "\n",
    "    #read multi word patterns and create a dictionary for their code\n",
    "\n",
    "    #get filler words that occur in multi word patterns\n",
    "    verb_file = open(verb_path, 'r')\n",
    "    verb_lines = verb_file.readlines()\n",
    "\n",
    "    pre_dict = {}\n",
    "    filter_list = []\n",
    "    for line in verb_lines:\n",
    "        if line.startswith(\"&\"):\n",
    "            cur_filter = line.rstrip()\n",
    "        elif line.startswith(\"\\n\") and \"cur_filter\" in locals():\n",
    "            pre_dict[cur_filter.lower()] = filter_list\n",
    "            cur_filter = \"\"\n",
    "            filter_list = []\n",
    "        elif line.startswith(\"+\") and cur_filter != \"\":\n",
    "            filter_list.append(line.rstrip()[1:].replace(\"_\", \"\").lower())\n",
    "    del pre_dict[\"\"]\n",
    "\n",
    "    #generate dictionaries for multi word patterns\n",
    "    verb_file = open(verb_path, 'r')\n",
    "    verb_lines = verb_file.readlines()\n",
    "\n",
    "    spec_dict = {}\n",
    "    spec_code = {}\n",
    "\n",
    "    count = 0\n",
    "    for line in verb_lines:\n",
    "        if line.startswith(\"- \"):\n",
    "            #get main verb as dict key\n",
    "            try: \n",
    "                verb_match = re.search(\"# *\\w+\", line).group()\n",
    "                verb_match = re.search(\"\\w+\", verb_match).group()\n",
    "                verb_match = verb_match.replace(\"_\", \" \").lower()\n",
    "            except: \n",
    "                count += 1\n",
    "\n",
    "            #get code for line\n",
    "            try:\n",
    "                code = re.search(\"\\[.*]\", line).group()[1:3]\n",
    "                if code != \"--\":\n",
    "                    #get all possibility that the line indicates\n",
    "                    poss = gen_poss(line, verb_match, pre_dict)\n",
    "                    for pattern in poss:\n",
    "                        spec_code[pattern] = main_codes[code]\n",
    "                    spec_dict[verb_match] = poss\n",
    "            except:\n",
    "                count += 1\n",
    "\n",
    "    print(f\"{count} patterns could not be loaded\")        \n",
    "\n",
    "    return verb_dict, spec_dict, spec_code\n",
    "\n",
    "\n",
    "def get_triples(sentence, verb_dict, spec_dict, spec_code, nlp):\n",
    "    \"\"\"create triplet structure for training from text input, \n",
    "    verb_dict needs to be loaded before,\n",
    "    spacy model needs to be initialized before \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    verbs = []\n",
    "    dict = {}\n",
    "\n",
    "\n",
    "    for possible_verb in doc:\n",
    "        if possible_verb.pos == VERB:\n",
    "            if neg in [child.dep for child in possible_verb.children]: continue\n",
    "            else: \n",
    "                for possible_subject in possible_verb.children: \n",
    "                    if possible_subject.dep == xcomp:   #subj / obj of composed verb should also be subj / obj of main verb\n",
    "                        main_verb = possible_subject\n",
    "                        main_idx = possible_subject.idx\n",
    "                        for token in doc.ents:\n",
    "                            if token.label_ in [\"GPE\", \"NORP\", \"EVENTS\", \"FAC\", \"LAW\", \"ORG\", \"PERSON\"]:\n",
    "                                if token.root.dep_ == \"poss\":\n",
    "                                    if token.root.head.head.idx == possible_verb.idx:\n",
    "                                        verbs.append([main_idx, main_verb.lemma_, token.text, token.root.head.dep_])\n",
    "                                        if main_idx in dict.keys(): dict[main_idx] += 1\n",
    "                                        else: dict[main_idx] = 1\n",
    "                                else:\n",
    "                                    if token.root.head.idx == possible_verb.idx:\n",
    "                                        verbs.append([main_idx, main_verb.lemma_, token.text, token.root.dep_])\n",
    "                                        if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "                                        else: dict[possible_verb.idx] = 1\n",
    "\n",
    "                for token in doc.ents:\n",
    "                    if token.label_ in [\"GPE\", \"NORP\", \"EVENTS\", \"FAC\", \"LAW\", \"ORG\", \"PERSON\"]:\n",
    "                        if token.root.dep_ == \"poss\":\n",
    "                            if token.root.head.head.idx == possible_verb.idx:\n",
    "                                verbs.append([possible_verb.idx, possible_verb.lemma_, token.text, token.root.head.dep_])\n",
    "                                if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "                                else: dict[possible_verb.idx] = 1\n",
    "                        else:\n",
    "                            if token.root.head.idx == possible_verb.idx:\n",
    "                                verbs.append([possible_verb.idx, possible_verb.lemma_, token.text, token.root.dep_])\n",
    "                                if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "                                else: dict[possible_verb.idx] = 1\n",
    "\n",
    "    trip_idx = [key for key in dict if dict[key] > 1]\n",
    "\n",
    "    # doc = nlp(sentence)\n",
    "    # verbs = []\n",
    "    # dict = {}\n",
    "\n",
    "    # for possible_verb in doc:           #parses through all words in sentence\n",
    "    #     if possible_verb.pos == VERB:   #we only care about verbs\n",
    "    #         if neg in [child.dep for child in possible_verb.children]: continue #we exclude all negated verbs\n",
    "    #         else: \n",
    "    #             for candidate in possible_verb.children: #for composed verbs of verb (e.g. \"want to join\" -> \"want join\")\n",
    "    #                 if candidate.dep == xcomp:   #subj / obj of composed verb should also be subj / obj of main verb\n",
    "    #                     main_verb = candidate    \n",
    "    #                     main_idx = candidate.idx\n",
    "    #                     for chunk in doc.noun_chunks:   #chunks are noun-groups (e.g.: \"78 out of 100 people\" instead of \"people\")\n",
    "    #                         if chunk.root.head.idx == possible_verb.idx:    #if chunk applies to xcomp (want),\n",
    "    #                                                                         #treat it like it aplles to main verb (\"join\")\n",
    "    #                             verbs.append([main_idx, main_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "    #                             if main_idx in dict.keys(): dict[main_idx] += 1 #count how often verb is used\n",
    "    #                             else: dict[main_idx] = 1\n",
    "\n",
    "    #             for chunk in doc.noun_chunks:       #for normal verbs, check chunks directly\n",
    "    #                 if chunk.root.head.idx == possible_verb.idx:\n",
    "    #                     verbs.append([possible_verb.idx, possible_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "    #                     if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "    #                     else: dict[possible_verb.idx] = 1\n",
    "    \n",
    "    # trip_idx = [key for key in dict if dict[key] > 1]   #if verbs used more than once, its candidate for triplet\n",
    "\n",
    "    #priority for subj-relation-obj triplets\n",
    "    mapper = {\"nsubj\":1,\"dobj\":2, \"pobj\":2, \"iobj\":2}\n",
    "\n",
    "    #create df from verbs extracted \n",
    "    df = pd.DataFrame(verbs, columns = [\"idx\", \"verb\", \"noun\", \"noun_type\"])\n",
    "    df[\"noun_map\"] = df.noun_type.map(mapper)  #turn noun_types into priority \n",
    "    return df\n",
    "\n",
    "    # #create groups that resolve around same word\n",
    "    # gb = df.groupby('idx')    \n",
    "    # #only keep groups if verb idx was identified as potential triplet before, sort by priority for structure\n",
    "    # df_l = [gb.get_group(x).sort_values(\"noun_map\") for x in gb.groups if gb.get_group(x).idx.iloc[0] in dict]\n",
    "    # matches = [merge_trip(group) for group in df_l if not merge_trip(group) == None] #get groups into triplet structure\n",
    "    \n",
    "    # #turn matches into triples by only keeping those with coded verbs, return code instead of verb\n",
    "    # triples = []\n",
    "    # for match in matches:\n",
    "    #     if match[1].lower() in spec_dict:\n",
    "    #         for poss_pattern in spec_dict[match[1].lower()]:\n",
    "    #             if set(poss_pattern.split()).intersection(sentence.split()) == set(poss_pattern.split()):\n",
    "    #                 triples.append(f\"<triplet> {match[0]} <subj> {match[2]} <obj> {spec_code[poss_pattern]}\")\n",
    "                    \n",
    "    #     elif match[1].lower() in verb_dict:\n",
    "    #         triples.append(f\"<triplet> {match[0]} <subj> {match[2]} <obj> {verb_dict[match[1].lower()]}\")\n",
    "    #     else: print(f\"couldn't match {match[1].lower()}\")\n",
    "\n",
    "    # #triples = [f\"<triplet> {match[0]} <subj> {match[2]} <obj> {verb_dict[match[1].lower()]}\" for match in matches if match[1].lower() in verb_dict]\n",
    "\n",
    "    # return triples\n",
    "\n",
    "def merge_trip(df):\n",
    "    \"\"\"helper function to turn two rows of a pandas groupby into subj, verb, obj\"\"\"\n",
    "    if df.shape[0] == 2:\n",
    "        if df.noun_type.iloc[0] != df.noun_type.iloc[1]:\n",
    "            return [df.iloc[0].noun, df.iloc[0].verb, df.iloc[1].noun]\n",
    "    elif df.shape[0] > 2:\n",
    "        for i in range(df.shape[0] - 1):\n",
    "            if df.noun_type.iloc[i] != df.noun_type.iloc[i+1]:\n",
    "                return [df.iloc[0].noun, df.iloc[0].verb, df.iloc[1].noun]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pico_path = r\"C:/Users/svawe/Thesis_RelationExtraction_PoliticsNews/soft_data/src/add_labels/dictionaries/PETR.Internal.Coding.Ontology.txt\"\n",
    "verb_path = r\"C:/Users/svawe/Thesis_RelationExtraction_PoliticsNews/soft_data/src/add_labels/dictionaries/newdict.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "read = read_lines(\"data/out_data/articles_url_coref2.csv.xml.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couldn't finde code in:  --- DEFEND  ###\n",
      "couldn't finde code in:  --- REVOKE_   ###\n",
      "couldn't finde code in:  --- SEND   ###\n",
      "couldn't finde code in:  --- COLLAPSE  ###\n",
      "22 patterns could not be loaded\n"
     ]
    }
   ],
   "source": [
    "verb_dict, spec_dict, spec_code = verb_code_dict(pico_path, verb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_2 = [[sentence, get_triples(sentence, verb_dict, spec_dict, spec_code, nlp)] for sentence in read]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21395"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfs_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['AdvertisingRead moreThis live page is no longer being updated.',\n",
       "  Empty DataFrame\n",
       "  Columns: [idx, verb, noun, noun_type, noun_map]\n",
       "  Index: []],\n",
       " ['For more on\\xa0our coverage of the war in Ukraine, click here.11:44pm:\\xa0UN grain coordinator\\xa0expects loaded ships to depart Ukraine on ThursdayThe UN coordinator for the Ukraine Black Sea grain deal said UN grain coordinator expects loaded ships to depart Ukrainian ports on ThursdayThe.',\n",
       "     idx    verb     noun noun_type  noun_map\n",
       "  0  113  depart  Ukraine      dobj         2],\n",
       " ['“Exports of grain and foodstuffs from Ukraine need to continue.',\n",
       "  Empty DataFrame\n",
       "  Columns: [idx, verb, noun, noun_type, noun_map]\n",
       "  Index: []],\n",
       " ['Although no movements of vessels are planned for 2 November under the #BlackSeaGrainInitiative, we expect loaded ships to sail on ThursdayThe,” UN coordinator Amir Abdulla posted on Twitter.',\n",
       "     idx  verb          noun noun_type  noun_map\n",
       "  0  172  post  Amir Abdulla     nsubj         1],\n",
       " ['Exports of grain and foodstuffs from Ukraine need to continue.',\n",
       "  Empty DataFrame\n",
       "  Columns: [idx, verb, noun, noun_type, noun_map]\n",
       "  Index: []]]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The UN Secretariat at coordination centreThere reports that the Ukrainian, Turkish and UN delegations agreed not to plan any movement of vessels in the Black Sea Grain Initiative for 2 November,\" \"The UN Secretariat at the Joint Coordination Centre said Tuesday, referring to the July deal brokered by Turkey and the UN.6:15pm:\\xa0Russian President Vladimir Putin tells Erdogan Russian President Vladimir Putin wants \\'real guarantees\\' from Kyiv on grain deal, says Russian President Vladimir Putin told Erdogan Tuesday that Russian President Vladimir Putin wanted \"real guarantees\" from Kyiv before Kyiv potentially rejoined grain deal.',\n",
       "    idx    verb                noun noun_type  noun_map\n",
       " 0   47  report  The UN Secretariat     nsubj         1\n",
       " 1  361    tell      Vladimir Putin     nsubj         1\n",
       " 2  408    want      Vladimir Putin     nsubj         1\n",
       " 3  495    tell      Vladimir Putin     nsubj         1\n",
       " 4  495    tell             Erdogan      dobj         2\n",
       " 5  554    want      Vladimir Putin     nsubj         1\n",
       " 6  613  rejoin                Kyiv     nsubj         1]"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_2[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidats_2 = [df for df in dfs_2 if df[1].shape[0] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6454"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidats_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     47\n",
       "1    361\n",
       "2    408\n",
       "3    495\n",
       "4    495\n",
       "5    554\n",
       "6    613\n",
       "Name: idx, dtype: int64"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_2[13][1][\"idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_2 = []\n",
    "for can in candidats_2:\n",
    "    for idx in can[1][\"idx\"]:\n",
    "        if can[1][\"idx\"].to_list().count(idx) > 1:\n",
    "            trips_2.append([can[0], can[1]])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2395"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trips_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_2 = []\n",
    "for idx, df in enumerate(trips_2):\n",
    "    #create groups that resolve around same word\n",
    "    gb = df[1].groupby('idx')  \n",
    "    #only keep groups if verb idx was identified as potential triplet before, sort by priority for structure\n",
    "    for x in gb.groups:\n",
    "        group = gb.get_group(x).sort_values(\"noun_map\")\n",
    "        if group.shape[0] == 2:\n",
    "            if group.noun_type.iloc[0] != group.noun_type.iloc[1]:\n",
    "                matches_2.append([df[0], group.iloc[0].noun, group.iloc[0].verb, group.iloc[1].noun])\n",
    "        elif group.shape[0] > 2:\n",
    "            for i in range(group.shape[0] - 1):\n",
    "                if group.noun_type.iloc[i] != group.noun_type.iloc[i+1]:\n",
    "                    matches_2.append([df[0], group.iloc[0].noun, group.iloc[0].verb, group.iloc[1].noun])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couldn't match evaluate\n",
      "couldn't match co\n",
      "couldn't match roll\n",
      "couldn't match love\n",
      "couldn't match feel\n",
      "couldn't match have\n",
      "couldn't match convince\n",
      "couldn't match teach\n",
      "couldn't match soak\n",
      "couldn't match think\n",
      "couldn't match feel\n",
      "couldn't match bind\n",
      "couldn't match recruit\n",
      "couldn't match classify\n",
      "couldn't match leak\n",
      "couldn't match begrudge\n",
      "couldn't match lose\n",
      "couldn't match become\n",
      "couldn't match have\n",
      "couldn't match have\n",
      "couldn't match renew\n",
      "couldn't match shift\n",
      "couldn't match stump\n",
      "couldn't match modify\n",
      "couldn't match exert\n",
      "couldn't match position\n",
      "couldn't match know\n",
      "couldn't match suspend\n",
      "couldn't match pack\n",
      "couldn't match know\n",
      "couldn't match have\n",
      "couldn't match exhaust\n",
      "couldn't match turn\n",
      "couldn't match have\n",
      "couldn't match type\n",
      "couldn't match post\n",
      "couldn't match upload\n",
      "couldn't match pack\n",
      "couldn't match turn\n",
      "couldn't match spare\n",
      "couldn't match to\n",
      "couldn't match position\n",
      "couldn't match do\n",
      "couldn't match evaluate\n",
      "couldn't match download\n",
      "couldn't match repopulate\n",
      "couldn't match discredit\n",
      "couldn't match suspend\n",
      "couldn't match suspend\n",
      "couldn't match intensify\n",
      "couldn't match mishandle\n",
      "couldn't match nominate\n",
      "couldn't match do\n",
      "couldn't match overtake\n",
      "couldn't match tap\n",
      "couldn't match be\n",
      "couldn't match nudge\n",
      "couldn't match post\n",
      "couldn't match update\n",
      "couldn't match shake\n",
      "couldn't match know\n",
      "couldn't match lower\n",
      "couldn't match stoke\n",
      "couldn't match delegitimise\n",
      "couldn't match tweet\n",
      "couldn't match have\n",
      "couldn't match isolate\n",
      "couldn't match love\n",
      "couldn't match feel\n",
      "couldn't match remember\n",
      "couldn't match have\n",
      "couldn't match remind\n",
      "couldn't match gloss\n",
      "couldn't match reflect\n",
      "couldn't match have\n",
      "couldn't match earn\n",
      "couldn't match swear\n",
      "couldn't match convince\n",
      "couldn't match near\n",
      "couldn't match macribookpara\n",
      "couldn't match be\n",
      "couldn't match love\n",
      "couldn't match thrill\n",
      "couldn't match like\n",
      "couldn't match become\n",
      "couldn't match crown\n",
      "couldn't match stoke\n",
      "couldn't match owe\n",
      "couldn't match catch\n",
      "couldn't match update\n",
      "couldn't match have\n",
      "couldn't match finish\n",
      "couldn't match like\n",
      "couldn't match know\n",
      "couldn't match downplay\n",
      "couldn't match downgrade\n",
      "couldn't match sack\n",
      "couldn't match have\n",
      "couldn't match play\n",
      "couldn't match feed\n",
      "couldn't match have\n",
      "couldn't match recycle\n",
      "couldn't match paint\n",
      "couldn't match wean\n",
      "couldn't match ramp\n",
      "couldn't match intensify\n",
      "couldn't match mishandle\n",
      "couldn't match suspend\n",
      "couldn't match sideline\n",
      "couldn't match lose\n",
      "couldn't match bolster\n",
      "couldn't match slow\n",
      "couldn't match become\n",
      "couldn't match become\n",
      "couldn't match write\n",
      "couldn't match suspend\n",
      "couldn't match revert\n",
      "couldn't match suspend\n",
      "couldn't match intensify\n",
      "couldn't match hang\n",
      "couldn't match overtake\n",
      "couldn't match content\n",
      "couldn't match trail\n",
      "couldn't match have\n",
      "couldn't match evaluate\n",
      "couldn't match lose\n",
      "couldn't match have\n",
      "couldn't match alter\n",
      "couldn't match lose\n",
      "couldn't match focus\n",
      "couldn't match evaluate\n",
      "couldn't match do\n",
      "couldn't match suspend\n",
      "couldn't match suspend\n",
      "couldn't match survey\n",
      "couldn't match have\n",
      "couldn't match write\n",
      "couldn't match read\n",
      "couldn't match like\n",
      "couldn't match turn\n",
      "couldn't match listen\n",
      "couldn't match treat\n",
      "couldn't match barnstorm\n",
      "couldn't match focus\n",
      "couldn't match have\n",
      "couldn't match rethink\n",
      "couldn't match position\n",
      "couldn't match have\n",
      "couldn't match overhaul\n",
      "couldn't match overcome\n",
      "couldn't match do\n",
      "couldn't match evaluate\n",
      "couldn't match secure\n",
      "couldn't match finish\n",
      "couldn't match write\n",
      "couldn't match have\n",
      "couldn't match do\n",
      "couldn't match evaluate\n",
      "couldn't match acquire\n",
      "couldn't match diversify\n",
      "couldn't match reflect\n",
      "couldn't match copy\n",
      "couldn't match exhaust\n",
      "couldn't match turn\n",
      "couldn't match shrink\n",
      "couldn't match backtrack\n",
      "couldn't match restart\n",
      "couldn't match suspend\n",
      "couldn't match sack\n",
      "couldn't match suspend\n",
      "couldn't match suspend\n",
      "couldn't match suspend\n",
      "couldn't match eat\n",
      "couldn't match retake\n",
      "couldn't match download\n",
      "couldn't match do\n",
      "couldn't match portray\n",
      "couldn't match play\n",
      "couldn't match don\n",
      "couldn't match singe\n",
      "couldn't match suspend\n",
      "couldn't match singe\n",
      "couldn't match widen\n",
      "couldn't match finish\n",
      "couldn't match treat\n",
      "couldn't match birth\n",
      "couldn't match have\n",
      "couldn't match lose\n",
      "couldn't match suspend\n",
      "couldn't match suspend\n",
      "couldn't match suspend\n",
      "couldn't match have\n",
      "couldn't match interrupt\n",
      "couldn't match do\n",
      "couldn't match have\n",
      "couldn't match cover\n",
      "couldn't match do\n",
      "couldn't match hug\n",
      "couldn't match permit\n",
      "couldn't match reinstate\n",
      "couldn't match reinstate\n",
      "couldn't match answer\n",
      "couldn't match reflect\n",
      "couldn't match exploit\n",
      "couldn't match suspend\n",
      "couldn't match suspend\n",
      "couldn't match suspend\n",
      "couldn't match alter\n",
      "couldn't match hug\n",
      "couldn't match do\n",
      "couldn't match turn\n",
      "couldn't match court\n",
      "couldn't match be\n",
      "couldn't match differentiate\n",
      "couldn't match have\n",
      "couldn't match shrink\n",
      "couldn't match backtrack\n",
      "couldn't match diversify\n",
      "couldn't match distract\n",
      "couldn't match outrun\n",
      "couldn't match single\n",
      "couldn't match ramp\n",
      "couldn't match discard\n",
      "couldn't match suspend\n",
      "couldn't match suspend\n",
      "couldn't match suspend\n",
      "couldn't match download\n",
      "couldn't match turn\n",
      "couldn't match inherit\n",
      "couldn't match single\n",
      "couldn't match single\n",
      "couldn't match overcome\n",
      "couldn't match outsmart\n",
      "couldn't match turn\n",
      "couldn't match rail\n",
      "couldn't match become\n",
      "couldn't match finish\n",
      "couldn't match disqualify\n",
      "couldn't match sleep\n",
      "couldn't match star\n",
      "couldn't match play\n",
      "couldn't match rename\n",
      "couldn't match exploit\n",
      "couldn't match have\n",
      "couldn't match sink\n",
      "couldn't match run\n",
      "couldn't match ramp\n",
      "couldn't match discard\n",
      "couldn't match intoto\n",
      "couldn't match reporters.1:59pm\n",
      "couldn't match suspend\n",
      "couldn't match pitch\n",
      "couldn't match carve\n",
      "couldn't match plunk\n",
      "couldn't match have\n",
      "couldn't match lose\n",
      "couldn't match disqualify\n",
      "couldn't match have\n",
      "couldn't match sink\n",
      "couldn't match intoto\n",
      "couldn't match reporters.1:59pm\n",
      "couldn't match suspend\n",
      "couldn't match lose\n",
      "couldn't match ramp\n",
      "couldn't match ramp\n",
      "couldn't match miss\n",
      "couldn't match suspend\n",
      "couldn't match play\n",
      "couldn't match finish\n",
      "couldn't match chip\n",
      "couldn't match know\n",
      "couldn't match bury\n",
      "couldn't match feel\n",
      "couldn't match intoto\n",
      "couldn't match reporters.1:59pm\n",
      "couldn't match suspend\n",
      "couldn't match download\n",
      "couldn't match have\n",
      "couldn't match reassess\n",
      "couldn't match work\n",
      "couldn't match cement\n",
      "couldn't match feel\n",
      "couldn't match do\n",
      "couldn't match consolidate\n",
      "couldn't match intoto\n",
      "couldn't match reporters.1:59pm\n",
      "couldn't match suspend\n",
      "couldn't match tweet\n",
      "couldn't match distract\n",
      "couldn't match energize\n",
      "couldn't match convince\n",
      "couldn't match climb\n",
      "couldn't match cement\n",
      "couldn't match elevate\n",
      "couldn't match outperform\n",
      "couldn't match edge\n",
      "couldn't match outstrip\n",
      "couldn't match have\n",
      "couldn't match suspend\n",
      "couldn't match prolong\n",
      "couldn't match consolidate\n",
      "couldn't match download\n",
      "couldn't match renew\n",
      "couldn't match ramp\n",
      "couldn't match intoto\n",
      "couldn't match reporters.1:59pm\n",
      "couldn't match suspend\n",
      "couldn't match download\n",
      "couldn't match operate\n",
      "couldn't match have\n",
      "couldn't match balance\n",
      "couldn't match balance\n",
      "couldn't match hawk\n",
      "couldn't match hawk\n",
      "couldn't match hawk\n",
      "couldn't match feel\n",
      "couldn't match alienate\n",
      "couldn't match convince\n",
      "couldn't match climb\n",
      "couldn't match miss\n",
      "couldn't match cement\n",
      "couldn't match uninvite\n",
      "couldn't match have\n",
      "couldn't match suspend\n",
      "couldn't match prolong\n",
      "couldn't match rapt\n",
      "couldn't match practice\n",
      "couldn't match download\n",
      "couldn't match download\n",
      "couldn't match ram\n",
      "couldn't match suspend\n",
      "couldn't match dial\n",
      "couldn't match suspend\n",
      "couldn't match reconstitute\n",
      "couldn't match post\n",
      "couldn't match suspend\n",
      "couldn't match prolong\n",
      "couldn't match ram\n",
      "couldn't match download\n",
      "couldn't match download\n",
      "couldn't match suspend\n",
      "couldn't match suspend\n",
      "couldn't match pause\n",
      "couldn't match Press)In\n",
      "couldn't match lose\n",
      "couldn't match reclaim\n",
      "couldn't match peg\n",
      "couldn't match stoke\n",
      "couldn't match have\n",
      "couldn't match accompany\n",
      "couldn't match suspend\n",
      "couldn't match justify\n",
      "couldn't match prevail\n",
      "couldn't match post\n",
      "couldn't match destabilise\n",
      "couldn't match front\n",
      "couldn't match like\n",
      "couldn't match transform\n",
      "couldn't match label\n",
      "couldn't match assail\n",
      "couldn't match seal\n",
      "couldn't match por\n",
      "couldn't match number\n",
      "couldn't match respond\n",
      "couldn't match have\n",
      "couldn't match feel\n",
      "couldn't match be\n",
      "couldn't match have\n",
      "couldn't match run\n",
      "couldn't match portray\n",
      "couldn't match front\n",
      "couldn't match become\n",
      "couldn't match work\n",
      "couldn't match lull\n",
      "couldn't match liberalize\n",
      "couldn't match prevail\n",
      "couldn't match compare\n",
      "couldn't match wean\n",
      "couldn't match lose\n",
      "couldn't match download\n",
      "couldn't match front\n",
      "couldn't match like\n",
      "couldn't match transform\n",
      "couldn't match label\n",
      "couldn't match accompany\n",
      "couldn't match loom\n",
      "couldn't match reference\n",
      "couldn't match have\n",
      "couldn't match lose\n",
      "couldn't match decrease\n",
      "couldn't match work\n",
      "couldn't match run\n",
      "couldn't match become\n"
     ]
    }
   ],
   "source": [
    "triples_2 = []\n",
    "ma_df = pd.DataFrame(matches_2, columns = [\"text\",\"subj\", \"verb\",\"obj\"])\n",
    "for row in ma_df.iterrows():\n",
    "    if row[1][\"verb\"] in spec_dict:\n",
    "        for poss_pattern in spec_dict[row[1][\"verb\"]]:\n",
    "            if set(poss_pattern.split()).intersection(row[1][\"text\"].split()) == set(poss_pattern.split()):\n",
    "                triples_2.append(f\"<triplet> {row[1]['subj']} <subj> {row[1]['obj']} <obj> {spec_code[poss_pattern]}\")\n",
    "    elif row[1][\"verb\"] in verb_dict:\n",
    "            triples_2.append(f\"<triplet> {row[1]['subj']} <subj> {row[1]['obj']} <obj> {verb_dict[row[1]['verb']]}\")\n",
    "    else: print(f\"couldn't match {row[1]['verb']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(triples_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<triplet> Emmanuel Macron <subj> Ukraine <obj> Reject',\n",
       " '<triplet> Finland <subj> Turkey <obj> Disapprove',\n",
       " '<triplet> the Associated Press <subj> Iran <obj> Make Public Statement',\n",
       " '<triplet> Vladimir Putin <subj> Vladimir Putin <obj> Provide Aid',\n",
       " \"<triplet> Liudmyla Mymrykova <subj> Liudmyla Mymrykova's <obj> Assault\",\n",
       " '<triplet> Indonesia <subj> Indonesia <obj> Reduce Relations',\n",
       " '<triplet> China <subj> Saudis <obj> Engage In Material Cooperation',\n",
       " '<triplet> Iran <subj> the Hamburg Islamic Center <obj> Make Public Statement',\n",
       " '<triplet> Anoush <subj> Anoush <obj> Engage In Diplomatic Cooperation',\n",
       " '<triplet> Bo Xilai <subj> Chongqing <obj> Engage In Material Cooperation']"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triples_2[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<triplet> Emmanuel Macron <subj> Ukraine <obj> Reject',\n",
       " '<triplet> Finland <subj> Turkey <obj> Disapprove',\n",
       " '<triplet> the Associated Press <subj> Iran <obj> Make Public Statement',\n",
       " '<triplet> Vladimir Putin <subj> Vladimir Putin <obj> Provide Aid',\n",
       " \"<triplet> Liudmyla Mymrykova <subj> Liudmyla Mymrykova's <obj> Assault\",\n",
       " '<triplet> Indonesia <subj> Indonesia <obj> Reduce Relations',\n",
       " '<triplet> China <subj> Saudis <obj> Engage In Material Cooperation',\n",
       " '<triplet> the Hamburg Islamic Center <subj> Iran <obj> Make Public Statement',\n",
       " '<triplet> Anoush <subj> Anoush <obj> Engage In Diplomatic Cooperation',\n",
       " '<triplet> Bo Xilai <subj> Chongqing <obj> Engage In Material Cooperation']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triples[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "226"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(triples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrial with chunks instead of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import xml.etree.ElementTree as ET\n",
    "from spacy.symbols import nsubj, dobj, pobj, iobj, neg, xcomp, VERB\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def read_lines(inputparsed):    \n",
    "    \"\"\"takes input from CoreNLP sentence parsed file and returns sentences\"\"\"\n",
    "    #parse all lines from CoreNLP sentence split\n",
    "    parsed = open(inputparsed, encoding = \"utf-8\")\n",
    "    parsedfile = parsed.readlines()\n",
    "    parsedlines = []\n",
    "\n",
    "    #Only keep those lines which have Sentence #n in the line before\n",
    "    for idx, text in enumerate(parsedfile):\n",
    "        if text.startswith(\"Sentence #\"):\n",
    "            parsedlines.append(parsedfile[idx+1].replace('\\n','').strip())\n",
    "    \n",
    "    return parsedlines\n",
    "\n",
    "def gen_poss(line, verb_match, pre_dict):\n",
    "    \"\"\"generates all possibilities of patterns that a multi word line implies,\n",
    "    by extracting partial patterns and resolving placeholder words\"\"\"\n",
    "    poss = []\n",
    "\n",
    "    #replace special tokens in text that are clear at this point\n",
    "    line = line.replace(\"*\", verb_match)\n",
    "    line = line.replace(\"- \", \"\")\n",
    "    line = line.replace(\"+\",\"\")\n",
    "    line = line.replace(\"%\",\"\")\n",
    "    line = line.replace(\"^\",\"\")\n",
    "    line = line.replace(\"$\",\"\")\n",
    "\n",
    "    #split line by possibility indicators and code (always ends possibility)\n",
    "    #example.: \"- $ * (P ON KILLING (P OF + [010] #  COMMENT <ELH 07 May 2008>\"\n",
    "    poss_split = re.split(\"\\(P |\\[.*]\",line) \n",
    "\n",
    "    if len(poss_split) > 2: #2 is if no (P in the line\n",
    "        #only combining the first (P, as they share the same code \n",
    "        #and the longer version will never be contained in a text if the shorter isnt\n",
    "        poss.append(strip_multiple_whitespaces(\" \".join(poss_split[:2])).lower().rstrip().lstrip())\n",
    "    else: \n",
    "        poss.append(strip_multiple_whitespaces(poss_split[0].lower().rstrip().lstrip()))\n",
    "\n",
    "    cleaned = []\n",
    "    for text in list(set(poss)):\n",
    "        c = 0\n",
    "        for tag in list(pre_dict.keys()):\n",
    "            if tag in text:\n",
    "                for replacement in pre_dict[tag]:\n",
    "                    cleaned.append(text.replace(tag, replacement))\n",
    "                    c += 1\n",
    "        if c == 0:\n",
    "            cleaned.append(text)\n",
    "\n",
    "    return cleaned\n",
    "    \n",
    "\n",
    "def verb_code_dict(pico_path, verb_path):\n",
    "    \"\"\"reads coding ontology and verb lists, \n",
    "    directly matches verbs to their CAMEO codes and returns this verbs:codes dictionairy.\n",
    "    verb with codes that cannot be read are printed out as full line of the file\"\"\"\n",
    "    #read PETRARCH Internal Coding Ontology (= pico)\n",
    "    pico_path = os.path.join(os.getcwd(), pico_path)\n",
    "    pico_file = open(pico_path, 'r')\n",
    "    pico_lines = pico_file.readlines()\n",
    "\n",
    "    #get all 20 codes with their respective code\n",
    "    main_codes = {}                             #we run one iteration for all the main codes, only main codes contain relation name\n",
    "    for line in pico_lines:\n",
    "        line = line.split('#')\n",
    "        if line[0] == \"\" or line[0] == \"\\n\":    #only intro comments and empty lines\n",
    "            continue\n",
    "        else: \n",
    "            code_split = line[0].split(\":\")     #splits into CAMEO code and related hex\n",
    "            if len(line) > 1 and code_split[0][2] == \"0\":      #only main categories have 0 in 3rd idx, [cat_num 0] -> [010]\n",
    "                main_codes[code_split[0][:2]] = line[-1].replace(\"\\n\",\"\")\n",
    "    \n",
    "    #map code to code we want to use in the training\n",
    "    map_codes = {\"DiplomaticCoop\" : \"Engage In Diplomatic Cooperation\", \n",
    "                \"MaterialCoop\" : \"Engage In Material Cooperation\",\n",
    "                \"ProvideAid\" : \"Provide Aid\",\n",
    "                \"Exhibit Force Posture\": \"Exhibit Military Posture\",\n",
    "                \"Use Unconventional Mass Violence\" : \"Engage In Unconventional Mass Violence\"}\n",
    "    main_codes = {k: (map_codes[v] if v in map_codes else v) for k, v in main_codes.items()}\n",
    "    \n",
    "    #read single word patterns and match their code to the relation extracted in main_codes\n",
    "    verb_path = os.path.join(os.getcwd(), verb_path)\n",
    "    verb_file = open(verb_path, 'r')\n",
    "    verb_lines = verb_file.readlines()\n",
    "    \n",
    "    verb_dict = {}\n",
    "    for line in verb_lines:\n",
    "        if line[0] == \"#\":\n",
    "            continue\n",
    "        elif line.startswith(\"---\"):    #main verbs have a lead code, which is applied to all very in the section\n",
    "                                        #unless a separate code is specified for a specific verb in section\n",
    "            try: cur_main_code = re.split(\"\\[|\\]|---\", line)[2].replace(\":\",\"\")[:2]  #we only need main codes which are first two numbers\n",
    "                                                                                #sometimes code starts with \":\", e.g.: ---  OFFEND   [:110]  ---\n",
    "                                                                                #we just remove those to get the main code\n",
    "            except:                     #depending on chosen verb dictionairy, there may be main verbs without lead codes\n",
    "                print(\"couldn't finde code in: \", line.replace(\"\\n\",\"\")) \n",
    "                cur_main_code == \"--\"\n",
    "            if cur_main_code == \"\": cur_main_code = \"--\"\n",
    "        elif line == \"\\n\":              #skip empty lines\n",
    "            continue\n",
    "        elif line[0] == \"-\" or line[0] == \"~\" or line[0] == \"+\" or line[0] == \"&\": #removes all special structures we cannot use\n",
    "            continue\n",
    "        else:\n",
    "            if len(re.split(\"\\[|\\]\", line)) > 1:    #verbs with their own code, e.g.: AFFIRM [051] \n",
    "                code = re.split(\"\\[|\\]\", line)[1].replace(\":\",\"\")[:2]\n",
    "                if code != \"--\":\n",
    "                    if \"{\" in line:         #conjugated verbs, e.g. \"APPLY {APPLYING APPLIED APPLIES } [020]\"\n",
    "                        line_s = re.split(\"\\{|\\}\", line)    #split at { and }\n",
    "                        verb_dict[line_s[0].lower()] = main_codes[code] \n",
    "                        for word in line_s[1].split():\n",
    "                            verb_dict[word.lower()] = main_codes[code]\n",
    "                    else:\n",
    "                        word = re.split(\"\\[|\\]\", line)[0]\n",
    "                        verb_dict[word.lower()] = main_codes[code]\n",
    "            else:\n",
    "                if cur_main_code != \"--\":\n",
    "                    if \"{\" in line:         #e.g. \"HURRY {HURRIES HURRYING HURRIED }\" \n",
    "                        line_s = re.split(\"\\{|\\}\", line)    #split at { and }\n",
    "                        verb_dict[line_s[0].lower()] = main_codes[cur_main_code]\n",
    "                        for word in line_s[1].split():\n",
    "                            verb_dict[word.lower()] = main_codes[cur_main_code]\n",
    "                    else:                   #only single words with sometimes comments, e.g.: CENSURE  # JON 5/17/95\n",
    "                        word = line.split(\"#\")[0].rstrip()    #gets part before \"#\", removes all whitespaces to the right\n",
    "                        verb_dict[word.lower()] = main_codes[cur_main_code]\n",
    "\n",
    "    #read multi word patterns and create a dictionary for their code\n",
    "\n",
    "    #get filler words that occur in multi word patterns\n",
    "    verb_file = open(verb_path, 'r')\n",
    "    verb_lines = verb_file.readlines()\n",
    "\n",
    "    pre_dict = {}\n",
    "    filter_list = []\n",
    "    for line in verb_lines:\n",
    "        if line.startswith(\"&\"):\n",
    "            cur_filter = line.rstrip()\n",
    "        elif line.startswith(\"\\n\") and \"cur_filter\" in locals():\n",
    "            pre_dict[cur_filter.lower()] = filter_list\n",
    "            cur_filter = \"\"\n",
    "            filter_list = []\n",
    "        elif line.startswith(\"+\") and cur_filter != \"\":\n",
    "            filter_list.append(line.rstrip()[1:].replace(\"_\", \"\").lower())\n",
    "    del pre_dict[\"\"]\n",
    "\n",
    "    #generate dictionaries for multi word patterns\n",
    "    verb_file = open(verb_path, 'r')\n",
    "    verb_lines = verb_file.readlines()\n",
    "\n",
    "    spec_dict = {}\n",
    "    spec_code = {}\n",
    "\n",
    "    count = 0\n",
    "    for line in verb_lines:\n",
    "        if line.startswith(\"- \"):\n",
    "            #get main verb as dict key\n",
    "            try: \n",
    "                verb_match = re.search(\"# *\\w+\", line).group()\n",
    "                verb_match = re.search(\"\\w+\", verb_match).group()\n",
    "                verb_match = verb_match.replace(\"_\", \" \").lower()\n",
    "            except: \n",
    "                count += 1\n",
    "\n",
    "            #get code for line\n",
    "            try:\n",
    "                code = re.search(\"\\[.*]\", line).group()[1:3]\n",
    "                if code != \"--\":\n",
    "                    #get all possibility that the line indicates\n",
    "                    poss = gen_poss(line, verb_match, pre_dict)\n",
    "                    for pattern in poss:\n",
    "                        spec_code[pattern] = main_codes[code]\n",
    "                    spec_dict[verb_match] = poss\n",
    "            except:\n",
    "                count += 1\n",
    "\n",
    "    print(f\"{count} patterns could not be loaded\")        \n",
    "\n",
    "    return verb_dict, spec_dict, spec_code\n",
    "\n",
    "\n",
    "def get_triples(sentence, verb_dict, spec_dict, spec_code, nlp):\n",
    "    \"\"\"create triplet structure for training from text input, \n",
    "    verb_dict needs to be loaded before,\n",
    "    spacy model needs to be initialized before \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    verbs = []\n",
    "    dict = {}\n",
    "\n",
    "\n",
    "    for possible_verb in doc:\n",
    "        if possible_verb.pos == VERB:\n",
    "            if neg in [child.dep for child in possible_verb.children]: continue\n",
    "            else: \n",
    "                for possible_subject in possible_verb.children: \n",
    "                    if possible_subject.dep == xcomp:   #subj / obj of composed verb should also be subj / obj of main verb\n",
    "                        main_verb = possible_subject\n",
    "                        main_idx = possible_subject.idx\n",
    "                        \n",
    "                        for chunk in doc.noun_chunks:\n",
    "                            if chunk.root.dep_ == \"poss\":\n",
    "                                if chunk.root.head.head.idx == possible_verb.idx:\n",
    "                                    verbs.append([main_idx, main_verb.lemma_, chunk.text, chunk.root.head.dep_])\n",
    "                                    if main_idx in dict.keys(): dict[main_idx] += 1\n",
    "                                    else: dict[main_idx] = 1\n",
    "                            else:\n",
    "                                if chunk.root.head.idx == possible_verb.idx:\n",
    "                                    verbs.append([main_idx, main_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "                                    if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "                                    else: dict[possible_verb.idx] = 1\n",
    "\n",
    "                for chunk in doc.noun_chunks:       #for normal verbs, check chunks directly\n",
    "        #                 if chunk.root.head.idx == possible_verb.idx:\n",
    "                    if chunk.root.head.dep_ == \"poss\":\n",
    "                        if chunk.root.head.head.idx == possible_verb.idx:\n",
    "                            verbs.append([possible_verb.idx, possible_verb.lemma_, chunk.text, chunk.root.head.dep_])\n",
    "                            if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "                            else: dict[possible_verb.idx] = 1\n",
    "                    else:\n",
    "                        if chunk.root.head.idx == possible_verb.idx:\n",
    "                            verbs.append([possible_verb.idx, possible_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "                            if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "                            else: dict[possible_verb.idx] = 1\n",
    "\n",
    "    trip_idx = [key for key in dict if dict[key] > 1]\n",
    "\n",
    "    # doc = nlp(sentence)\n",
    "    # verbs = []\n",
    "    # dict = {}\n",
    "\n",
    "    # for possible_verb in doc:           #parses through all words in sentence\n",
    "    #     if possible_verb.pos == VERB:   #we only care about verbs\n",
    "    #         if neg in [child.dep for child in possible_verb.children]: continue #we exclude all negated verbs\n",
    "    #         else: \n",
    "    #             for candidate in possible_verb.children: #for composed verbs of verb (e.g. \"want to join\" -> \"want join\")\n",
    "    #                 if candidate.dep == xcomp:   #subj / obj of composed verb should also be subj / obj of main verb\n",
    "    #                     main_verb = candidate    \n",
    "    #                     main_idx = candidate.idx\n",
    "    #                     for chunk in doc.noun_chunks:   #chunks are noun-groups (e.g.: \"78 out of 100 people\" instead of \"people\")\n",
    "    #                         if chunk.root.head.idx == possible_verb.idx:    #if chunk applies to xcomp (want),\n",
    "    #                                                                         #treat it like it aplles to main verb (\"join\")\n",
    "    #                             verbs.append([main_idx, main_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "    #                             if main_idx in dict.keys(): dict[main_idx] += 1 #count how often verb is used\n",
    "    #                             else: dict[main_idx] = 1\n",
    "\n",
    "    #             for chunk in doc.noun_chunks:       #for normal verbs, check chunks directly\n",
    "    #                 if chunk.root.head.idx == possible_verb.idx:\n",
    "    #                     verbs.append([possible_verb.idx, possible_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "    #                     if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "    #                     else: dict[possible_verb.idx] = 1\n",
    "    \n",
    "    # trip_idx = [key for key in dict if dict[key] > 1]   #if verbs used more than once, its candidate for triplet\n",
    "\n",
    "    #priority for subj-relation-obj triplets\n",
    "    mapper = {\"nsubj\":1,\"dobj\":2, \"pobj\":2, \"iobj\":2}\n",
    "\n",
    "    #create df from verbs extracted \n",
    "    df = pd.DataFrame(verbs, columns = [\"idx\", \"verb\", \"noun\", \"noun_type\"])\n",
    "    df[\"noun_map\"] = df.noun_type.map(mapper)  #turn noun_types into priority \n",
    "    return df\n",
    "\n",
    "    # #create groups that resolve around same word\n",
    "    # gb = df.groupby('idx')    \n",
    "    # #only keep groups if verb idx was identified as potential triplet before, sort by priority for structure\n",
    "    # df_l = [gb.get_group(x).sort_values(\"noun_map\") for x in gb.groups if gb.get_group(x).idx.iloc[0] in dict]\n",
    "    # matches = [merge_trip(group) for group in df_l if not merge_trip(group) == None] #get groups into triplet structure\n",
    "    \n",
    "    # #turn matches into triples by only keeping those with coded verbs, return code instead of verb\n",
    "    # triples = []\n",
    "    # for match in matches:\n",
    "    #     if match[1].lower() in spec_dict:\n",
    "    #         for poss_pattern in spec_dict[match[1].lower()]:\n",
    "    #             if set(poss_pattern.split()).intersection(sentence.split()) == set(poss_pattern.split()):\n",
    "    #                 triples.append(f\"<triplet> {match[0]} <subj> {match[2]} <obj> {spec_code[poss_pattern]}\")\n",
    "                    \n",
    "    #     elif match[1].lower() in verb_dict:\n",
    "    #         triples.append(f\"<triplet> {match[0]} <subj> {match[2]} <obj> {verb_dict[match[1].lower()]}\")\n",
    "    #     else: print(f\"couldn't match {match[1].lower()}\")\n",
    "\n",
    "    # #triples = [f\"<triplet> {match[0]} <subj> {match[2]} <obj> {verb_dict[match[1].lower()]}\" for match in matches if match[1].lower() in verb_dict]\n",
    "\n",
    "    # return triples\n",
    "\n",
    "def merge_trip(df):\n",
    "    \"\"\"helper function to turn two rows of a pandas groupby into subj, verb, obj\"\"\"\n",
    "    if df.shape[0] == 2:\n",
    "        if df.noun_type.iloc[0] != df.noun_type.iloc[1]:\n",
    "            return [df.iloc[0].noun, df.iloc[0].verb, df.iloc[1].noun]\n",
    "    elif df.shape[0] > 2:\n",
    "        for i in range(df.shape[0] - 1):\n",
    "            if df.noun_type.iloc[i] != df.noun_type.iloc[i+1]:\n",
    "                return [df.iloc[0].noun, df.iloc[0].verb, df.iloc[1].noun]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [[sentence, get_triples(sentence, verb_dict, spec_dict, spec_code, nlp)] for sentence in read]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21395"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidats = [df for df in dfs if df[1].shape[0] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16590"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = []\n",
    "for can in candidats:\n",
    "    for idx in can[1][\"idx\"]:\n",
    "        if can[1][\"idx\"].to_list().count(idx) > 1:\n",
    "            trips.append([can[0], can[1]])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12594"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trips)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = []\n",
    "for idx, df in enumerate(trips):\n",
    "    #create groups that resolve around same word\n",
    "    gb = df[1].groupby('idx')    \n",
    "    #only keep groups if verb idx was identified as potential triplet before, sort by priority for structure\n",
    "    for x in gb.groups:\n",
    "        if gb.get_group(x).shape[0] == 2:\n",
    "            if gb.get_group(x).noun_type.iloc[0] != gb.get_group(x).noun_type.iloc[1]:\n",
    "                matches.append([df[0], gb.get_group(x).iloc[0].noun, gb.get_group(x).iloc[0].verb, gb.get_group(x).iloc[1].noun])\n",
    "        elif gb.get_group(x).shape[0] > 2:\n",
    "            for i in range(gb.get_group(x).shape[0] - 1):\n",
    "                if gb.get_group(x).noun_type.iloc[i] != gb.get_group(x).noun_type.iloc[i+1]:\n",
    "                    matches.append([df[0], gb.get_group(x).iloc[0].noun, gb.get_group(x).iloc[0].verb, gb.get_group(x).iloc[1].noun])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20489"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triples = []\n",
    "# ma_df = pd.DataFrame(matches, columns = [\"text\",\"subj\", \"verb\",\"obj\"])\n",
    "# for row in ma_df.iterrows():\n",
    "#     if row[1][\"verb\"] in spec_dict:\n",
    "#         for poss_pattern in spec_dict[row[1][\"verb\"]]:\n",
    "#             if set(poss_pattern.split()).intersection(row[1][\"text\"].split()) == set(poss_pattern.split()):\n",
    "#                 triples.append([row[1][\"text\"], f\"<triplet> {row[1]['subj']} <subj> {row[1]['obj']} <obj> {spec_code[poss_pattern]}\"])\n",
    "#     elif row[1][\"verb\"] in verb_dict:\n",
    "#             triples.append([row[1][\"text\"], f\"<triplet> {row[1]['subj']} <subj> {row[1]['obj']} <obj> {verb_dict[row[1]['verb']]}\"])\n",
    "\n",
    "triples = []\n",
    "ma_df = pd.DataFrame(matches, columns = [\"text\",\"subj\", \"verb\",\"obj\"])\n",
    "for row in ma_df.iterrows():\n",
    "    if row[1][\"verb\"] in spec_dict:\n",
    "        for poss_pattern in spec_dict[row[1][\"verb\"]]:\n",
    "            if set(poss_pattern.split()).intersection(row[1][\"text\"].split()) == set(poss_pattern.split()):\n",
    "                triples.append([row[1][\"text\"], row[1]['subj'], row[1]['obj'] , spec_code[poss_pattern]])\n",
    "    elif row[1][\"verb\"] in verb_dict:\n",
    "            triples.append([row[1][\"text\"], row[1]['subj'], row[1]['obj'], verb_dict[row[1]['verb']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "huh = pd.DataFrame(triples, columns = [\"text\", \"subj\", \"obj\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1249"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huh.text.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subj</th>\n",
       "      <th>obj</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For more on our coverage of the war in Ukraine...</td>\n",
       "      <td>loaded ships</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>Consult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>For more on our coverage of the war in Ukraine...</td>\n",
       "      <td>loaded ships</td>\n",
       "      <td>Ukrainian ports</td>\n",
       "      <td>Consult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Russia must clearly understand that Russia wi...</td>\n",
       "      <td>Russia</td>\n",
       "      <td>a harsh global response</td>\n",
       "      <td>Consult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Russia must clearly understand that Russia wi...</td>\n",
       "      <td>70,000 people</td>\n",
       "      <td>the Kherson region</td>\n",
       "      <td>Yield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Russian President Vladimir Putin told Erdogan ...</td>\n",
       "      <td>President Emmanuel Macron</td>\n",
       "      <td>Ukraine's defence needs</td>\n",
       "      <td>Reject</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  For more on our coverage of the war in Ukraine...   \n",
       "1  For more on our coverage of the war in Ukraine...   \n",
       "2  \"Russia must clearly understand that Russia wi...   \n",
       "3  \"Russia must clearly understand that Russia wi...   \n",
       "4  Russian President Vladimir Putin told Erdogan ...   \n",
       "\n",
       "                        subj                      obj    label  \n",
       "0               loaded ships                  Ukraine  Consult  \n",
       "1               loaded ships          Ukrainian ports  Consult  \n",
       "2                     Russia  a harsh global response  Consult  \n",
       "3              70,000 people       the Kherson region    Yield  \n",
       "4  President Emmanuel Macron  Ukraine's defence needs   Reject  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huh.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For more on\\xa0our coverage of the war in Ukraine, click here.11:44pm:\\xa0UN grain coordinator\\xa0expects loaded ships to depart Ukraine on ThursdayThe UN coordinator for the Ukraine Black Sea grain deal said UN grain coordinator expects loaded ships to depart Ukrainian ports on ThursdayThe.'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "\n",
    "premise = sequence\n",
    "hypothesis = f'{subj} does {rel} towards {obj} .'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = huh.text.iloc[0]\n",
    "subj = huh.subj.iloc[0]\n",
    "rel = huh.obj.iloc[0]\n",
    "obj =  huh.label.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\svawe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:2339: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "probs_l = []\n",
    "for row in huh.iterrows():\n",
    "    sequence = row[1][\"text\"]\n",
    "    subj = row[1][\"subj\"]\n",
    "    rel = row[1][\"label\"]\n",
    "    obj =  row[1][\"obj\"]\n",
    "\n",
    "    # run through model pre-trained on MNLI\n",
    "    x = tokenizer.encode(premise, hypothesis, return_tensors='pt',\n",
    "                        truncation_strategy='only_first')\n",
    "    logits = nli_model(x)[0]\n",
    "\n",
    "    # we throw away \"neutral\" (dim 1) and take the probability of\n",
    "    # \"entailment\" (2) as the probability of the label being true \n",
    "    entail_contradiction_logits = logits[:,[0,2]]\n",
    "    probs = entail_contradiction_logits.softmax(dim=1)\n",
    "    prob_label_is_true = probs[:,1]\n",
    "\n",
    "    probs_l.append([row[0], prob_label_is_true])\n",
    "    if row[0] % 100 == 0: print(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb6e93dfa86b7f97c60c39b0c0c403458d25a4deb04fc60caf4cf2dd07f32ebe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
