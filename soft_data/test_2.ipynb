{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import xml.etree.ElementTree as ET\n",
    "from spacy.symbols import nsubj, dobj, pobj, iobj, neg, xcomp, ccomp, VERB\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def read_lines(inputparsed):    \n",
    "    \"\"\"takes input from CoreNLP sentence parsed file and returns sentences\"\"\"\n",
    "    #parse all lines from CoreNLP sentence split\n",
    "    parsed = open(inputparsed, encoding = \"utf-8\")\n",
    "    parsedfile = parsed.readlines()\n",
    "    parsedlines = []\n",
    "\n",
    "    #Only keep those lines which have Sentence #n in the line before\n",
    "    for idx, text in enumerate(parsedfile):\n",
    "        if text.startswith(\"Sentence #\"):\n",
    "            parsedlines.append(parsedfile[idx+1].replace('\\n','').strip())\n",
    "    \n",
    "    return parsedlines\n",
    "\n",
    "def gen_poss(line, verb_match, pre_dict):\n",
    "    \"\"\"generates all possibilities of patterns that a multi word line implies,\n",
    "    by extracting partial patterns and resolving placeholder words\"\"\"\n",
    "    poss = []\n",
    "\n",
    "    #replace special tokens in text that are clear at this point\n",
    "    line = line.replace(\"*\", verb_match)\n",
    "    line = line.replace(\"- \", \"\")\n",
    "    line = line.replace(\"+\",\"\")\n",
    "    line = line.replace(\"%\",\"\")\n",
    "    line = line.replace(\"^\",\"\")\n",
    "    line = line.replace(\"$\",\"\")\n",
    "\n",
    "    #split line by possibility indicators and code (always ends possibility)\n",
    "    #example.: \"- $ * (P ON KILLING (P OF + [010] #  COMMENT <ELH 07 May 2008>\"\n",
    "    poss_split = re.split(\"\\(P |\\[.*]\",line) \n",
    "\n",
    "    if len(poss_split) > 2: #2 is if no (P in the line\n",
    "        #only combining the first (P, as they share the same code \n",
    "        #and the longer version will never be contained in a text if the shorter isnt\n",
    "        poss.append(strip_multiple_whitespaces(\" \".join(poss_split[:2])).lower().rstrip().lstrip())\n",
    "    else: \n",
    "        poss.append(strip_multiple_whitespaces(poss_split[0].lower().rstrip().lstrip()))\n",
    "\n",
    "    cleaned = []\n",
    "    for text in list(set(poss)):\n",
    "        c = 0\n",
    "        for tag in list(pre_dict.keys()):\n",
    "            if tag in text:\n",
    "                for replacement in pre_dict[tag]:\n",
    "                    cleaned.append(text.replace(tag, replacement))\n",
    "                    c += 1\n",
    "        if c == 0:\n",
    "            cleaned.append(text)\n",
    "\n",
    "    return cleaned\n",
    "    \n",
    "\n",
    "def verb_code_dict(pico_path, verb_path):\n",
    "    \"\"\"reads coding ontology and verb lists, \n",
    "    directly matches verbs to their CAMEO codes and returns this verbs:codes dictionairy.\n",
    "    verb with codes that cannot be read are printed out as full line of the file\"\"\"\n",
    "    #read PETRARCH Internal Coding Ontology (= pico)\n",
    "    pico_path = os.path.join(os.getcwd(), pico_path)\n",
    "    pico_file = open(pico_path, 'r')\n",
    "    pico_lines = pico_file.readlines()\n",
    "\n",
    "    #get all 20 codes with their respective code\n",
    "    main_codes = {}                             #we run one iteration for all the main codes, only main codes contain relation name\n",
    "    for line in pico_lines:\n",
    "        line = line.split('#')\n",
    "        if line[0] == \"\" or line[0] == \"\\n\":    #only intro comments and empty lines\n",
    "            continue\n",
    "        else: \n",
    "            code_split = line[0].split(\":\")     #splits into CAMEO code and related hex\n",
    "            if len(line) > 1 and code_split[0][2] == \"0\":      #only main categories have 0 in 3rd idx, [cat_num 0] -> [010]\n",
    "                main_codes[code_split[0][:2]] = line[-1].replace(\"\\n\",\"\")\n",
    "    \n",
    "    #map code to code we want to use in the training\n",
    "    map_codes = {\"DiplomaticCoop\" : \"Engage In Diplomatic Cooperation\", \n",
    "                \"MaterialCoop\" : \"Engage In Material Cooperation\",\n",
    "                \"ProvideAid\" : \"Provide Aid\",\n",
    "                \"Exhibit Force Posture\": \"Exhibit Military Posture\",\n",
    "                \"Use Unconventional Mass Violence\" : \"Engage In Unconventional Mass Violence\"}\n",
    "    main_codes = {k: (map_codes[v] if v in map_codes else v) for k, v in main_codes.items()}\n",
    "    \n",
    "    #read single word patterns and match their code to the relation extracted in main_codes\n",
    "    verb_path = os.path.join(os.getcwd(), verb_path)\n",
    "    verb_file = open(verb_path, 'r')\n",
    "    verb_lines = verb_file.readlines()\n",
    "    \n",
    "    verb_dict = {}\n",
    "    for line in verb_lines:\n",
    "        if line[0] == \"#\":\n",
    "            continue\n",
    "        elif line.startswith(\"---\"):    #main verbs have a lead code, which is applied to all very in the section\n",
    "                                        #unless a separate code is specified for a specific verb in section\n",
    "            try: cur_main_code = re.split(\"\\[|\\]|---\", line)[2].replace(\":\",\"\")[:2]  #we only need main codes which are first two numbers\n",
    "                                                                                #sometimes code starts with \":\", e.g.: ---  OFFEND   [:110]  ---\n",
    "                                                                                #we just remove those to get the main code\n",
    "            except:                     #depending on chosen verb dictionairy, there may be main verbs without lead codes\n",
    "                print(\"couldn't finde code in: \", line.replace(\"\\n\",\"\")) \n",
    "                cur_main_code == \"--\"\n",
    "            if cur_main_code == \"\": cur_main_code = \"--\"\n",
    "        elif line == \"\\n\":              #skip empty lines\n",
    "            continue\n",
    "        elif line[0] == \"-\" or line[0] == \"~\" or line[0] == \"+\" or line[0] == \"&\": #removes all special structures we cannot use\n",
    "            continue\n",
    "        else:\n",
    "            if len(re.split(\"\\[|\\]\", line)) > 1:    #verbs with their own code, e.g.: AFFIRM [051] \n",
    "                code = re.split(\"\\[|\\]\", line)[1].replace(\":\",\"\")[:2]\n",
    "                if code != \"--\":\n",
    "                    if \"{\" in line:         #conjugated verbs, e.g. \"APPLY {APPLYING APPLIED APPLIES } [020]\"\n",
    "                        line_s = re.split(\"\\{|\\}\", line)    #split at { and }\n",
    "                        verb_dict[line_s[0].lower()] = main_codes[code] \n",
    "                        for word in line_s[1].split():\n",
    "                            verb_dict[word.lower()] = main_codes[code]\n",
    "                    else:\n",
    "                        word = re.split(\"\\[|\\]\", line)[0]\n",
    "                        verb_dict[word.lower()] = main_codes[code]\n",
    "            else:\n",
    "                if cur_main_code != \"--\":\n",
    "                    if \"{\" in line:         #e.g. \"HURRY {HURRIES HURRYING HURRIED }\" \n",
    "                        line_s = re.split(\"\\{|\\}\", line)    #split at { and }\n",
    "                        verb_dict[line_s[0].lower()] = main_codes[cur_main_code]\n",
    "                        for word in line_s[1].split():\n",
    "                            verb_dict[word.lower()] = main_codes[cur_main_code]\n",
    "                    else:                   #only single words with sometimes comments, e.g.: CENSURE  # JON 5/17/95\n",
    "                        word = line.split(\"#\")[0].rstrip()    #gets part before \"#\", removes all whitespaces to the right\n",
    "                        verb_dict[word.lower()] = main_codes[cur_main_code]\n",
    "\n",
    "    #read multi word patterns and create a dictionary for their code\n",
    "\n",
    "    #get filler words that occur in multi word patterns\n",
    "    verb_file = open(verb_path, 'r')\n",
    "    verb_lines = verb_file.readlines()\n",
    "\n",
    "    pre_dict = {}\n",
    "    filter_list = []\n",
    "    for line in verb_lines:\n",
    "        if line.startswith(\"&\"):\n",
    "            cur_filter = line.rstrip()\n",
    "        elif line.startswith(\"\\n\") and \"cur_filter\" in locals():\n",
    "            pre_dict[cur_filter.lower()] = filter_list\n",
    "            cur_filter = \"\"\n",
    "            filter_list = []\n",
    "        elif line.startswith(\"+\") and cur_filter != \"\":\n",
    "            filter_list.append(line.rstrip()[1:].replace(\"_\", \"\").lower())\n",
    "    del pre_dict[\"\"]\n",
    "\n",
    "    #generate dictionaries for multi word patterns\n",
    "    verb_file = open(verb_path, 'r')\n",
    "    verb_lines = verb_file.readlines()\n",
    "\n",
    "    spec_dict = {}\n",
    "    spec_code = {}\n",
    "\n",
    "    count = 0\n",
    "    for line in verb_lines:\n",
    "        if line.startswith(\"- \"):\n",
    "            #get main verb as dict key\n",
    "            try: \n",
    "                verb_match = re.search(\"# *\\w+\", line).group()\n",
    "                verb_match = re.search(\"\\w+\", verb_match).group()\n",
    "                verb_match = verb_match.replace(\"_\", \" \").lower()\n",
    "            except: \n",
    "                count += 1\n",
    "\n",
    "            #get code for line\n",
    "            try:\n",
    "                code = re.search(\"\\[.*]\", line).group()[1:3]\n",
    "                if code != \"--\":\n",
    "                    #get all possibility that the line indicates\n",
    "                    poss = gen_poss(line, verb_match, pre_dict)\n",
    "                    for pattern in poss:\n",
    "                        spec_code[pattern] = main_codes[code]\n",
    "                    spec_dict[verb_match] = poss\n",
    "            except:\n",
    "                count += 1\n",
    "\n",
    "    print(f\"{count} patterns could not be loaded\")        \n",
    "\n",
    "    return verb_dict, spec_dict, spec_code\n",
    "\n",
    "\n",
    "def get_triples(sentence, verb_dict, spec_dict, spec_code, nlp):\n",
    "    \"\"\"create triplet structure for training from text input, \n",
    "    verb_dict needs to be loaded before,\n",
    "    spacy model needs to be initialized before \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    verbs = []\n",
    "    dict = {}\n",
    "\n",
    "\n",
    "    for possible_verb in doc:\n",
    "        if possible_verb.pos == VERB:\n",
    "            if neg in [child.dep for child in possible_verb.children]: continue\n",
    "            else: \n",
    "                for possible_subject in possible_verb.children: \n",
    "                    if possible_subject.dep == xcomp or possible_subject.dep == ccomp: \n",
    "                        main_verb = possible_subject\n",
    "                        main_idx = possible_subject.idx\n",
    "                        for token in doc.ents:\n",
    "                            if token.label_ in [\"GPE\", \"NORP\", \"EVENTS\", \"FAC\", \"LAW\", \"ORG\", \"PERSON\"]:\n",
    "                                if token.root.dep_ == \"poss\" or token.root.dep_ == \"prep\":\n",
    "                                    if token.root.head.head.idx == possible_verb.idx:\n",
    "                                        verbs.append([main_idx, main_verb.lemma_, token.text, token.root.head.dep_])\n",
    "                                        if main_idx in dict.keys(): dict[main_idx] += 1\n",
    "                                        else: dict[main_idx] = 1\n",
    "                                else:\n",
    "                                    if token.root.head.idx == possible_verb.idx:\n",
    "                                        verbs.append([main_idx, main_verb.lemma_, token.text, token.root.dep_])\n",
    "                                        if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "                                        else: dict[possible_verb.idx] = 1\n",
    "\n",
    "                for token in doc.ents:\n",
    "                    if token.label_ in [\"GPE\", \"NORP\", \"EVENTS\", \"FAC\", \"LAW\", \"ORG\", \"PERSON\"]:\n",
    "                        if token.root.dep_ == \"poss\" or token.root.dep_ == \"prep\":\n",
    "                            if token.root.head.head.idx == possible_verb.idx:\n",
    "                                verbs.append([possible_verb.idx, possible_verb.lemma_, token.text, token.root.head.dep_])\n",
    "                                if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "                                else: dict[possible_verb.idx] = 1\n",
    "                        else:\n",
    "                            if token.root.head.idx == possible_verb.idx:\n",
    "                                verbs.append([possible_verb.idx, possible_verb.lemma_, token.text, token.root.dep_])\n",
    "                                if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "                                else: dict[possible_verb.idx] = 1\n",
    "\n",
    "    trip_idx = [key for key in dict if dict[key] > 1]\n",
    "\n",
    "    # doc = nlp(sentence)\n",
    "    # verbs = []\n",
    "    # dict = {}\n",
    "\n",
    "    # for possible_verb in doc:           #parses through all words in sentence\n",
    "    #     if possible_verb.pos == VERB:   #we only care about verbs\n",
    "    #         if neg in [child.dep for child in possible_verb.children]: continue #we exclude all negated verbs\n",
    "    #         else: \n",
    "    #             for candidate in possible_verb.children: #for composed verbs of verb (e.g. \"want to join\" -> \"want join\")\n",
    "    #                 if candidate.dep == xcomp:   #subj / obj of composed verb should also be subj / obj of main verb\n",
    "    #                     main_verb = candidate    \n",
    "    #                     main_idx = candidate.idx\n",
    "    #                     for chunk in doc.noun_chunks:   #chunks are noun-groups (e.g.: \"78 out of 100 people\" instead of \"people\")\n",
    "    #                         if chunk.root.head.idx == possible_verb.idx:    #if chunk applies to xcomp (want),\n",
    "    #                                                                         #treat it like it aplles to main verb (\"join\")\n",
    "    #                             verbs.append([main_idx, main_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "    #                             if main_idx in dict.keys(): dict[main_idx] += 1 #count how often verb is used\n",
    "    #                             else: dict[main_idx] = 1\n",
    "\n",
    "    #             for chunk in doc.noun_chunks:       #for normal verbs, check chunks directly\n",
    "    #                 if chunk.root.head.idx == possible_verb.idx:\n",
    "    #                     verbs.append([possible_verb.idx, possible_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "    #                     if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "    #                     else: dict[possible_verb.idx] = 1\n",
    "    \n",
    "    # trip_idx = [key for key in dict if dict[key] > 1]   #if verbs used more than once, its candidate for triplet\n",
    "\n",
    "    #priority for subj-relation-obj triplets\n",
    "    mapper = {\"nsubj\":1,\"dobj\":2, \"pobj\":2, \"iobj\":2}\n",
    "\n",
    "    #create df from verbs extracted \n",
    "    df = pd.DataFrame(verbs, columns = [\"idx\", \"verb\", \"noun\", \"noun_type\"])\n",
    "    df[\"noun_map\"] = df.noun_type.map(mapper)  #turn noun_types into priority \n",
    "    return df\n",
    "\n",
    "    # #create groups that resolve around same word\n",
    "    # gb = df.groupby('idx')    \n",
    "    # #only keep groups if verb idx was identified as potential triplet before, sort by priority for structure\n",
    "    # df_l = [gb.get_group(x).sort_values(\"noun_map\") for x in gb.groups if gb.get_group(x).idx.iloc[0] in dict]\n",
    "    # matches = [merge_trip(group) for group in df_l if not merge_trip(group) == None] #get groups into triplet structure\n",
    "    \n",
    "    # #turn matches into triples by only keeping those with coded verbs, return code instead of verb\n",
    "    # triples = []\n",
    "    # for match in matches:\n",
    "    #     if match[1].lower() in spec_dict:\n",
    "    #         for poss_pattern in spec_dict[match[1].lower()]:\n",
    "    #             if set(poss_pattern.split()).intersection(sentence.split()) == set(poss_pattern.split()):\n",
    "    #                 triples.append(f\"<triplet> {match[0]} <subj> {match[2]} <obj> {spec_code[poss_pattern]}\")\n",
    "                    \n",
    "    #     elif match[1].lower() in verb_dict:\n",
    "    #         triples.append(f\"<triplet> {match[0]} <subj> {match[2]} <obj> {verb_dict[match[1].lower()]}\")\n",
    "    #     else: print(f\"couldn't match {match[1].lower()}\")\n",
    "\n",
    "    # #triples = [f\"<triplet> {match[0]} <subj> {match[2]} <obj> {verb_dict[match[1].lower()]}\" for match in matches if match[1].lower() in verb_dict]\n",
    "\n",
    "    # return triples\n",
    "\n",
    "def merge_trip(df):\n",
    "    \"\"\"helper function to turn two rows of a pandas groupby into subj, verb, obj\"\"\"\n",
    "    if df.shape[0] == 2:\n",
    "        if df.noun_type.iloc[0] != df.noun_type.iloc[1]:\n",
    "            return [df.iloc[0].noun, df.iloc[0].verb, df.iloc[1].noun]\n",
    "    elif df.shape[0] > 2:\n",
    "        for i in range(df.shape[0] - 1):\n",
    "            if df.noun_type.iloc[i] != df.noun_type.iloc[i+1]:\n",
    "                return [df.iloc[0].noun, df.iloc[0].verb, df.iloc[1].noun]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "pico_path = r\"C:/Users/svawe/Thesis_RelationExtraction_PoliticsNews/soft_data/src/add_labels/dictionaries/PETR.Internal.Coding.Ontology.txt\"\n",
    "verb_path = r\"C:/Users/svawe/Thesis_RelationExtraction_PoliticsNews/soft_data/src/add_labels/dictionaries/newdict.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "read = read_lines(\"data/out_data/articles_url_coref2.csv.xml.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couldn't finde code in:  --- DEFEND  ###\n",
      "couldn't finde code in:  --- REVOKE_   ###\n",
      "couldn't finde code in:  --- SEND   ###\n",
      "couldn't finde code in:  --- COLLAPSE  ###\n",
      "22 patterns could not be loaded\n"
     ]
    }
   ],
   "source": [
    "verb_dict, spec_dict, spec_code = verb_code_dict(pico_path, verb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_2 = [[sentence, get_triples(sentence, verb_dict, spec_dict, spec_code, nlp)] for sentence in read]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21395"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfs_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['AdvertisingRead moreThis live page is no longer being updated.',\n",
       "  Empty DataFrame\n",
       "  Columns: [idx, verb, noun, noun_type, noun_map]\n",
       "  Index: []],\n",
       " ['For more on\\xa0our coverage of the war in Ukraine, click here.11:44pm:\\xa0UN grain coordinator\\xa0expects loaded ships to depart Ukraine on ThursdayThe UN coordinator for the Ukraine Black Sea grain deal said UN grain coordinator expects loaded ships to depart Ukrainian ports on ThursdayThe.',\n",
       "     idx    verb     noun noun_type  noun_map\n",
       "  0  113  depart  Ukraine      dobj         2],\n",
       " ['“Exports of grain and foodstuffs from Ukraine need to continue.',\n",
       "  Empty DataFrame\n",
       "  Columns: [idx, verb, noun, noun_type, noun_map]\n",
       "  Index: []],\n",
       " ['Although no movements of vessels are planned for 2 November under the #BlackSeaGrainInitiative, we expect loaded ships to sail on ThursdayThe,” UN coordinator Amir Abdulla posted on Twitter.',\n",
       "     idx  verb          noun noun_type  noun_map\n",
       "  0  172  post  Amir Abdulla     nsubj         1],\n",
       " ['Exports of grain and foodstuffs from Ukraine need to continue.',\n",
       "  Empty DataFrame\n",
       "  Columns: [idx, verb, noun, noun_type, noun_map]\n",
       "  Index: []]]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The UN Secretariat at coordination centreThere reports that the Ukrainian, Turkish and UN delegations agreed not to plan any movement of vessels in the Black Sea Grain Initiative for 2 November,\" \"The UN Secretariat at the Joint Coordination Centre said Tuesday, referring to the July deal brokered by Turkey and the UN.6:15pm:\\xa0Russian President Vladimir Putin tells Erdogan Russian President Vladimir Putin wants \\'real guarantees\\' from Kyiv on grain deal, says Russian President Vladimir Putin told Erdogan Tuesday that Russian President Vladimir Putin wanted \"real guarantees\" from Kyiv before Kyiv potentially rejoined grain deal.',\n",
       "    idx    verb                noun noun_type  noun_map\n",
       " 0   47  report  The UN Secretariat     nsubj         1\n",
       " 1  361    tell      Vladimir Putin     nsubj         1\n",
       " 2  408    want      Vladimir Putin     nsubj         1\n",
       " 3  495    tell      Vladimir Putin     nsubj         1\n",
       " 4  495    tell             Erdogan      dobj         2\n",
       " 5  554    want      Vladimir Putin     nsubj         1\n",
       " 6  613  rejoin                Kyiv     nsubj         1]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_2[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidats_2 = [df for df in dfs_2 if df[1].shape[0] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8364"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidats_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     102\n",
       "1      47\n",
       "2     408\n",
       "3     361\n",
       "4     408\n",
       "5     554\n",
       "6     554\n",
       "7     495\n",
       "8     495\n",
       "9     554\n",
       "10    613\n",
       "Name: idx, dtype: int64"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_2[13][1][\"idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_2 = []\n",
    "for can in candidats_2:\n",
    "    for idx in can[1][\"idx\"]:\n",
    "        if can[1][\"idx\"].to_list().count(idx) > 1:\n",
    "            trips_2.append([can[0], can[1]])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3510"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trips_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_2 = []\n",
    "for idx, df in enumerate(trips_2):\n",
    "    #create groups that resolve around same word\n",
    "    gb = df[1].groupby('idx')  \n",
    "    #only keep groups if verb idx was identified as potential triplet before, sort by priority for structure\n",
    "    for x in gb.groups:\n",
    "        group = gb.get_group(x).sort_values(\"noun_map\")\n",
    "        if group.shape[0] == 2:\n",
    "            if group.noun_type.iloc[0] != group.noun_type.iloc[1]:\n",
    "                matches_2.append([df[0], group.iloc[0].noun, group.iloc[0].verb, group.iloc[1].noun])\n",
    "        elif group.shape[0] > 2:\n",
    "            for i in range(group.shape[0] - 1):\n",
    "                if group.noun_type.iloc[i] != group.noun_type.iloc[i+1]:\n",
    "                    matches_2.append([df[0], group.iloc[0].noun, group.iloc[0].verb, group.iloc[1].noun])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples_2 = []\n",
    "ma_df = pd.DataFrame(matches_2, columns = [\"text\",\"subj\", \"verb\",\"obj\"])\n",
    "for row in ma_df.iterrows():\n",
    "    if row[1][\"verb\"] in spec_dict:\n",
    "        for poss_pattern in spec_dict[row[1][\"verb\"]]:\n",
    "            if set(poss_pattern.split()).intersection(row[1][\"text\"].split()) == set(poss_pattern.split()):\n",
    "                triples_2.append([row[1][\"text\"], row[1]['subj'], row[1]['obj'] , spec_code[poss_pattern]])\n",
    "    elif row[1][\"verb\"] in verb_dict:\n",
    "            triples_2.append([row[1][\"text\"], row[1]['subj'], row[1]['obj'], verb_dict[row[1]['verb']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(triples_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "huh2 = pd.DataFrame(triples_2, columns = [\"text\", \"subj\", \"obj\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\svawe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:2339: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "probs_l = []\n",
    "for row in huh2.iterrows():\n",
    "    premise = row[1][\"text\"]\n",
    "    subj = row[1][\"subj\"]\n",
    "    rel = row[1][\"label\"]\n",
    "    obj =  row[1][\"obj\"]\n",
    "\n",
    "    hypothesis = f'{subj} does {rel} towards {obj}.'\n",
    "\n",
    "    # run through model pre-trained on MNLI\n",
    "    x = tokenizer.encode(premise, hypothesis, return_tensors='pt',\n",
    "                        truncation_strategy='only_first')\n",
    "    logits = nli_model(x)[0]\n",
    "\n",
    "    # we throw away \"neutral\" (dim 1) and take the probability of\n",
    "    # \"entailment\" (2) as the probability of the label being true \n",
    "    entail_contradiction_logits = logits[:,[0,2]]\n",
    "    probs = entail_contradiction_logits.softmax(dim=1)\n",
    "    prob_label_is_true = probs[:,1]\n",
    "\n",
    "    probs_l.append([row[0], prob_label_is_true.item()])\n",
    "    if row[0] % 100 == 0: print(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent = pd.merge(huh2.reset_index(), pd.DataFrame(probs_l, columns = [\"index\", \"prob\"]), on = \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Russian President Vladimir Putin told Erdogan in a phone call that Russia sought \"real guarantees from Kyiv about the strict observance of the Istanbul agreement, in particular about not using the humanitarian corridor for military purposes\", according to a statement from the Kremlin.3:51pm: France says Russia endangering world food securityFrench President Emmanuel Macron on Tuesday accused Russia of endangering world food supplies by a unilateral decision by Russia which again harms global food security Russia\\'s participation in a landmark Ukraine grain deal.Russia on Saturday halted Russia\\'s participation in the agreement that allowed vital grain exports from Ukraine, blaming drone attacks on Russian ships in the Crimea.In a call with Ukrainian President Volodymyr Zelensky, securityFrench President Emmanuel Macron \"the announcement a unilateral decision by Russia which again harms global food security\", the president\\'s office said.Russia made the announcement after Russia\\'s army accused Kyiv of a \"massive\" drone attack on Kyiv\\'s Black Sea fleet, which Ukraine labelled a \"false pretext\" and UN urged grain deal\\'s preservation.The July deal to unlock grain exports signed between Russia and Ukraine and brokered by Turkey and UN has been seen as critical to easing the global food crisis caused by the conflict.3:18pm: securityFrench President Emmanuel Macron says France will help Ukraine get through winter, fix securityFrench President Emmanuel Macron, following a telephone call with Ukrainian President Volodymyr Zelensky, said France would help Ukraine get through winter and help repair water and energy infrastructure damaged by Russian strikes.France will also help boost Ukraine\\'s anti-air defences and securityFrench President Emmanuel Macron said securityFrench President Emmanuel Macron and Ukrainian President Volodymyr Zelensky had agreed to hold an international conference in Paris on December 13 to support Ukraine civilians in winter.A bilateral conference on December 12 will also aim at raising support for Ukraine from French companies, securityFrench President Emmanuel Macron said in a statement.3:13pm: securityFrench President Emmanuel Macron, Ukrainian President Volodymyr Zelensky discuss Ukraine\\'s defence needs, restoring energy ZelenskyUkraine\\'s President Volodymyr Zelensky said on Tuesday ZelenskyUkraine\\'s President Volodymyr Zelensky had an \"extremely important and productive conversation\" with securityFrench President Emmanuel Macron about strengthening Ukraine\\'s defence capabilities and restoring damaged\\xa0energy infrastructure.ZelenskyUkraine\\'s President Volodymyr Zelensky did not say what had been agreed but thanked securityFrench President Emmanuel Macron for securityFrench President Emmanuel Macron\\'s support since Russia invaded Ukraine.'"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent.text.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                                    1\n",
       "text     10:23am: Three more ships leave Ukraine ports ...\n",
       "subj                                               Finland\n",
       "obj                                                 Turkey\n",
       "label                                           Disapprove\n",
       "prob                                              0.097846\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>subj</th>\n",
       "      <th>obj</th>\n",
       "      <th>label</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\"Those who intend to confront and subvert the ...</td>\n",
       "      <td>the Associated Press</td>\n",
       "      <td>Iran</td>\n",
       "      <td>Make Public Statement</td>\n",
       "      <td>0.599615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>China have suggested to the Saudis that China ...</td>\n",
       "      <td>China</td>\n",
       "      <td>China</td>\n",
       "      <td>Engage In Material Cooperation</td>\n",
       "      <td>0.666303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>Maxim Dlugy said, \"Well, world champion Magnus...</td>\n",
       "      <td>Magnus Carlsen</td>\n",
       "      <td>Maxim Dlugy</td>\n",
       "      <td>Engage In Diplomatic Cooperation</td>\n",
       "      <td>0.829087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>Can China Bring Vladimir Putin's To Vladimir P...</td>\n",
       "      <td>China</td>\n",
       "      <td>Vladimir Putin's</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.598665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>Can China Bring Vladimir Putin's To Vladimir P...</td>\n",
       "      <td>China</td>\n",
       "      <td>Vladimir Putin's</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.598665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>266</td>\n",
       "      <td>In June, Alexander Nikulin - who worked at the...</td>\n",
       "      <td>Alexander Nikulin</td>\n",
       "      <td>Britons</td>\n",
       "      <td>Coerce</td>\n",
       "      <td>0.838175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>268</td>\n",
       "      <td>(Fernando Vergara/Associated Press)Earlier thi...</td>\n",
       "      <td>Nicolas Maduro</td>\n",
       "      <td>Gustavo Petro</td>\n",
       "      <td>Disapprove</td>\n",
       "      <td>0.587277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>269</td>\n",
       "      <td>In June, Alexander Nikulin - who worked at the...</td>\n",
       "      <td>Alexander Nikulin</td>\n",
       "      <td>Britons</td>\n",
       "      <td>Coerce</td>\n",
       "      <td>0.838175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>270</td>\n",
       "      <td>'Jordan, AdvertisingRead moreJordan Bardella, ...</td>\n",
       "      <td>moreJordan Bardella</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>Make Public Statement</td>\n",
       "      <td>0.524766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>273</td>\n",
       "      <td>Donald Trump's victory in Pennsylvania helped ...</td>\n",
       "      <td>Donald Trump's</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Provide Aid</td>\n",
       "      <td>0.836310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                               text  \\\n",
       "2        2  \"Those who intend to confront and subvert the ...   \n",
       "10      10  China have suggested to the Saudis that China ...   \n",
       "17      17  Maxim Dlugy said, \"Well, world champion Magnus...   \n",
       "23      23  Can China Bring Vladimir Putin's To Vladimir P...   \n",
       "24      24  Can China Bring Vladimir Putin's To Vladimir P...   \n",
       "..     ...                                                ...   \n",
       "266    266  In June, Alexander Nikulin - who worked at the...   \n",
       "268    268  (Fernando Vergara/Associated Press)Earlier thi...   \n",
       "269    269  In June, Alexander Nikulin - who worked at the...   \n",
       "270    270  'Jordan, AdvertisingRead moreJordan Bardella, ...   \n",
       "273    273  Donald Trump's victory in Pennsylvania helped ...   \n",
       "\n",
       "                     subj               obj                             label  \\\n",
       "2    the Associated Press              Iran             Make Public Statement   \n",
       "10                  China             China    Engage In Material Cooperation   \n",
       "17         Magnus Carlsen       Maxim Dlugy  Engage In Diplomatic Cooperation   \n",
       "23                  China  Vladimir Putin's                           Consult   \n",
       "24                  China  Vladimir Putin's                           Consult   \n",
       "..                    ...               ...                               ...   \n",
       "266     Alexander Nikulin           Britons                            Coerce   \n",
       "268        Nicolas Maduro     Gustavo Petro                        Disapprove   \n",
       "269     Alexander Nikulin           Britons                            Coerce   \n",
       "270   moreJordan Bardella            Jordan             Make Public Statement   \n",
       "273        Donald Trump's      Donald Trump                       Provide Aid   \n",
       "\n",
       "         prob  \n",
       "2    0.599615  \n",
       "10   0.666303  \n",
       "17   0.829087  \n",
       "23   0.598665  \n",
       "24   0.598665  \n",
       "..        ...  \n",
       "266  0.838175  \n",
       "268  0.587277  \n",
       "269  0.838175  \n",
       "270  0.524766  \n",
       "273  0.836310  \n",
       "\n",
       "[93 rows x 6 columns]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent[ent.prob > 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrial with chunks instead of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import xml.etree.ElementTree as ET\n",
    "from spacy.symbols import nsubj, dobj, pobj, iobj, neg, xcomp, VERB\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def read_lines(inputparsed):    \n",
    "    \"\"\"takes input from CoreNLP sentence parsed file and returns sentences\"\"\"\n",
    "    #parse all lines from CoreNLP sentence split\n",
    "    parsed = open(inputparsed, encoding = \"utf-8\")\n",
    "    parsedfile = parsed.readlines()\n",
    "    parsedlines = []\n",
    "\n",
    "    #Only keep those lines which have Sentence #n in the line before\n",
    "    for idx, text in enumerate(parsedfile):\n",
    "        if text.startswith(\"Sentence #\"):\n",
    "            parsedlines.append(parsedfile[idx+1].replace('\\n','').strip())\n",
    "    \n",
    "    return parsedlines\n",
    "\n",
    "def gen_poss(line, verb_match, pre_dict):\n",
    "    \"\"\"generates all possibilities of patterns that a multi word line implies,\n",
    "    by extracting partial patterns and resolving placeholder words\"\"\"\n",
    "    poss = []\n",
    "\n",
    "    #replace special tokens in text that are clear at this point\n",
    "    line = line.replace(\"*\", verb_match)\n",
    "    line = line.replace(\"- \", \"\")\n",
    "    line = line.replace(\"+\",\"\")\n",
    "    line = line.replace(\"%\",\"\")\n",
    "    line = line.replace(\"^\",\"\")\n",
    "    line = line.replace(\"$\",\"\")\n",
    "\n",
    "    #split line by possibility indicators and code (always ends possibility)\n",
    "    #example.: \"- $ * (P ON KILLING (P OF + [010] #  COMMENT <ELH 07 May 2008>\"\n",
    "    poss_split = re.split(\"\\(P |\\[.*]\",line) \n",
    "\n",
    "    if len(poss_split) > 2: #2 is if no (P in the line\n",
    "        #only combining the first (P, as they share the same code \n",
    "        #and the longer version will never be contained in a text if the shorter isnt\n",
    "        poss.append(strip_multiple_whitespaces(\" \".join(poss_split[:2])).lower().rstrip().lstrip())\n",
    "    else: \n",
    "        poss.append(strip_multiple_whitespaces(poss_split[0].lower().rstrip().lstrip()))\n",
    "\n",
    "    cleaned = []\n",
    "    for text in list(set(poss)):\n",
    "        c = 0\n",
    "        for tag in list(pre_dict.keys()):\n",
    "            if tag in text:\n",
    "                for replacement in pre_dict[tag]:\n",
    "                    cleaned.append(text.replace(tag, replacement))\n",
    "                    c += 1\n",
    "        if c == 0:\n",
    "            cleaned.append(text)\n",
    "\n",
    "    return cleaned\n",
    "    \n",
    "\n",
    "def verb_code_dict(pico_path, verb_path):\n",
    "    \"\"\"reads coding ontology and verb lists, \n",
    "    directly matches verbs to their CAMEO codes and returns this verbs:codes dictionairy.\n",
    "    verb with codes that cannot be read are printed out as full line of the file\"\"\"\n",
    "    #read PETRARCH Internal Coding Ontology (= pico)\n",
    "    pico_path = os.path.join(os.getcwd(), pico_path)\n",
    "    pico_file = open(pico_path, 'r')\n",
    "    pico_lines = pico_file.readlines()\n",
    "\n",
    "    #get all 20 codes with their respective code\n",
    "    main_codes = {}                             #we run one iteration for all the main codes, only main codes contain relation name\n",
    "    for line in pico_lines:\n",
    "        line = line.split('#')\n",
    "        if line[0] == \"\" or line[0] == \"\\n\":    #only intro comments and empty lines\n",
    "            continue\n",
    "        else: \n",
    "            code_split = line[0].split(\":\")     #splits into CAMEO code and related hex\n",
    "            if len(line) > 1 and code_split[0][2] == \"0\":      #only main categories have 0 in 3rd idx, [cat_num 0] -> [010]\n",
    "                main_codes[code_split[0][:2]] = line[-1].replace(\"\\n\",\"\")\n",
    "    \n",
    "    #map code to code we want to use in the training\n",
    "    map_codes = {\"DiplomaticCoop\" : \"Engage In Diplomatic Cooperation\", \n",
    "                \"MaterialCoop\" : \"Engage In Material Cooperation\",\n",
    "                \"ProvideAid\" : \"Provide Aid\",\n",
    "                \"Exhibit Force Posture\": \"Exhibit Military Posture\",\n",
    "                \"Use Unconventional Mass Violence\" : \"Engage In Unconventional Mass Violence\"}\n",
    "    main_codes = {k: (map_codes[v] if v in map_codes else v) for k, v in main_codes.items()}\n",
    "    \n",
    "    #read single word patterns and match their code to the relation extracted in main_codes\n",
    "    verb_path = os.path.join(os.getcwd(), verb_path)\n",
    "    verb_file = open(verb_path, 'r')\n",
    "    verb_lines = verb_file.readlines()\n",
    "    \n",
    "    verb_dict = {}\n",
    "    for line in verb_lines:\n",
    "        if line[0] == \"#\":\n",
    "            continue\n",
    "        elif line.startswith(\"---\"):    #main verbs have a lead code, which is applied to all very in the section\n",
    "                                        #unless a separate code is specified for a specific verb in section\n",
    "            try: cur_main_code = re.split(\"\\[|\\]|---\", line)[2].replace(\":\",\"\")[:2]  #we only need main codes which are first two numbers\n",
    "                                                                                #sometimes code starts with \":\", e.g.: ---  OFFEND   [:110]  ---\n",
    "                                                                                #we just remove those to get the main code\n",
    "            except:                     #depending on chosen verb dictionairy, there may be main verbs without lead codes\n",
    "                print(\"couldn't finde code in: \", line.replace(\"\\n\",\"\")) \n",
    "                cur_main_code == \"--\"\n",
    "            if cur_main_code == \"\": cur_main_code = \"--\"\n",
    "        elif line == \"\\n\":              #skip empty lines\n",
    "            continue\n",
    "        elif line[0] == \"-\" or line[0] == \"~\" or line[0] == \"+\" or line[0] == \"&\": #removes all special structures we cannot use\n",
    "            continue\n",
    "        else:\n",
    "            if len(re.split(\"\\[|\\]\", line)) > 1:    #verbs with their own code, e.g.: AFFIRM [051] \n",
    "                code = re.split(\"\\[|\\]\", line)[1].replace(\":\",\"\")[:2]\n",
    "                if code != \"--\":\n",
    "                    if \"{\" in line:         #conjugated verbs, e.g. \"APPLY {APPLYING APPLIED APPLIES } [020]\"\n",
    "                        line_s = re.split(\"\\{|\\}\", line)    #split at { and }\n",
    "                        verb_dict[line_s[0].lower()] = main_codes[code] \n",
    "                        for word in line_s[1].split():\n",
    "                            verb_dict[word.lower()] = main_codes[code]\n",
    "                    else:\n",
    "                        word = re.split(\"\\[|\\]\", line)[0]\n",
    "                        verb_dict[word.lower()] = main_codes[code]\n",
    "            else:\n",
    "                if cur_main_code != \"--\":\n",
    "                    if \"{\" in line:         #e.g. \"HURRY {HURRIES HURRYING HURRIED }\" \n",
    "                        line_s = re.split(\"\\{|\\}\", line)    #split at { and }\n",
    "                        verb_dict[line_s[0].lower()] = main_codes[cur_main_code]\n",
    "                        for word in line_s[1].split():\n",
    "                            verb_dict[word.lower()] = main_codes[cur_main_code]\n",
    "                    else:                   #only single words with sometimes comments, e.g.: CENSURE  # JON 5/17/95\n",
    "                        word = line.split(\"#\")[0].rstrip()    #gets part before \"#\", removes all whitespaces to the right\n",
    "                        verb_dict[word.lower()] = main_codes[cur_main_code]\n",
    "\n",
    "    #read multi word patterns and create a dictionary for their code\n",
    "\n",
    "    #get filler words that occur in multi word patterns\n",
    "    verb_file = open(verb_path, 'r')\n",
    "    verb_lines = verb_file.readlines()\n",
    "\n",
    "    pre_dict = {}\n",
    "    filter_list = []\n",
    "    for line in verb_lines:\n",
    "        if line.startswith(\"&\"):\n",
    "            cur_filter = line.rstrip()\n",
    "        elif line.startswith(\"\\n\") and \"cur_filter\" in locals():\n",
    "            pre_dict[cur_filter.lower()] = filter_list\n",
    "            cur_filter = \"\"\n",
    "            filter_list = []\n",
    "        elif line.startswith(\"+\") and cur_filter != \"\":\n",
    "            filter_list.append(line.rstrip()[1:].replace(\"_\", \"\").lower())\n",
    "    del pre_dict[\"\"]\n",
    "\n",
    "    #generate dictionaries for multi word patterns\n",
    "    verb_file = open(verb_path, 'r')\n",
    "    verb_lines = verb_file.readlines()\n",
    "\n",
    "    spec_dict = {}\n",
    "    spec_code = {}\n",
    "\n",
    "    count = 0\n",
    "    for line in verb_lines:\n",
    "        if line.startswith(\"- \"):\n",
    "            #get main verb as dict key\n",
    "            try: \n",
    "                verb_match = re.search(\"# *\\w+\", line).group()\n",
    "                verb_match = re.search(\"\\w+\", verb_match).group()\n",
    "                verb_match = verb_match.replace(\"_\", \" \").lower()\n",
    "            except: \n",
    "                count += 1\n",
    "\n",
    "            #get code for line\n",
    "            try:\n",
    "                code = re.search(\"\\[.*]\", line).group()[1:3]\n",
    "                if code != \"--\":\n",
    "                    #get all possibility that the line indicates\n",
    "                    poss = gen_poss(line, verb_match, pre_dict)\n",
    "                    for pattern in poss:\n",
    "                        spec_code[pattern] = main_codes[code]\n",
    "                    spec_dict[verb_match] = poss\n",
    "            except:\n",
    "                count += 1\n",
    "\n",
    "    print(f\"{count} patterns could not be loaded\")        \n",
    "\n",
    "    return verb_dict, spec_dict, spec_code\n",
    "\n",
    "\n",
    "def get_triples(sentence, verb_dict, spec_dict, spec_code, nlp):\n",
    "    \"\"\"create triplet structure for training from text input, \n",
    "    verb_dict needs to be loaded before,\n",
    "    spacy model needs to be initialized before \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    verbs = []\n",
    "    dict = {}\n",
    "    for possible_verb in doc:\n",
    "        if possible_verb.pos == VERB:\n",
    "            if neg in [child.dep for child in possible_verb.children]: continue\n",
    "            else: \n",
    "                for possible_subject in possible_verb.children: \n",
    "                    if possible_subject.dep == xcomp or possible_subject.dep == ccomp:   #subj / obj of composed verb should also be subj / obj of main verb\n",
    "                        main_verb = possible_subject\n",
    "                        main_idx = possible_subject.idx\n",
    "                        \n",
    "                        for chunk in doc.noun_chunks:\n",
    "                            if chunk.root.dep_ == \"poss\" or chunk.root.dep_ == \"prep\":\n",
    "                                if chunk.root.head.head.idx == possible_verb.idx:\n",
    "                                    verbs.append([main_idx, main_verb.lemma_, chunk.text, chunk.root.head.dep_])\n",
    "\n",
    "                            else:\n",
    "                                if chunk.root.head.idx == possible_verb.idx:\n",
    "                                    verbs.append([main_idx, main_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "\n",
    "\n",
    "                for chunk in doc.noun_chunks:       #for normal verbs, check chunks directly\n",
    "                    if chunk.root.head.dep_ == \"poss\" or chunk.root.head.dep_ == \"prep\":\n",
    "                        if chunk.root.head.head.idx == possible_verb.idx:\n",
    "                            verbs.append([possible_verb.idx, possible_verb.lemma_, chunk.text, chunk.root.head.dep_])\n",
    "\n",
    "                    else:\n",
    "                        if chunk.root.head.idx == possible_verb.idx:\n",
    "                            verbs.append([possible_verb.idx, possible_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "\n",
    "    # doc = nlp(sentence)\n",
    "    # verbs = []\n",
    "    # dict = {}\n",
    "\n",
    "    # for possible_verb in doc:           #parses through all words in sentence\n",
    "    #     if possible_verb.pos == VERB:   #we only care about verbs\n",
    "    #         if neg in [child.dep for child in possible_verb.children]: continue #we exclude all negated verbs\n",
    "    #         else: \n",
    "    #             for candidate in possible_verb.children: #for composed verbs of verb (e.g. \"want to join\" -> \"want join\")\n",
    "    #                 if candidate.dep == xcomp:   #subj / obj of composed verb should also be subj / obj of main verb\n",
    "    #                     main_verb = candidate    \n",
    "    #                     main_idx = candidate.idx\n",
    "    #                     for chunk in doc.noun_chunks:   #chunks are noun-groups (e.g.: \"78 out of 100 people\" instead of \"people\")\n",
    "    #                         if chunk.root.head.idx == possible_verb.idx:    #if chunk applies to xcomp (want),\n",
    "    #                                                                         #treat it like it aplles to main verb (\"join\")\n",
    "    #                             verbs.append([main_idx, main_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "    #                             if main_idx in dict.keys(): dict[main_idx] += 1 #count how often verb is used\n",
    "    #                             else: dict[main_idx] = 1\n",
    "\n",
    "    #             for chunk in doc.noun_chunks:       #for normal verbs, check chunks directly\n",
    "    #                 if chunk.root.head.idx == possible_verb.idx:\n",
    "    #                     verbs.append([possible_verb.idx, possible_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "    #                     if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "    #                     else: dict[possible_verb.idx] = 1\n",
    "    \n",
    "    # trip_idx = [key for key in dict if dict[key] > 1]   #if verbs used more than once, its candidate for triplet\n",
    "\n",
    "    #priority for subj-relation-obj triplets\n",
    "    mapper = {\"nsubj\":1,\"dobj\":2, \"pobj\":2, \"iobj\":2}\n",
    "\n",
    "    #create df from verbs extracted \n",
    "    df = pd.DataFrame(verbs, columns = [\"idx\", \"verb\", \"noun\", \"noun_type\"])\n",
    "    df[\"noun_map\"] = df.noun_type.map(mapper)  #turn noun_types into priority \n",
    "    return df\n",
    "\n",
    "    # #create groups that resolve around same word\n",
    "    # gb = df.groupby('idx')    \n",
    "    # #only keep groups if verb idx was identified as potential triplet before, sort by priority for structure\n",
    "    # df_l = [gb.get_group(x).sort_values(\"noun_map\") for x in gb.groups if gb.get_group(x).idx.iloc[0] in dict]\n",
    "    # matches = [merge_trip(group) for group in df_l if not merge_trip(group) == None] #get groups into triplet structure\n",
    "    \n",
    "    # #turn matches into triples by only keeping those with coded verbs, return code instead of verb\n",
    "    # triples = []\n",
    "    # for match in matches:\n",
    "    #     if match[1].lower() in spec_dict:\n",
    "    #         for poss_pattern in spec_dict[match[1].lower()]:\n",
    "    #             if set(poss_pattern.split()).intersection(sentence.split()) == set(poss_pattern.split()):\n",
    "    #                 triples.append(f\"<triplet> {match[0]} <subj> {match[2]} <obj> {spec_code[poss_pattern]}\")\n",
    "                    \n",
    "    #     elif match[1].lower() in verb_dict:\n",
    "    #         triples.append(f\"<triplet> {match[0]} <subj> {match[2]} <obj> {verb_dict[match[1].lower()]}\")\n",
    "    #     else: print(f\"couldn't match {match[1].lower()}\")\n",
    "\n",
    "    # #triples = [f\"<triplet> {match[0]} <subj> {match[2]} <obj> {verb_dict[match[1].lower()]}\" for match in matches if match[1].lower() in verb_dict]\n",
    "\n",
    "    # return triples\n",
    "\n",
    "def merge_trip(df):\n",
    "    \"\"\"helper function to turn two rows of a pandas groupby into subj, verb, obj\"\"\"\n",
    "    if df.shape[0] == 2:\n",
    "        if df.noun_type.iloc[0] != df.noun_type.iloc[1]:\n",
    "            return [df.iloc[0].noun, df.iloc[0].verb, df.iloc[1].noun]\n",
    "    elif df.shape[0] > 2:\n",
    "        for i in range(df.shape[0] - 1):\n",
    "            if df.noun_type.iloc[i] != df.noun_type.iloc[i+1]:\n",
    "                return [df.iloc[0].noun, df.iloc[0].verb, df.iloc[1].noun]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [[sentence, get_triples(sentence, verb_dict, spec_dict, spec_code, nlp)] for sentence in read]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21395"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['“They continue their indiscriminate bombardment of civilians and attacks on civilian infrastructure.',\n",
       "    idx      verb                              noun noun_type  noun_map\n",
       " 0    6  continue                              They     nsubj         1\n",
       " 1    6  continue  their indiscriminate bombardment      dobj         2]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidats = [df for df in dfs if df[1].shape[0] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18552"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = []\n",
    "for can in candidats:\n",
    "    for idx in can[1][\"idx\"]:\n",
    "        if can[1][\"idx\"].to_list().count(idx) > 1:\n",
    "            trips.append([can[0], can[1]])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17157"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trips)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = []\n",
    "for idx, df in enumerate(trips):\n",
    "    #create groups that resolve around same word\n",
    "    gb = df[1].groupby('idx')    \n",
    "    #only keep groups if verb idx was identified as potential triplet before, sort by priority for structure\n",
    "    for x in gb.groups:\n",
    "        if gb.get_group(x).shape[0] == 2:\n",
    "            if gb.get_group(x).noun_type.iloc[0] != gb.get_group(x).noun_type.iloc[1]:\n",
    "                matches.append([df[0], gb.get_group(x).iloc[0].noun, gb.get_group(x).iloc[0].verb, gb.get_group(x).iloc[1].noun])\n",
    "        elif gb.get_group(x).shape[0] > 2:\n",
    "            for i in range(gb.get_group(x).shape[0] - 1):\n",
    "                if gb.get_group(x).noun_type.iloc[i] != gb.get_group(x).noun_type.iloc[i+1]:\n",
    "                    matches.append([df[0], gb.get_group(x).iloc[0].noun, gb.get_group(x).iloc[0].verb, gb.get_group(x).iloc[1].noun])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46112"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triples = []\n",
    "# ma_df = pd.DataFrame(matches, columns = [\"text\",\"subj\", \"verb\",\"obj\"])\n",
    "# for row in ma_df.iterrows():\n",
    "#     if row[1][\"verb\"] in spec_dict:\n",
    "#         for poss_pattern in spec_dict[row[1][\"verb\"]]:\n",
    "#             if set(poss_pattern.split()).intersection(row[1][\"text\"].split()) == set(poss_pattern.split()):\n",
    "#                 triples.append([row[1][\"text\"], f\"<triplet> {row[1]['subj']} <subj> {row[1]['obj']} <obj> {spec_code[poss_pattern]}\"])\n",
    "#     elif row[1][\"verb\"] in verb_dict:\n",
    "#             triples.append([row[1][\"text\"], f\"<triplet> {row[1]['subj']} <subj> {row[1]['obj']} <obj> {verb_dict[row[1]['verb']]}\"])\n",
    "\n",
    "triples = []\n",
    "ma_df = pd.DataFrame(matches, columns = [\"text\",\"subj\", \"verb\",\"obj\"])\n",
    "for row in ma_df.iterrows():\n",
    "    if row[1][\"verb\"] in spec_dict:\n",
    "        for poss_pattern in spec_dict[row[1][\"verb\"]]:\n",
    "            if set(poss_pattern.split()).intersection(row[1][\"text\"].split()) == set(poss_pattern.split()):\n",
    "                triples.append([row[1][\"text\"], row[1]['subj'], row[1]['obj'] , spec_code[poss_pattern]])\n",
    "    elif row[1][\"verb\"] in verb_dict:\n",
    "            triples.append([row[1][\"text\"], row[1]['subj'], row[1]['obj'], verb_dict[row[1]['verb']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "huh = pd.DataFrame(triples, columns = [\"text\", \"subj\", \"obj\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2117"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huh.text.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\svawe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:2339: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 45914176 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [212], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39m# run through model pre-trained on MNLI\u001b[39;00m\n\u001b[0;32m     11\u001b[0m x \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mencode(premise, hypothesis, return_tensors\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m                     truncation_strategy\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39monly_first\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 13\u001b[0m logits \u001b[39m=\u001b[39m nli_model(x)[\u001b[39m0\u001b[39m]\n\u001b[0;32m     15\u001b[0m \u001b[39m# we throw away \"neutral\" (dim 1) and take the probability of\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m# \"entailment\" (2) as the probability of the label being true \u001b[39;00m\n\u001b[0;32m     17\u001b[0m entail_contradiction_logits \u001b[39m=\u001b[39m logits[:,[\u001b[39m0\u001b[39m,\u001b[39m2\u001b[39m]]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1507\u001b[0m, in \u001b[0;36mBartForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1502\u001b[0m \u001b[39mif\u001b[39;00m input_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m inputs_embeds \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1503\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\n\u001b[0;32m   1504\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPassing input embeddings is currently not supported for \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1505\u001b[0m     )\n\u001b[1;32m-> 1507\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[0;32m   1508\u001b[0m     input_ids,\n\u001b[0;32m   1509\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1510\u001b[0m     decoder_input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[0;32m   1511\u001b[0m     decoder_attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[0;32m   1512\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1513\u001b[0m     decoder_head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[0;32m   1514\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[0;32m   1515\u001b[0m     encoder_outputs\u001b[39m=\u001b[39;49mencoder_outputs,\n\u001b[0;32m   1516\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1517\u001b[0m     decoder_inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[0;32m   1518\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1519\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1520\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1521\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1522\u001b[0m )\n\u001b[0;32m   1523\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]  \u001b[39m# last hidden state\u001b[39;00m\n\u001b[0;32m   1525\u001b[0m eos_mask \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39meq(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39meos_token_id)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1249\u001b[0m, in \u001b[0;36mBartModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1242\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1243\u001b[0m         last_hidden_state\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m0\u001b[39m],\n\u001b[0;32m   1244\u001b[0m         hidden_states\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1245\u001b[0m         attentions\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1246\u001b[0m     )\n\u001b[0;32m   1248\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1249\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[0;32m   1250\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[0;32m   1251\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[0;32m   1252\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_outputs[\u001b[39m0\u001b[39;49m],\n\u001b[0;32m   1253\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1254\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[0;32m   1255\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[0;32m   1256\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1257\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[0;32m   1258\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1259\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1260\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1261\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1262\u001b[0m )\n\u001b[0;32m   1264\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1265\u001b[0m     \u001b[39mreturn\u001b[39;00m decoder_outputs \u001b[39m+\u001b[39m encoder_outputs\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1107\u001b[0m, in \u001b[0;36mBartDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m   1096\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[0;32m   1097\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1103\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[0;32m   1105\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1107\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[0;32m   1108\u001b[0m         hidden_states,\n\u001b[0;32m   1109\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1110\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1111\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m   1112\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49m(head_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1113\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49m(\n\u001b[0;32m   1114\u001b[0m             cross_attn_head_mask[idx] \u001b[39mif\u001b[39;49;00m cross_attn_head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[0;32m   1115\u001b[0m         ),\n\u001b[0;32m   1116\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[0;32m   1117\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1118\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1119\u001b[0m     )\n\u001b[0;32m   1120\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1122\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\bart\\modeling_bart.py:439\u001b[0m, in \u001b[0;36mBartDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[39m# cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\u001b[39;00m\n\u001b[0;32m    438\u001b[0m cross_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 439\u001b[0m hidden_states, cross_attn_weights, cross_attn_present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder_attn(\n\u001b[0;32m    440\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[0;32m    441\u001b[0m     key_value_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m    442\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[0;32m    443\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[0;32m    444\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mcross_attn_past_key_value,\n\u001b[0;32m    445\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m    446\u001b[0m )\n\u001b[0;32m    447\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[0;32m    448\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\models\\bart\\modeling_bart.py:230\u001b[0m, in \u001b[0;36mBartAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[0;32m    227\u001b[0m value_states \u001b[39m=\u001b[39m value_states\u001b[39m.\u001b[39mview(\u001b[39m*\u001b[39mproj_shape)\n\u001b[0;32m    229\u001b[0m src_len \u001b[39m=\u001b[39m key_states\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)\n\u001b[1;32m--> 230\u001b[0m attn_weights \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbmm(query_states, key_states\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m))\n\u001b[0;32m    232\u001b[0m \u001b[39mif\u001b[39;00m attn_weights\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m (bsz \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, tgt_len, src_len):\n\u001b[0;32m    233\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    234\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAttention weights should be of size \u001b[39m\u001b[39m{\u001b[39;00m(bsz \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, tgt_len, src_len)\u001b[39m}\u001b[39;00m\u001b[39m, but is\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    235\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mattn_weights\u001b[39m.\u001b[39msize()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    236\u001b[0m     )\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at ..\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 45914176 bytes."
     ]
    }
   ],
   "source": [
    "probs_l = []\n",
    "for row in huh.iterrows():\n",
    "    premise = row[1][\"text\"]\n",
    "    subj = row[1][\"subj\"]\n",
    "    rel = row[1][\"label\"]\n",
    "    obj =  row[1][\"obj\"]\n",
    "\n",
    "    hypothesis = f'{subj} does {rel} towards {obj}.'\n",
    "\n",
    "    # run through model pre-trained on MNLI\n",
    "    x = tokenizer.encode(premise, hypothesis, return_tensors='pt',\n",
    "                        truncation_strategy='only_first')\n",
    "    logits = nli_model(x)[0]\n",
    "\n",
    "    # we throw away \"neutral\" (dim 1) and take the probability of\n",
    "    # \"entailment\" (2) as the probability of the label being true \n",
    "    entail_contradiction_logits = logits[:,[0,2]]\n",
    "    probs = entail_contradiction_logits.softmax(dim=1)\n",
    "    prob_label_is_true = probs[:,1]\n",
    "\n",
    "    probs_l.append([row[0], prob_label_is_true.item()])\n",
    "    if row[0] % 100 == 0: print(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "huh = pd.merge(huh.reset_index(), pd.DataFrame(probs_l, columns = [\"index\", \"prob\"]), on = \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dec = huh[huh.prob < 0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep = huh[huh.prob > 0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>subj</th>\n",
       "      <th>obj</th>\n",
       "      <th>label</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>For more on our coverage of the war in Ukraine...</td>\n",
       "      <td>loaded ships</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.937298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"Russia must clearly understand that Russia wi...</td>\n",
       "      <td>70,000 people</td>\n",
       "      <td>the Kherson region</td>\n",
       "      <td>Yield</td>\n",
       "      <td>0.810604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>12:04pm Kremlin accuses UK of ‘directing and c...</td>\n",
       "      <td>coordinating</td>\n",
       "      <td>Nord Stream blastsThe Kremlin</td>\n",
       "      <td>Engage In Material Cooperation</td>\n",
       "      <td>0.898151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Swedish, Finnish NATO bidsFinland's Prime Mini...</td>\n",
       "      <td>Finland</td>\n",
       "      <td>the NATO defence alliance</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.716120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Swedish, Finnish NATO bidsFinland's Prime Mini...</td>\n",
       "      <td>Finland</td>\n",
       "      <td>the NATO defence alliance</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.716120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>a powerful new player, has surged in opinion p...</td>\n",
       "      <td>a vote</td>\n",
       "      <td>a \"fully right-wing government</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.846488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>former leader Benjamin Netanyahu's most likely...</td>\n",
       "      <td>some</td>\n",
       "      <td>reforms</td>\n",
       "      <td>Engage In Material Cooperation</td>\n",
       "      <td>0.927108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>voters from Israel's Palestinian minority, who...</td>\n",
       "      <td>the votes</td>\n",
       "      <td>-</td>\n",
       "      <td>Make Public Statement</td>\n",
       "      <td>0.713901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>Monday's cargoes constituted a record daily vo...</td>\n",
       "      <td>Monday's cargoes</td>\n",
       "      <td>a record daily volume</td>\n",
       "      <td>Engage In Material Cooperation</td>\n",
       "      <td>0.846379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>\" Several polls by Israel's's main TV channels...</td>\n",
       "      <td>which</td>\n",
       "      <td>ultra-Orthodox Jewish parties</td>\n",
       "      <td>Yield</td>\n",
       "      <td>0.921763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>\"Those who intend to confront and subvert the ...</td>\n",
       "      <td>the Associated Press</td>\n",
       "      <td>Iran's judiciary chief</td>\n",
       "      <td>Make Public Statement</td>\n",
       "      <td>0.780828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>five people included one man accused of \"corru...</td>\n",
       "      <td>five people</td>\n",
       "      <td>one man</td>\n",
       "      <td>Yield</td>\n",
       "      <td>0.898522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>There were reports that officers beat Ms Amini...</td>\n",
       "      <td>officers</td>\n",
       "      <td>Ms Amini's's head</td>\n",
       "      <td>Assault</td>\n",
       "      <td>0.852491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>Few Russian tycoons have criticised Russia's's...</td>\n",
       "      <td>Few Russian tycoons</td>\n",
       "      <td>Russia's's invasion</td>\n",
       "      <td>Disapprove</td>\n",
       "      <td>0.878540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>In April, A billionaire Russian banker, Oleg T...</td>\n",
       "      <td>A billionaire Russian banker</td>\n",
       "      <td>Kremlin</td>\n",
       "      <td>Disapprove</td>\n",
       "      <td>0.984955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>The reforms Elon Musk is contemplating include...</td>\n",
       "      <td>The reforms</td>\n",
       "      <td>changes</td>\n",
       "      <td>Yield</td>\n",
       "      <td>0.976958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>technology investor Jason Calacanis who change...</td>\n",
       "      <td>technology investor Jason Calacanis</td>\n",
       "      <td>opinions</td>\n",
       "      <td>Appeal</td>\n",
       "      <td>0.948570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>On Monday, Senator Chris Murphy, a Democrat, s...</td>\n",
       "      <td>Senator Chris Murphy</td>\n",
       "      <td>the government</td>\n",
       "      <td>Make Public Statement</td>\n",
       "      <td>0.970696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>40</td>\n",
       "      <td>New York City prosecutors accuse the Trump Org...</td>\n",
       "      <td>New York City prosecutors</td>\n",
       "      <td>the Trump Organization</td>\n",
       "      <td>Disapprove</td>\n",
       "      <td>0.769564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>48</td>\n",
       "      <td>a soldier had already been to Liudmyla Mymryko...</td>\n",
       "      <td>Liudmyla Mymrykova</td>\n",
       "      <td>soldiers</td>\n",
       "      <td>Engage In Diplomatic Cooperation</td>\n",
       "      <td>0.796336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>49</td>\n",
       "      <td>a soldier had already been to Liudmyla Mymryko...</td>\n",
       "      <td>she</td>\n",
       "      <td>them</td>\n",
       "      <td>Engage In Diplomatic Cooperation</td>\n",
       "      <td>0.734115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>54</td>\n",
       "      <td>\" his father Rajesh adds: \"the family want ans...</td>\n",
       "      <td>the family</td>\n",
       "      <td>answers</td>\n",
       "      <td>Appeal</td>\n",
       "      <td>0.850619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>55</td>\n",
       "      <td>And the family want justice.</td>\n",
       "      <td>the family</td>\n",
       "      <td>justice</td>\n",
       "      <td>Appeal</td>\n",
       "      <td>0.864699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>56</td>\n",
       "      <td>Korean actor Lee Jihan's agency 935 Entertainm...</td>\n",
       "      <td>who</td>\n",
       "      <td>everyone</td>\n",
       "      <td>Engage In Diplomatic Cooperation</td>\n",
       "      <td>0.834468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>58</td>\n",
       "      <td>The BBC is not responsible for the content of ...</td>\n",
       "      <td>View original post</td>\n",
       "      <td>The authorities</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.807112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>59</td>\n",
       "      <td>numerous factors include poor drainage systems...</td>\n",
       "      <td>numerous factors</td>\n",
       "      <td>poor drainage systems</td>\n",
       "      <td>Yield</td>\n",
       "      <td>0.911941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>61</td>\n",
       "      <td>In the years after slavery was outlawed, laws ...</td>\n",
       "      <td>that</td>\n",
       "      <td>black communities</td>\n",
       "      <td>Coerce</td>\n",
       "      <td>0.980904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>68</td>\n",
       "      <td>Use this form to ask your question:  If you ar...</td>\n",
       "      <td>you</td>\n",
       "      <td>the mobile version</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.966455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>72</td>\n",
       "      <td>China have suggested to the Saudis that China ...</td>\n",
       "      <td>the oil China</td>\n",
       "      <td>the Saudis</td>\n",
       "      <td>Engage In Material Cooperation</td>\n",
       "      <td>0.952729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>74</td>\n",
       "      <td>In addition to information about this \"swamp o...</td>\n",
       "      <td>it</td>\n",
       "      <td>instructions</td>\n",
       "      <td>Coerce</td>\n",
       "      <td>0.772457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>76</td>\n",
       "      <td>The deputy head, whom the security authorities...</td>\n",
       "      <td>whom</td>\n",
       "      <td>the security authorities</td>\n",
       "      <td>Disapprove</td>\n",
       "      <td>0.724027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>83</td>\n",
       "      <td>a revolt that has pushed the Islamist regime a...</td>\n",
       "      <td>young women</td>\n",
       "      <td>young women's hijabs</td>\n",
       "      <td>Assault</td>\n",
       "      <td>0.829506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>92</td>\n",
       "      <td>In early November, Olaf Scholz will be traveli...</td>\n",
       "      <td>Olaf Scholz</td>\n",
       "      <td>a significant delegation</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.985579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>93</td>\n",
       "      <td>Such pronouncements, of course, serve a clear ...</td>\n",
       "      <td>Such pronouncements</td>\n",
       "      <td>a clear propagandistic purpose</td>\n",
       "      <td>Engage In Material Cooperation</td>\n",
       "      <td>0.865925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>an aspiring young politician propagates a reju...</td>\n",
       "      <td>an aspiring young politician</td>\n",
       "      <td>a rejuvenated country</td>\n",
       "      <td>Make Public Statement</td>\n",
       "      <td>0.786802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>100</td>\n",
       "      <td>In order to force Canada to back down in a dis...</td>\n",
       "      <td>an aspiring young politician's quiver</td>\n",
       "      <td>less belligerent methods</td>\n",
       "      <td>Yield</td>\n",
       "      <td>0.969811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>108</td>\n",
       "      <td>Will Meloni, Salvini and Silvio Berlusconi ass...</td>\n",
       "      <td>Will Meloni</td>\n",
       "      <td>a relatively normal conservative government</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.919096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>110</td>\n",
       "      <td>Only later do we learn the name of A young wom...</td>\n",
       "      <td>we</td>\n",
       "      <td>the name</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.922548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>115</td>\n",
       "      <td>The motto of a demonstration in Berlin was \"Ou...</td>\n",
       "      <td>the radical right-wing Alternative</td>\n",
       "      <td>over 100,000 euros</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.953659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>116</td>\n",
       "      <td>the radical right-wing Alternative for Germany...</td>\n",
       "      <td>the radical right-wing Alternative</td>\n",
       "      <td>close Björn Höcke allies</td>\n",
       "      <td>Yield</td>\n",
       "      <td>0.723702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>122</td>\n",
       "      <td>We got to talking, and Maxim Dlugy watched his...</td>\n",
       "      <td>Maxim Dlugy</td>\n",
       "      <td>his opponent</td>\n",
       "      <td>Make Public Statement</td>\n",
       "      <td>0.755192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>126</td>\n",
       "      <td>his opponent Hans Niemann understood that his ...</td>\n",
       "      <td>his opponent</td>\n",
       "      <td>some really difficult endgames</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.871387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>139</td>\n",
       "      <td>\"The man told Nina Chemeris, 60, that Nina Che...</td>\n",
       "      <td>Either Nina Chemeris</td>\n",
       "      <td>Nina Chemeris</td>\n",
       "      <td>Engage In Material Cooperation</td>\n",
       "      <td>0.779084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>143</td>\n",
       "      <td>Those who continue to receive goods from Ukrai...</td>\n",
       "      <td>who</td>\n",
       "      <td>goods</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.779255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>153</td>\n",
       "      <td>There was no German tourism in London in 1941 ...</td>\n",
       "      <td>who</td>\n",
       "      <td>a declaration</td>\n",
       "      <td>Engage In Diplomatic Cooperation</td>\n",
       "      <td>0.897743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>162</td>\n",
       "      <td>One Russian reportedly set One Russian on fire...</td>\n",
       "      <td>One Russian</td>\n",
       "      <td>One Russian</td>\n",
       "      <td>Assault</td>\n",
       "      <td>0.716082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>163</td>\n",
       "      <td>One Russian reportedly set One Russian on fire...</td>\n",
       "      <td>One Russian</td>\n",
       "      <td>One Russian</td>\n",
       "      <td>Assault</td>\n",
       "      <td>0.716082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>168</th>\n",
       "      <td>168</td>\n",
       "      <td>What can be learned from that when it comes to...</td>\n",
       "      <td>You</td>\n",
       "      <td>a coalition</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.904998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169</th>\n",
       "      <td>169</td>\n",
       "      <td>note: the head of the left-wing liberal party ...</td>\n",
       "      <td>the head</td>\n",
       "      <td>an alliance</td>\n",
       "      <td>Engage In Diplomatic Cooperation</td>\n",
       "      <td>0.811181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>170</td>\n",
       "      <td>former leader Benjamin Netanyahu's most likely...</td>\n",
       "      <td>some</td>\n",
       "      <td>reforms</td>\n",
       "      <td>Engage In Material Cooperation</td>\n",
       "      <td>0.934808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                               text  \\\n",
       "0        0  For more on our coverage of the war in Ukraine...   \n",
       "3        3  \"Russia must clearly understand that Russia wi...   \n",
       "5        5  12:04pm Kremlin accuses UK of ‘directing and c...   \n",
       "7        7  Swedish, Finnish NATO bidsFinland's Prime Mini...   \n",
       "8        8  Swedish, Finnish NATO bidsFinland's Prime Mini...   \n",
       "9        9  a powerful new player, has surged in opinion p...   \n",
       "10      10  former leader Benjamin Netanyahu's most likely...   \n",
       "12      12  voters from Israel's Palestinian minority, who...   \n",
       "15      15  Monday's cargoes constituted a record daily vo...   \n",
       "18      18  \" Several polls by Israel's's main TV channels...   \n",
       "21      21  \"Those who intend to confront and subvert the ...   \n",
       "22      22  five people included one man accused of \"corru...   \n",
       "23      23  There were reports that officers beat Ms Amini...   \n",
       "26      26  Few Russian tycoons have criticised Russia's's...   \n",
       "27      27  In April, A billionaire Russian banker, Oleg T...   \n",
       "30      30  The reforms Elon Musk is contemplating include...   \n",
       "31      31  technology investor Jason Calacanis who change...   \n",
       "34      34  On Monday, Senator Chris Murphy, a Democrat, s...   \n",
       "40      40  New York City prosecutors accuse the Trump Org...   \n",
       "48      48  a soldier had already been to Liudmyla Mymryko...   \n",
       "49      49  a soldier had already been to Liudmyla Mymryko...   \n",
       "54      54  \" his father Rajesh adds: \"the family want ans...   \n",
       "55      55                       And the family want justice.   \n",
       "56      56  Korean actor Lee Jihan's agency 935 Entertainm...   \n",
       "58      58  The BBC is not responsible for the content of ...   \n",
       "59      59  numerous factors include poor drainage systems...   \n",
       "61      61  In the years after slavery was outlawed, laws ...   \n",
       "68      68  Use this form to ask your question:  If you ar...   \n",
       "72      72  China have suggested to the Saudis that China ...   \n",
       "74      74  In addition to information about this \"swamp o...   \n",
       "76      76  The deputy head, whom the security authorities...   \n",
       "83      83  a revolt that has pushed the Islamist regime a...   \n",
       "92      92  In early November, Olaf Scholz will be traveli...   \n",
       "93      93  Such pronouncements, of course, serve a clear ...   \n",
       "97      97  an aspiring young politician propagates a reju...   \n",
       "100    100  In order to force Canada to back down in a dis...   \n",
       "108    108  Will Meloni, Salvini and Silvio Berlusconi ass...   \n",
       "110    110  Only later do we learn the name of A young wom...   \n",
       "115    115  The motto of a demonstration in Berlin was \"Ou...   \n",
       "116    116  the radical right-wing Alternative for Germany...   \n",
       "122    122  We got to talking, and Maxim Dlugy watched his...   \n",
       "126    126  his opponent Hans Niemann understood that his ...   \n",
       "139    139  \"The man told Nina Chemeris, 60, that Nina Che...   \n",
       "143    143  Those who continue to receive goods from Ukrai...   \n",
       "153    153  There was no German tourism in London in 1941 ...   \n",
       "162    162  One Russian reportedly set One Russian on fire...   \n",
       "163    163  One Russian reportedly set One Russian on fire...   \n",
       "168    168  What can be learned from that when it comes to...   \n",
       "169    169  note: the head of the left-wing liberal party ...   \n",
       "170    170  former leader Benjamin Netanyahu's most likely...   \n",
       "\n",
       "                                      subj  \\\n",
       "0                             loaded ships   \n",
       "3                            70,000 people   \n",
       "5                             coordinating   \n",
       "7                                  Finland   \n",
       "8                                  Finland   \n",
       "9                                   a vote   \n",
       "10                                    some   \n",
       "12                               the votes   \n",
       "15                        Monday's cargoes   \n",
       "18                                   which   \n",
       "21                    the Associated Press   \n",
       "22                             five people   \n",
       "23                                officers   \n",
       "26                     Few Russian tycoons   \n",
       "27            A billionaire Russian banker   \n",
       "30                             The reforms   \n",
       "31     technology investor Jason Calacanis   \n",
       "34                    Senator Chris Murphy   \n",
       "40               New York City prosecutors   \n",
       "48                      Liudmyla Mymrykova   \n",
       "49                                     she   \n",
       "54                              the family   \n",
       "55                              the family   \n",
       "56                                     who   \n",
       "58                      View original post   \n",
       "59                        numerous factors   \n",
       "61                                    that   \n",
       "68                                     you   \n",
       "72                           the oil China   \n",
       "74                                      it   \n",
       "76                                    whom   \n",
       "83                             young women   \n",
       "92                             Olaf Scholz   \n",
       "93                     Such pronouncements   \n",
       "97            an aspiring young politician   \n",
       "100  an aspiring young politician's quiver   \n",
       "108                            Will Meloni   \n",
       "110                                     we   \n",
       "115     the radical right-wing Alternative   \n",
       "116     the radical right-wing Alternative   \n",
       "122                            Maxim Dlugy   \n",
       "126                           his opponent   \n",
       "139                   Either Nina Chemeris   \n",
       "143                                    who   \n",
       "153                                    who   \n",
       "162                            One Russian   \n",
       "163                            One Russian   \n",
       "168                                    You   \n",
       "169                               the head   \n",
       "170                                   some   \n",
       "\n",
       "                                             obj  \\\n",
       "0                                        Ukraine   \n",
       "3                             the Kherson region   \n",
       "5                  Nord Stream blastsThe Kremlin   \n",
       "7                      the NATO defence alliance   \n",
       "8                      the NATO defence alliance   \n",
       "9                 a \"fully right-wing government   \n",
       "10                                       reforms   \n",
       "12                                             -   \n",
       "15                         a record daily volume   \n",
       "18                 ultra-Orthodox Jewish parties   \n",
       "21                        Iran's judiciary chief   \n",
       "22                                       one man   \n",
       "23                             Ms Amini's's head   \n",
       "26                           Russia's's invasion   \n",
       "27                                       Kremlin   \n",
       "30                                       changes   \n",
       "31                                      opinions   \n",
       "34                                the government   \n",
       "40                        the Trump Organization   \n",
       "48                                      soldiers   \n",
       "49                                          them   \n",
       "54                                       answers   \n",
       "55                                       justice   \n",
       "56                                      everyone   \n",
       "58                               The authorities   \n",
       "59                         poor drainage systems   \n",
       "61                             black communities   \n",
       "68                            the mobile version   \n",
       "72                                    the Saudis   \n",
       "74                                  instructions   \n",
       "76                      the security authorities   \n",
       "83                          young women's hijabs   \n",
       "92                      a significant delegation   \n",
       "93                a clear propagandistic purpose   \n",
       "97                         a rejuvenated country   \n",
       "100                     less belligerent methods   \n",
       "108  a relatively normal conservative government   \n",
       "110                                     the name   \n",
       "115                           over 100,000 euros   \n",
       "116                     close Björn Höcke allies   \n",
       "122                                 his opponent   \n",
       "126               some really difficult endgames   \n",
       "139                                Nina Chemeris   \n",
       "143                                        goods   \n",
       "153                                a declaration   \n",
       "162                                  One Russian   \n",
       "163                                  One Russian   \n",
       "168                                  a coalition   \n",
       "169                                  an alliance   \n",
       "170                                      reforms   \n",
       "\n",
       "                                label      prob  \n",
       "0                             Consult  0.937298  \n",
       "3                               Yield  0.810604  \n",
       "5      Engage In Material Cooperation  0.898151  \n",
       "7                             Consult  0.716120  \n",
       "8                             Consult  0.716120  \n",
       "9                             Consult  0.846488  \n",
       "10     Engage In Material Cooperation  0.927108  \n",
       "12              Make Public Statement  0.713901  \n",
       "15     Engage In Material Cooperation  0.846379  \n",
       "18                              Yield  0.921763  \n",
       "21              Make Public Statement  0.780828  \n",
       "22                              Yield  0.898522  \n",
       "23                            Assault  0.852491  \n",
       "26                         Disapprove  0.878540  \n",
       "27                         Disapprove  0.984955  \n",
       "30                              Yield  0.976958  \n",
       "31                             Appeal  0.948570  \n",
       "34              Make Public Statement  0.970696  \n",
       "40                         Disapprove  0.769564  \n",
       "48   Engage In Diplomatic Cooperation  0.796336  \n",
       "49   Engage In Diplomatic Cooperation  0.734115  \n",
       "54                             Appeal  0.850619  \n",
       "55                             Appeal  0.864699  \n",
       "56   Engage In Diplomatic Cooperation  0.834468  \n",
       "58                            Consult  0.807112  \n",
       "59                              Yield  0.911941  \n",
       "61                             Coerce  0.980904  \n",
       "68                            Consult  0.966455  \n",
       "72     Engage In Material Cooperation  0.952729  \n",
       "74                             Coerce  0.772457  \n",
       "76                         Disapprove  0.724027  \n",
       "83                            Assault  0.829506  \n",
       "92                            Consult  0.985579  \n",
       "93     Engage In Material Cooperation  0.865925  \n",
       "97              Make Public Statement  0.786802  \n",
       "100                             Yield  0.969811  \n",
       "108                           Consult  0.919096  \n",
       "110                           Consult  0.922548  \n",
       "115                           Consult  0.953659  \n",
       "116                             Yield  0.723702  \n",
       "122             Make Public Statement  0.755192  \n",
       "126                           Consult  0.871387  \n",
       "139    Engage In Material Cooperation  0.779084  \n",
       "143                           Consult  0.779255  \n",
       "153  Engage In Diplomatic Cooperation  0.897743  \n",
       "162                           Assault  0.716082  \n",
       "163                           Assault  0.716082  \n",
       "168                           Consult  0.904998  \n",
       "169  Engage In Diplomatic Cooperation  0.811181  \n",
       "170    Engage In Material Cooperation  0.934808  "
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"In early November, Olaf Scholz will be traveling to Beijing for the first time as German chancellor, and despite the ongoing debate about the German economy’s unsustainable dependence on China, Olaf Scholz will likely bring along a significant delegation of German executives.an aspiring young politician's Communist Party is the cornerstone of China, a vast institution with 97 million members, far more than the entire population of Germany.\""
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huh.iloc[92].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['travel to']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec_dict[\"travel\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "Appeal                                     20\n",
       "Assault                                    62\n",
       "Coerce                                     66\n",
       "Consult                                    92\n",
       "Demand                                      1\n",
       "Disapprove                                 46\n",
       "Engage In Diplomatic Cooperation          118\n",
       "Engage In Material Cooperation            113\n",
       "Engage In Unconventional Mass Violence      1\n",
       "Exhibit Military Posture                    6\n",
       "Fight                                      38\n",
       "Intend                                     46\n",
       "Investigate                                 4\n",
       "Make Public Statement                     118\n",
       "Protest                                     9\n",
       "Provide Aid                                65\n",
       "Reduce Relations                           53\n",
       "Reject                                     47\n",
       "Threaten                                    6\n",
       "Yield                                     128\n",
       "Name: index, dtype: int64"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec.groupby(\"label\").index.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>subj</th>\n",
       "      <th>obj</th>\n",
       "      <th>label</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>For more on our coverage of the war in Ukraine...</td>\n",
       "      <td>loaded ships</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.937298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"Russia must clearly understand that Russia wi...</td>\n",
       "      <td>70,000 people</td>\n",
       "      <td>the Kherson region</td>\n",
       "      <td>Yield</td>\n",
       "      <td>0.810604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>12:04pm Kremlin accuses UK of ‘directing and c...</td>\n",
       "      <td>coordinating</td>\n",
       "      <td>Nord Stream blastsThe Kremlin</td>\n",
       "      <td>Engage In Material Cooperation</td>\n",
       "      <td>0.898151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Swedish, Finnish NATO bidsFinland's Prime Mini...</td>\n",
       "      <td>Finland</td>\n",
       "      <td>the NATO defence alliance</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.716120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Swedish, Finnish NATO bidsFinland's Prime Mini...</td>\n",
       "      <td>Finland</td>\n",
       "      <td>the NATO defence alliance</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.716120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1621</th>\n",
       "      <td>1621</td>\n",
       "      <td>As a result, Lesotho's has been run by coaliti...</td>\n",
       "      <td>no prime minister</td>\n",
       "      <td>a full five-year term</td>\n",
       "      <td>Engage In Material Cooperation</td>\n",
       "      <td>0.912791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1623</th>\n",
       "      <td>1623</td>\n",
       "      <td>The UK delegation will also include Foreign Se...</td>\n",
       "      <td>The UK delegation</td>\n",
       "      <td>Foreign Secretary James Cleverly</td>\n",
       "      <td>Yield</td>\n",
       "      <td>0.746475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1626</th>\n",
       "      <td>1626</td>\n",
       "      <td>\" a spokesperson for the State Department also...</td>\n",
       "      <td>a spokesperson</td>\n",
       "      <td>remarks</td>\n",
       "      <td>Make Public Statement</td>\n",
       "      <td>0.964414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1628</th>\n",
       "      <td>1628</td>\n",
       "      <td>MUMBAI: Mass layoffs at Twitter and also other...</td>\n",
       "      <td>Mass layoffs</td>\n",
       "      <td>also other companies</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.930462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1632</th>\n",
       "      <td>1632</td>\n",
       "      <td>“It seems Joe Biden's positions change dependi...</td>\n",
       "      <td>Joe Biden</td>\n",
       "      <td>a lesson</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.903089</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>594 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                               text  \\\n",
       "0         0  For more on our coverage of the war in Ukraine...   \n",
       "3         3  \"Russia must clearly understand that Russia wi...   \n",
       "5         5  12:04pm Kremlin accuses UK of ‘directing and c...   \n",
       "7         7  Swedish, Finnish NATO bidsFinland's Prime Mini...   \n",
       "8         8  Swedish, Finnish NATO bidsFinland's Prime Mini...   \n",
       "...     ...                                                ...   \n",
       "1621   1621  As a result, Lesotho's has been run by coaliti...   \n",
       "1623   1623  The UK delegation will also include Foreign Se...   \n",
       "1626   1626  \" a spokesperson for the State Department also...   \n",
       "1628   1628  MUMBAI: Mass layoffs at Twitter and also other...   \n",
       "1632   1632  “It seems Joe Biden's positions change dependi...   \n",
       "\n",
       "                   subj                               obj  \\\n",
       "0          loaded ships                           Ukraine   \n",
       "3         70,000 people                the Kherson region   \n",
       "5          coordinating     Nord Stream blastsThe Kremlin   \n",
       "7               Finland         the NATO defence alliance   \n",
       "8               Finland         the NATO defence alliance   \n",
       "...                 ...                               ...   \n",
       "1621  no prime minister             a full five-year term   \n",
       "1623  The UK delegation  Foreign Secretary James Cleverly   \n",
       "1626     a spokesperson                           remarks   \n",
       "1628       Mass layoffs              also other companies   \n",
       "1632          Joe Biden                          a lesson   \n",
       "\n",
       "                               label      prob  \n",
       "0                            Consult  0.937298  \n",
       "3                              Yield  0.810604  \n",
       "5     Engage In Material Cooperation  0.898151  \n",
       "7                            Consult  0.716120  \n",
       "8                            Consult  0.716120  \n",
       "...                              ...       ...  \n",
       "1621  Engage In Material Cooperation  0.912791  \n",
       "1623                           Yield  0.746475  \n",
       "1626           Make Public Statement  0.964414  \n",
       "1628                         Consult  0.930462  \n",
       "1632                         Consult  0.903089  \n",
       "\n",
       "[594 rows x 6 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the nine people arrested include two managers, two ticket clerks, two contractors and three security guards.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec.iloc[10].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                                   16\n",
       "text     the nine people arrested include two managers,...\n",
       "subj                                       the nine people\n",
       "obj                                           two managers\n",
       "label                                                Yield\n",
       "prob                                              0.064997\n",
       "Name: 16, dtype: object"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec.iloc[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8752286434173584"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premise = \"In early November, Olaf Scholz will be traveling to Beijing for the first time as German chancellor, and despite the ongoing debate about the German economy’s unsustainable dependence on China, Olaf Scholz will likely bring along a significant delegation of German executives\"\n",
    "subj = \"Olaf Scholz\"\n",
    "rel = \"Consult\"\n",
    "obj =  \"Beijing\"\n",
    "\n",
    "hypothesis = f'{subj} does {rel} towards {obj}.'\n",
    "\n",
    "# run through model pre-trained on MNLI\n",
    "x = tokenizer.encode(premise, hypothesis, return_tensors='pt',\n",
    "                    truncation_strategy='only_first')\n",
    "logits = nli_model(x)[0]\n",
    "\n",
    "entail_contradiction_logits = logits[:,[0,2]]\n",
    "probs = entail_contradiction_logits.softmax(dim=1)\n",
    "prob_label_is_true = probs[:,1]\n",
    "\n",
    "prob_label_is_true.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "eb6e93dfa86b7f97c60c39b0c0c403458d25a4deb04fc60caf4cf2dd07f32ebe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
