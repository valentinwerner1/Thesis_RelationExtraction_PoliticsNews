{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/werner/thesis_valentin/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import xml.etree.ElementTree as ET\n",
    "from spacy.symbols import nsubj, dobj, pobj, iobj, neg, xcomp, ccomp, VERB\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def read_lines(inputparsed):    \n",
    "    \"\"\"takes input from CoreNLP sentence parsed file and returns sentences\"\"\"\n",
    "    #parse all lines from CoreNLP sentence split\n",
    "    parsed = open(inputparsed, encoding = \"utf-8\")\n",
    "    parsedfile = parsed.readlines()\n",
    "    parsedlines = []\n",
    "\n",
    "    #Only keep those lines which have Sentence #n in the line before\n",
    "    for idx, text in enumerate(parsedfile):\n",
    "        if text.startswith(\"Sentence #\"):\n",
    "            parsedlines.append(parsedfile[idx+1].replace('\\n','').strip())\n",
    "    \n",
    "    return parsedlines\n",
    "\n",
    "def gen_poss(line, verb_match, pre_dict):\n",
    "    \"\"\"generates all possibilities of patterns that a multi word line implies,\n",
    "    by extracting partial patterns and resolving placeholder words\"\"\"\n",
    "    poss = []\n",
    "\n",
    "    #replace special tokens in text that are clear at this point\n",
    "    line = line.replace(\"*\", verb_match)\n",
    "    line = line.replace(\"- \", \"\")\n",
    "    line = line.replace(\"+\",\"\")\n",
    "    line = line.replace(\"%\",\"\")\n",
    "    line = line.replace(\"^\",\"\")\n",
    "    line = line.replace(\"$\",\"\")\n",
    "\n",
    "    #split line by possibility indicators and code (always ends possibility)\n",
    "    #example.: \"- $ * (P ON KILLING (P OF + [010] #  COMMENT <ELH 07 May 2008>\"\n",
    "    poss_split = re.split(\"\\(P |\\[.*]\",line) \n",
    "\n",
    "    if len(poss_split) > 2: #2 is if no (P in the line\n",
    "        #only combining the first (P, as they share the same code \n",
    "        #and the longer version will never be contained in a text if the shorter isnt\n",
    "        poss.append(strip_multiple_whitespaces(\" \".join(poss_split[:2])).lower().rstrip().lstrip())\n",
    "    else: \n",
    "        poss.append(strip_multiple_whitespaces(poss_split[0].lower().rstrip().lstrip()))\n",
    "\n",
    "    cleaned = []\n",
    "    for text in list(set(poss)):\n",
    "        c = 0\n",
    "        for tag in list(pre_dict.keys()):\n",
    "            if tag in text:\n",
    "                for replacement in pre_dict[tag]:\n",
    "                    cleaned.append(text.replace(tag, replacement))\n",
    "                    c += 1\n",
    "        if c == 0:\n",
    "            cleaned.append(text)\n",
    "\n",
    "    return cleaned\n",
    "    \n",
    "\n",
    "def verb_code_dict(pico_path, verb_path):\n",
    "    \"\"\"reads coding ontology and verb lists, \n",
    "    directly matches verbs to their CAMEO codes and returns this verbs:codes dictionairy.\n",
    "    verb with codes that cannot be read are printed out as full line of the file\"\"\"\n",
    "    #read PETRARCH Internal Coding Ontology (= pico)\n",
    "    pico_path = os.path.join(os.getcwd(), pico_path)\n",
    "    pico_file = open(pico_path, 'r')\n",
    "    pico_lines = pico_file.readlines()\n",
    "\n",
    "    #get all 20 codes with their respective code\n",
    "    main_codes = {}                             #we run one iteration for all the main codes, only main codes contain relation name\n",
    "    for line in pico_lines:\n",
    "        line = line.split('#')\n",
    "        if line[0] == \"\" or line[0] == \"\\n\":    #only intro comments and empty lines\n",
    "            continue\n",
    "        else: \n",
    "            code_split = line[0].split(\":\")     #splits into CAMEO code and related hex\n",
    "            if len(line) > 1 and code_split[0][2] == \"0\":      #only main categories have 0 in 3rd idx, [cat_num 0] -> [010]\n",
    "                main_codes[code_split[0][:2]] = line[-1].replace(\"\\n\",\"\")\n",
    "    \n",
    "    #map code to code we want to use in the training\n",
    "    map_codes = {\"DiplomaticCoop\" : \"Engage In Diplomatic Cooperation\", \n",
    "                \"MaterialCoop\" : \"Engage In Material Cooperation\",\n",
    "                \"ProvideAid\" : \"Provide Aid\",\n",
    "                \"Exhibit Force Posture\": \"Exhibit Military Posture\",\n",
    "                \"Use Unconventional Mass Violence\" : \"Engage In Unconventional Mass Violence\"}\n",
    "    main_codes = {k: (map_codes[v] if v in map_codes else v) for k, v in main_codes.items()}\n",
    "    \n",
    "    #read single word patterns and match their code to the relation extracted in main_codes\n",
    "    verb_path = os.path.join(os.getcwd(), verb_path)\n",
    "    verb_file = open(verb_path, 'r')\n",
    "    verb_lines = verb_file.readlines()\n",
    "    \n",
    "    verb_dict = {}\n",
    "    for line in verb_lines:\n",
    "        if line[0] == \"#\":\n",
    "            continue\n",
    "        elif line.startswith(\"---\"):    #main verbs have a lead code, which is applied to all very in the section\n",
    "                                        #unless a separate code is specified for a specific verb in section\n",
    "            try: cur_main_code = re.split(\"\\[|\\]|---\", line)[2].replace(\":\",\"\")[:2]  #we only need main codes which are first two numbers\n",
    "                                                                                #sometimes code starts with \":\", e.g.: ---  OFFEND   [:110]  ---\n",
    "                                                                                #we just remove those to get the main code\n",
    "            except:                     #depending on chosen verb dictionairy, there may be main verbs without lead codes\n",
    "                print(\"couldn't finde code in: \", line.replace(\"\\n\",\"\")) \n",
    "                cur_main_code == \"--\"\n",
    "            if cur_main_code == \"\": cur_main_code = \"--\"\n",
    "        elif line == \"\\n\":              #skip empty lines\n",
    "            continue\n",
    "        elif line[0] == \"-\" or line[0] == \"~\" or line[0] == \"+\" or line[0] == \"&\": #removes all special structures we cannot use\n",
    "            continue\n",
    "        else:\n",
    "            if len(re.split(\"\\[|\\]\", line)) > 1:    #verbs with their own code, e.g.: AFFIRM [051] \n",
    "                code = re.split(\"\\[|\\]\", line)[1].replace(\":\",\"\")[:2]\n",
    "                if code != \"--\":\n",
    "                    if \"{\" in line:         #conjugated verbs, e.g. \"APPLY {APPLYING APPLIED APPLIES } [020]\"\n",
    "                        line_s = re.split(\"\\{|\\}\", line)    #split at { and }\n",
    "                        verb_dict[line_s[0].lower()] = main_codes[code] \n",
    "                        for word in line_s[1].split():\n",
    "                            verb_dict[word.lower()] = main_codes[code]\n",
    "                    else:\n",
    "                        word = re.split(\"\\[|\\]\", line)[0]\n",
    "                        verb_dict[word.lower()] = main_codes[code]\n",
    "            else:\n",
    "                if cur_main_code != \"--\":\n",
    "                    if \"{\" in line:         #e.g. \"HURRY {HURRIES HURRYING HURRIED }\" \n",
    "                        line_s = re.split(\"\\{|\\}\", line)    #split at { and }\n",
    "                        verb_dict[line_s[0].lower()] = main_codes[cur_main_code]\n",
    "                        for word in line_s[1].split():\n",
    "                            verb_dict[word.lower()] = main_codes[cur_main_code]\n",
    "                    else:                   #only single words with sometimes comments, e.g.: CENSURE  # JON 5/17/95\n",
    "                        word = line.split(\"#\")[0].rstrip()    #gets part before \"#\", removes all whitespaces to the right\n",
    "                        verb_dict[word.lower()] = main_codes[cur_main_code]\n",
    "\n",
    "    #read multi word patterns and create a dictionary for their code\n",
    "\n",
    "    #get filler words that occur in multi word patterns\n",
    "    verb_file = open(verb_path, 'r')\n",
    "    verb_lines = verb_file.readlines()\n",
    "\n",
    "    pre_dict = {}\n",
    "    filter_list = []\n",
    "    for line in verb_lines:\n",
    "        if line.startswith(\"&\"):\n",
    "            cur_filter = line.rstrip()\n",
    "        elif line.startswith(\"\\n\") and \"cur_filter\" in locals():\n",
    "            pre_dict[cur_filter.lower()] = filter_list\n",
    "            cur_filter = \"\"\n",
    "            filter_list = []\n",
    "        elif line.startswith(\"+\") and cur_filter != \"\":\n",
    "            filter_list.append(line.rstrip()[1:].replace(\"_\", \"\").lower())\n",
    "    del pre_dict[\"\"]\n",
    "\n",
    "    #generate dictionaries for multi word patterns\n",
    "    verb_file = open(verb_path, 'r')\n",
    "    verb_lines = verb_file.readlines()\n",
    "\n",
    "    spec_dict = {}\n",
    "    spec_code = {}\n",
    "\n",
    "    count = 0\n",
    "    for line in verb_lines:\n",
    "        if line.startswith(\"- \"):\n",
    "            #get main verb as dict key\n",
    "            try: \n",
    "                verb_match = re.search(\"# *\\w+\", line).group()\n",
    "                verb_match = re.search(\"\\w+\", verb_match).group()\n",
    "                verb_match = verb_match.replace(\"_\", \" \").lower()\n",
    "            except: \n",
    "                count += 1\n",
    "\n",
    "            #get code for line\n",
    "            try:\n",
    "                code = re.search(\"\\[.*]\", line).group()[1:3]\n",
    "                if code != \"--\":\n",
    "                    #get all possibility that the line indicates\n",
    "                    poss = gen_poss(line, verb_match, pre_dict)\n",
    "                    for pattern in poss:\n",
    "                        spec_code[pattern] = main_codes[code]\n",
    "                    spec_dict[verb_match] = poss\n",
    "            except:\n",
    "                count += 1\n",
    "\n",
    "    print(f\"{count} patterns could not be loaded\")        \n",
    "\n",
    "    return verb_dict, spec_dict, spec_code\n",
    "\n",
    "\n",
    "def get_triples(sentence, verb_dict, spec_dict, spec_code, nlp):\n",
    "    \"\"\"create triplet structure for training from text input, \n",
    "    verb_dict needs to be loaded before,\n",
    "    spacy model needs to be initialized before \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    verbs = []\n",
    "    dict = {}\n",
    "\n",
    "\n",
    "    for possible_verb in doc:\n",
    "        if possible_verb.pos == VERB:\n",
    "            if neg in [child.dep for child in possible_verb.children]: continue\n",
    "            else: \n",
    "                for possible_subject in possible_verb.children: \n",
    "                    if possible_subject.dep == xcomp or possible_subject.dep == ccomp: \n",
    "                        main_verb = possible_subject\n",
    "                        main_idx = possible_subject.idx\n",
    "                        for token in doc.ents:\n",
    "                            if token.label_ in [\"GPE\", \"NORP\", \"EVENTS\", \"FAC\", \"LAW\", \"ORG\", \"PERSON\"]:\n",
    "                                if token.root.dep_ == \"poss\" or token.root.dep_ == \"prep\":\n",
    "                                    if token.root.head.head.idx == possible_verb.idx:\n",
    "                                        verbs.append([main_idx, main_verb.lemma_, token.text, token.root.head.dep_])\n",
    "                                        if main_idx in dict.keys(): dict[main_idx] += 1\n",
    "                                        else: dict[main_idx] = 1\n",
    "                                else:\n",
    "                                    if token.root.head.idx == possible_verb.idx:\n",
    "                                        verbs.append([main_idx, main_verb.lemma_, token.text, token.root.dep_])\n",
    "                                        if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "                                        else: dict[possible_verb.idx] = 1\n",
    "\n",
    "                for token in doc.ents:\n",
    "                    if token.label_ in [\"GPE\", \"NORP\", \"EVENTS\", \"FAC\", \"LAW\", \"ORG\", \"PERSON\"]:\n",
    "                        if token.root.dep_ == \"poss\" or token.root.dep_ == \"prep\":\n",
    "                            if token.root.head.head.idx == possible_verb.idx:\n",
    "                                verbs.append([possible_verb.idx, possible_verb.lemma_, token.text, token.root.head.dep_])\n",
    "                                if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "                                else: dict[possible_verb.idx] = 1\n",
    "                        else:\n",
    "                            if token.root.head.idx == possible_verb.idx:\n",
    "                                verbs.append([possible_verb.idx, possible_verb.lemma_, token.text, token.root.dep_])\n",
    "                                if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "                                else: dict[possible_verb.idx] = 1\n",
    "\n",
    "    trip_idx = [key for key in dict if dict[key] > 1]\n",
    "\n",
    "    # doc = nlp(sentence)\n",
    "    # verbs = []\n",
    "    # dict = {}\n",
    "\n",
    "    # for possible_verb in doc:           #parses through all words in sentence\n",
    "    #     if possible_verb.pos == VERB:   #we only care about verbs\n",
    "    #         if neg in [child.dep for child in possible_verb.children]: continue #we exclude all negated verbs\n",
    "    #         else: \n",
    "    #             for candidate in possible_verb.children: #for composed verbs of verb (e.g. \"want to join\" -> \"want join\")\n",
    "    #                 if candidate.dep == xcomp:   #subj / obj of composed verb should also be subj / obj of main verb\n",
    "    #                     main_verb = candidate    \n",
    "    #                     main_idx = candidate.idx\n",
    "    #                     for chunk in doc.noun_chunks:   #chunks are noun-groups (e.g.: \"78 out of 100 people\" instead of \"people\")\n",
    "    #                         if chunk.root.head.idx == possible_verb.idx:    #if chunk applies to xcomp (want),\n",
    "    #                                                                         #treat it like it aplles to main verb (\"join\")\n",
    "    #                             verbs.append([main_idx, main_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "    #                             if main_idx in dict.keys(): dict[main_idx] += 1 #count how often verb is used\n",
    "    #                             else: dict[main_idx] = 1\n",
    "\n",
    "    #             for chunk in doc.noun_chunks:       #for normal verbs, check chunks directly\n",
    "    #                 if chunk.root.head.idx == possible_verb.idx:\n",
    "    #                     verbs.append([possible_verb.idx, possible_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "    #                     if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "    #                     else: dict[possible_verb.idx] = 1\n",
    "    \n",
    "    # trip_idx = [key for key in dict if dict[key] > 1]   #if verbs used more than once, its candidate for triplet\n",
    "\n",
    "    #priority for subj-relation-obj triplets\n",
    "    mapper = {\"nsubj\":1,\"dobj\":2, \"pobj\":2, \"iobj\":2}\n",
    "\n",
    "    #create df from verbs extracted \n",
    "    df = pd.DataFrame(verbs, columns = [\"idx\", \"verb\", \"noun\", \"noun_type\"])\n",
    "    df[\"noun_map\"] = df.noun_type.map(mapper)  #turn noun_types into priority \n",
    "    return df\n",
    "\n",
    "    # #create groups that resolve around same word\n",
    "    # gb = df.groupby('idx')    \n",
    "    # #only keep groups if verb idx was identified as potential triplet before, sort by priority for structure\n",
    "    # df_l = [gb.get_group(x).sort_values(\"noun_map\") for x in gb.groups if gb.get_group(x).idx.iloc[0] in dict]\n",
    "    # matches = [merge_trip(group) for group in df_l if not merge_trip(group) == None] #get groups into triplet structure\n",
    "    \n",
    "    # #turn matches into triples by only keeping those with coded verbs, return code instead of verb\n",
    "    # triples = []\n",
    "    # for match in matches:\n",
    "    #     if match[1].lower() in spec_dict:\n",
    "    #         for poss_pattern in spec_dict[match[1].lower()]:\n",
    "    #             if set(poss_pattern.split()).intersection(sentence.split()) == set(poss_pattern.split()):\n",
    "    #                 triples.append(f\"<triplet> {match[0]} <subj> {match[2]} <obj> {spec_code[poss_pattern]}\")\n",
    "                    \n",
    "    #     elif match[1].lower() in verb_dict:\n",
    "    #         triples.append(f\"<triplet> {match[0]} <subj> {match[2]} <obj> {verb_dict[match[1].lower()]}\")\n",
    "    #     else: print(f\"couldn't match {match[1].lower()}\")\n",
    "\n",
    "    # #triples = [f\"<triplet> {match[0]} <subj> {match[2]} <obj> {verb_dict[match[1].lower()]}\" for match in matches if match[1].lower() in verb_dict]\n",
    "\n",
    "    # return triples\n",
    "\n",
    "def merge_trip(df):\n",
    "    \"\"\"helper function to turn two rows of a pandas groupby into subj, verb, obj\"\"\"\n",
    "    if df.shape[0] == 2:\n",
    "        if df.noun_type.iloc[0] != df.noun_type.iloc[1]:\n",
    "            return [df.iloc[0].noun, df.iloc[0].verb, df.iloc[1].noun]\n",
    "    elif df.shape[0] > 2:\n",
    "        for i in range(df.shape[0] - 1):\n",
    "            if df.noun_type.iloc[i] != df.noun_type.iloc[i+1]:\n",
    "                return [df.iloc[0].noun, df.iloc[0].verb, df.iloc[1].noun]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pico_path = \"src/add_labels/dictionaries/PETR.Internal.Coding.Ontology.txt\"\n",
    "verb_path = \"src/add_labels/dictionaries/newdict.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "read = read_lines(\"data/out_data/articles_url_coref3.csv.xml.out\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "couldn't finde code in:  --- DEFEND  ###\n",
      "couldn't finde code in:  --- REVOKE_   ###\n",
      "couldn't finde code in:  --- SEND   ###\n",
      "couldn't finde code in:  --- COLLAPSE  ###\n",
      "22 patterns could not be loaded\n"
     ]
    }
   ],
   "source": [
    "verb_dict, spec_dict, spec_code = verb_code_dict(pico_path, verb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_2 = [[sentence, get_triples(sentence, verb_dict, spec_dict, spec_code, nlp)] for sentence in read]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21395"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfs_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['AdvertisingRead moreThis live page is no longer being updated.',\n",
       "  Empty DataFrame\n",
       "  Columns: [idx, verb, noun, noun_type, noun_map]\n",
       "  Index: []],\n",
       " ['For more on\\xa0our coverage of the war in Ukraine, click here.11:44pm:\\xa0UN grain coordinator\\xa0expects loaded ships to depart Ukraine on ThursdayThe UN coordinator for the Ukraine Black Sea grain deal said UN grain coordinator expects loaded ships to depart Ukrainian ports on ThursdayThe.',\n",
       "     idx    verb     noun noun_type  noun_map\n",
       "  0  113  depart  Ukraine      dobj         2],\n",
       " ['“Exports of grain and foodstuffs from Ukraine need to continue.',\n",
       "  Empty DataFrame\n",
       "  Columns: [idx, verb, noun, noun_type, noun_map]\n",
       "  Index: []],\n",
       " ['Although no movements of vessels are planned for 2 November under the #BlackSeaGrainInitiative, we expect loaded ships to sail on ThursdayThe,” UN coordinator Amir Abdulla posted on Twitter.',\n",
       "     idx  verb          noun noun_type  noun_map\n",
       "  0  172  post  Amir Abdulla     nsubj         1],\n",
       " ['Exports of grain and foodstuffs from Ukraine need to continue.',\n",
       "  Empty DataFrame\n",
       "  Columns: [idx, verb, noun, noun_type, noun_map]\n",
       "  Index: []]]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_2[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The UN Secretariat at coordination centreThere reports that the Ukrainian, Turkish and UN delegations agreed not to plan any movement of vessels in the Black Sea Grain Initiative for 2 November,\" \"The UN Secretariat at the Joint Coordination Centre said Tuesday, referring to the July deal brokered by Turkey and the UN.6:15pm:\\xa0Russian President Vladimir Putin tells Erdogan Russian President Vladimir Putin wants \\'real guarantees\\' from Kyiv on grain deal, says Russian President Vladimir Putin told Erdogan Tuesday that Russian President Vladimir Putin wanted \"real guarantees\" from Kyiv before Kyiv potentially rejoined grain deal.',\n",
       "    idx    verb                noun noun_type  noun_map\n",
       " 0   47  report  The UN Secretariat     nsubj         1\n",
       " 1  361    tell      Vladimir Putin     nsubj         1\n",
       " 2  408    want      Vladimir Putin     nsubj         1\n",
       " 3  495    tell      Vladimir Putin     nsubj         1\n",
       " 4  495    tell             Erdogan      dobj         2\n",
       " 5  554    want      Vladimir Putin     nsubj         1\n",
       " 6  613  rejoin                Kyiv     nsubj         1]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_2[13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidats_2 = [df for df in dfs_2 if df[1].shape[0] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8364"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidats_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     102\n",
       "1      47\n",
       "2     408\n",
       "3     361\n",
       "4     408\n",
       "5     554\n",
       "6     554\n",
       "7     495\n",
       "8     495\n",
       "9     554\n",
       "10    613\n",
       "Name: idx, dtype: int64"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs_2[13][1][\"idx\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_2 = []\n",
    "for can in candidats_2:\n",
    "    for idx in can[1][\"idx\"]:\n",
    "        if can[1][\"idx\"].to_list().count(idx) > 1:\n",
    "            trips_2.append([can[0], can[1]])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3510"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trips_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_2 = []\n",
    "for idx, df in enumerate(trips_2):\n",
    "    #create groups that resolve around same word\n",
    "    gb = df[1].groupby('idx')  \n",
    "    #only keep groups if verb idx was identified as potential triplet before, sort by priority for structure\n",
    "    for x in gb.groups:\n",
    "        group = gb.get_group(x).sort_values(\"noun_map\")\n",
    "        if group.shape[0] == 2:\n",
    "            if group.noun_type.iloc[0] != group.noun_type.iloc[1]:\n",
    "                matches_2.append([df[0], group.iloc[0].noun, group.iloc[0].verb, group.iloc[1].noun])\n",
    "        elif group.shape[0] > 2:\n",
    "            for i in range(group.shape[0] - 1):\n",
    "                if group.noun_type.iloc[i] != group.noun_type.iloc[i+1]:\n",
    "                    matches_2.append([df[0], group.iloc[0].noun, group.iloc[0].verb, group.iloc[1].noun])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "triples_2 = []\n",
    "ma_df = pd.DataFrame(matches_2, columns = [\"text\",\"subj\", \"verb\",\"obj\"])\n",
    "for row in ma_df.iterrows():\n",
    "    if row[1][\"verb\"] in spec_dict:\n",
    "        for poss_pattern in spec_dict[row[1][\"verb\"]]:\n",
    "            if set(poss_pattern.split()).intersection(row[1][\"text\"].split()) == set(poss_pattern.split()):\n",
    "                triples_2.append([row[1][\"text\"], row[1]['subj'], row[1]['obj'] , spec_code[poss_pattern]])\n",
    "    elif row[1][\"verb\"] in verb_dict:\n",
    "            triples_2.append([row[1][\"text\"], row[1]['subj'], row[1]['obj'], verb_dict[row[1]['verb']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "276"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(triples_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "huh2 = pd.DataFrame(triples_2, columns = [\"text\", \"subj\", \"obj\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\svawe\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\transformers\\tokenization_utils_base.py:2339: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')\n",
    "probs_l = []\n",
    "for row in huh2.iterrows():\n",
    "    premise = row[1][\"text\"]\n",
    "    subj = row[1][\"subj\"]\n",
    "    rel = row[1][\"label\"]\n",
    "    obj =  row[1][\"obj\"]\n",
    "\n",
    "    hypothesis = f'{subj} does {rel} towards {obj}.'\n",
    "\n",
    "    # run through model pre-trained on MNLI\n",
    "    x = tokenizer.encode(premise, hypothesis, return_tensors='pt',\n",
    "                        truncation_strategy='only_first')\n",
    "    logits = nli_model(x)[0]\n",
    "\n",
    "    # we throw away \"neutral\" (dim 1) and take the probability of\n",
    "    # \"entailment\" (2) as the probability of the label being true \n",
    "    entail_contradiction_logits = logits[:,[0,2]]\n",
    "    probs = entail_contradiction_logits.softmax(dim=1)\n",
    "    prob_label_is_true = probs[:,1]\n",
    "\n",
    "    probs_l.append([row[0], prob_label_is_true.item()])\n",
    "    if row[0] % 100 == 0: print(row[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent = pd.merge(huh2.reset_index(), pd.DataFrame(probs_l, columns = [\"index\", \"prob\"]), on = \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Russian President Vladimir Putin told Erdogan in a phone call that Russia sought \"real guarantees from Kyiv about the strict observance of the Istanbul agreement, in particular about not using the humanitarian corridor for military purposes\", according to a statement from the Kremlin.3:51pm: France says Russia endangering world food securityFrench President Emmanuel Macron on Tuesday accused Russia of endangering world food supplies by a unilateral decision by Russia which again harms global food security Russia\\'s participation in a landmark Ukraine grain deal.Russia on Saturday halted Russia\\'s participation in the agreement that allowed vital grain exports from Ukraine, blaming drone attacks on Russian ships in the Crimea.In a call with Ukrainian President Volodymyr Zelensky, securityFrench President Emmanuel Macron \"the announcement a unilateral decision by Russia which again harms global food security\", the president\\'s office said.Russia made the announcement after Russia\\'s army accused Kyiv of a \"massive\" drone attack on Kyiv\\'s Black Sea fleet, which Ukraine labelled a \"false pretext\" and UN urged grain deal\\'s preservation.The July deal to unlock grain exports signed between Russia and Ukraine and brokered by Turkey and UN has been seen as critical to easing the global food crisis caused by the conflict.3:18pm: securityFrench President Emmanuel Macron says France will help Ukraine get through winter, fix securityFrench President Emmanuel Macron, following a telephone call with Ukrainian President Volodymyr Zelensky, said France would help Ukraine get through winter and help repair water and energy infrastructure damaged by Russian strikes.France will also help boost Ukraine\\'s anti-air defences and securityFrench President Emmanuel Macron said securityFrench President Emmanuel Macron and Ukrainian President Volodymyr Zelensky had agreed to hold an international conference in Paris on December 13 to support Ukraine civilians in winter.A bilateral conference on December 12 will also aim at raising support for Ukraine from French companies, securityFrench President Emmanuel Macron said in a statement.3:13pm: securityFrench President Emmanuel Macron, Ukrainian President Volodymyr Zelensky discuss Ukraine\\'s defence needs, restoring energy ZelenskyUkraine\\'s President Volodymyr Zelensky said on Tuesday ZelenskyUkraine\\'s President Volodymyr Zelensky had an \"extremely important and productive conversation\" with securityFrench President Emmanuel Macron about strengthening Ukraine\\'s defence capabilities and restoring damaged\\xa0energy infrastructure.ZelenskyUkraine\\'s President Volodymyr Zelensky did not say what had been agreed but thanked securityFrench President Emmanuel Macron for securityFrench President Emmanuel Macron\\'s support since Russia invaded Ukraine.'"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent.text.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index                                                    1\n",
       "text     10:23am: Three more ships leave Ukraine ports ...\n",
       "subj                                               Finland\n",
       "obj                                                 Turkey\n",
       "label                                           Disapprove\n",
       "prob                                              0.097846\n",
       "Name: 1, dtype: object"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent.iloc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>subj</th>\n",
       "      <th>obj</th>\n",
       "      <th>label</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>\"Those who intend to confront and subvert the ...</td>\n",
       "      <td>the Associated Press</td>\n",
       "      <td>Iran</td>\n",
       "      <td>Make Public Statement</td>\n",
       "      <td>0.599615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>China have suggested to the Saudis that China ...</td>\n",
       "      <td>China</td>\n",
       "      <td>China</td>\n",
       "      <td>Engage In Material Cooperation</td>\n",
       "      <td>0.666303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>Maxim Dlugy said, \"Well, world champion Magnus...</td>\n",
       "      <td>Magnus Carlsen</td>\n",
       "      <td>Maxim Dlugy</td>\n",
       "      <td>Engage In Diplomatic Cooperation</td>\n",
       "      <td>0.829087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>Can China Bring Vladimir Putin's To Vladimir P...</td>\n",
       "      <td>China</td>\n",
       "      <td>Vladimir Putin's</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.598665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>Can China Bring Vladimir Putin's To Vladimir P...</td>\n",
       "      <td>China</td>\n",
       "      <td>Vladimir Putin's</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.598665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>266</td>\n",
       "      <td>In June, Alexander Nikulin - who worked at the...</td>\n",
       "      <td>Alexander Nikulin</td>\n",
       "      <td>Britons</td>\n",
       "      <td>Coerce</td>\n",
       "      <td>0.838175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>268</td>\n",
       "      <td>(Fernando Vergara/Associated Press)Earlier thi...</td>\n",
       "      <td>Nicolas Maduro</td>\n",
       "      <td>Gustavo Petro</td>\n",
       "      <td>Disapprove</td>\n",
       "      <td>0.587277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>269</td>\n",
       "      <td>In June, Alexander Nikulin - who worked at the...</td>\n",
       "      <td>Alexander Nikulin</td>\n",
       "      <td>Britons</td>\n",
       "      <td>Coerce</td>\n",
       "      <td>0.838175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>270</td>\n",
       "      <td>'Jordan, AdvertisingRead moreJordan Bardella, ...</td>\n",
       "      <td>moreJordan Bardella</td>\n",
       "      <td>Jordan</td>\n",
       "      <td>Make Public Statement</td>\n",
       "      <td>0.524766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>273</td>\n",
       "      <td>Donald Trump's victory in Pennsylvania helped ...</td>\n",
       "      <td>Donald Trump's</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Provide Aid</td>\n",
       "      <td>0.836310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     index                                               text  \\\n",
       "2        2  \"Those who intend to confront and subvert the ...   \n",
       "10      10  China have suggested to the Saudis that China ...   \n",
       "17      17  Maxim Dlugy said, \"Well, world champion Magnus...   \n",
       "23      23  Can China Bring Vladimir Putin's To Vladimir P...   \n",
       "24      24  Can China Bring Vladimir Putin's To Vladimir P...   \n",
       "..     ...                                                ...   \n",
       "266    266  In June, Alexander Nikulin - who worked at the...   \n",
       "268    268  (Fernando Vergara/Associated Press)Earlier thi...   \n",
       "269    269  In June, Alexander Nikulin - who worked at the...   \n",
       "270    270  'Jordan, AdvertisingRead moreJordan Bardella, ...   \n",
       "273    273  Donald Trump's victory in Pennsylvania helped ...   \n",
       "\n",
       "                     subj               obj                             label  \\\n",
       "2    the Associated Press              Iran             Make Public Statement   \n",
       "10                  China             China    Engage In Material Cooperation   \n",
       "17         Magnus Carlsen       Maxim Dlugy  Engage In Diplomatic Cooperation   \n",
       "23                  China  Vladimir Putin's                           Consult   \n",
       "24                  China  Vladimir Putin's                           Consult   \n",
       "..                    ...               ...                               ...   \n",
       "266     Alexander Nikulin           Britons                            Coerce   \n",
       "268        Nicolas Maduro     Gustavo Petro                        Disapprove   \n",
       "269     Alexander Nikulin           Britons                            Coerce   \n",
       "270   moreJordan Bardella            Jordan             Make Public Statement   \n",
       "273        Donald Trump's      Donald Trump                       Provide Aid   \n",
       "\n",
       "         prob  \n",
       "2    0.599615  \n",
       "10   0.666303  \n",
       "17   0.829087  \n",
       "23   0.598665  \n",
       "24   0.598665  \n",
       "..        ...  \n",
       "266  0.838175  \n",
       "268  0.587277  \n",
       "269  0.838175  \n",
       "270  0.524766  \n",
       "273  0.836310  \n",
       "\n",
       "[93 rows x 6 columns]"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ent[ent.prob > 0.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrial with chunks instead of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import xml.etree.ElementTree as ET\n",
    "from spacy.symbols import nsubj, dobj, pobj, iobj, neg, xcomp, VERB\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "\n",
    "def read_lines(inputparsed):    \n",
    "    \"\"\"takes input from CoreNLP sentence parsed file and returns sentences\"\"\"\n",
    "    #parse all lines from CoreNLP sentence split\n",
    "    parsed = open(inputparsed, encoding = \"utf-8\")\n",
    "    parsedfile = parsed.readlines()\n",
    "    parsedlines = []\n",
    "\n",
    "    #Only keep those lines which have Sentence #n in the line before\n",
    "    for idx, text in enumerate(parsedfile):\n",
    "        if text.startswith(\"Sentence #\"):\n",
    "            parsedlines.append(parsedfile[idx+1].replace('\\n','').strip())\n",
    "    \n",
    "    return parsedlines\n",
    "\n",
    "def gen_poss(line, verb_match, pre_dict):\n",
    "    \"\"\"generates all possibilities of patterns that a multi word line implies,\n",
    "    by extracting partial patterns and resolving placeholder words\"\"\"\n",
    "    poss = []\n",
    "\n",
    "    #replace special tokens in text that are clear at this point\n",
    "    line = line.replace(\"*\", verb_match)\n",
    "    line = line.replace(\"- \", \"\")\n",
    "    line = line.replace(\"+\",\"\")\n",
    "    line = line.replace(\"%\",\"\")\n",
    "    line = line.replace(\"^\",\"\")\n",
    "    line = line.replace(\"$\",\"\")\n",
    "\n",
    "    #split line by possibility indicators and code (always ends possibility)\n",
    "    #example.: \"- $ * (P ON KILLING (P OF + [010] #  COMMENT <ELH 07 May 2008>\"\n",
    "    poss_split = re.split(\"\\(P |\\[.*]\",line) \n",
    "\n",
    "    if len(poss_split) > 2: #2 is if no (P in the line\n",
    "        #only combining the first (P, as they share the same code \n",
    "        #and the longer version will never be contained in a text if the shorter isnt\n",
    "        poss.append(strip_multiple_whitespaces(\" \".join(poss_split[:2])).lower().rstrip().lstrip())\n",
    "    else: \n",
    "        poss.append(strip_multiple_whitespaces(poss_split[0].lower().rstrip().lstrip()))\n",
    "\n",
    "    cleaned = []\n",
    "    for text in list(set(poss)):\n",
    "        c = 0\n",
    "        for tag in list(pre_dict.keys()):\n",
    "            if tag in text:\n",
    "                for replacement in pre_dict[tag]:\n",
    "                    cleaned.append(text.replace(tag, replacement))\n",
    "                    c += 1\n",
    "        if c == 0:\n",
    "            cleaned.append(text)\n",
    "\n",
    "    return cleaned\n",
    "    \n",
    "\n",
    "def verb_code_dict(pico_path, verb_path):\n",
    "    \"\"\"reads coding ontology and verb lists, \n",
    "    directly matches verbs to their CAMEO codes and returns this verbs:codes dictionairy.\n",
    "    verb with codes that cannot be read are printed out as full line of the file\"\"\"\n",
    "    #read PETRARCH Internal Coding Ontology (= pico)\n",
    "    pico_path = os.path.join(os.getcwd(), pico_path)\n",
    "    pico_file = open(pico_path, 'r')\n",
    "    pico_lines = pico_file.readlines()\n",
    "\n",
    "    #get all 20 codes with their respective code\n",
    "    main_codes = {}                             #we run one iteration for all the main codes, only main codes contain relation name\n",
    "    for line in pico_lines:\n",
    "        line = line.split('#')\n",
    "        if line[0] == \"\" or line[0] == \"\\n\":    #only intro comments and empty lines\n",
    "            continue\n",
    "        else: \n",
    "            code_split = line[0].split(\":\")     #splits into CAMEO code and related hex\n",
    "            if len(line) > 1 and code_split[0][2] == \"0\":      #only main categories have 0 in 3rd idx, [cat_num 0] -> [010]\n",
    "                main_codes[code_split[0][:2]] = line[-1].replace(\"\\n\",\"\")\n",
    "    \n",
    "    #map code to code we want to use in the training\n",
    "    map_codes = {\"DiplomaticCoop\" : \"Engage In Diplomatic Cooperation\", \n",
    "                \"MaterialCoop\" : \"Engage In Material Cooperation\",\n",
    "                \"ProvideAid\" : \"Provide Aid\",\n",
    "                \"Exhibit Force Posture\": \"Exhibit Military Posture\",\n",
    "                \"Use Unconventional Mass Violence\" : \"Engage In Unconventional Mass Violence\"}\n",
    "    main_codes = {k: (map_codes[v] if v in map_codes else v) for k, v in main_codes.items()}\n",
    "    \n",
    "    #read single word patterns and match their code to the relation extracted in main_codes\n",
    "    verb_path = os.path.join(os.getcwd(), verb_path)\n",
    "    verb_file = open(verb_path, 'r')\n",
    "    verb_lines = verb_file.readlines()\n",
    "    \n",
    "    verb_dict = {}\n",
    "    for line in verb_lines:\n",
    "        if line[0] == \"#\":\n",
    "            continue\n",
    "        elif line.startswith(\"---\"):    #main verbs have a lead code, which is applied to all very in the section\n",
    "                                        #unless a separate code is specified for a specific verb in section\n",
    "            try: cur_main_code = re.split(\"\\[|\\]|---\", line)[2].replace(\":\",\"\")[:2]  #we only need main codes which are first two numbers\n",
    "                                                                                #sometimes code starts with \":\", e.g.: ---  OFFEND   [:110]  ---\n",
    "                                                                                #we just remove those to get the main code\n",
    "            except:                     #depending on chosen verb dictionairy, there may be main verbs without lead codes\n",
    "                print(\"couldn't finde code in: \", line.replace(\"\\n\",\"\")) \n",
    "                cur_main_code == \"--\"\n",
    "            if cur_main_code == \"\": cur_main_code = \"--\"\n",
    "        elif line == \"\\n\":              #skip empty lines\n",
    "            continue\n",
    "        elif line[0] == \"-\" or line[0] == \"~\" or line[0] == \"+\" or line[0] == \"&\": #removes all special structures we cannot use\n",
    "            continue\n",
    "        else:\n",
    "            if len(re.split(\"\\[|\\]\", line)) > 1:    #verbs with their own code, e.g.: AFFIRM [051] \n",
    "                code = re.split(\"\\[|\\]\", line)[1].replace(\":\",\"\")[:2]\n",
    "                if code != \"--\":\n",
    "                    if \"{\" in line:         #conjugated verbs, e.g. \"APPLY {APPLYING APPLIED APPLIES } [020]\"\n",
    "                        line_s = re.split(\"\\{|\\}\", line)    #split at { and }\n",
    "                        verb_dict[line_s[0].lower()] = main_codes[code] \n",
    "                        for word in line_s[1].split():\n",
    "                            verb_dict[word.lower()] = main_codes[code]\n",
    "                    else:\n",
    "                        word = re.split(\"\\[|\\]\", line)[0]\n",
    "                        verb_dict[word.lower()] = main_codes[code]\n",
    "            else:\n",
    "                if cur_main_code != \"--\":\n",
    "                    if \"{\" in line:         #e.g. \"HURRY {HURRIES HURRYING HURRIED }\" \n",
    "                        line_s = re.split(\"\\{|\\}\", line)    #split at { and }\n",
    "                        verb_dict[line_s[0].lower()] = main_codes[cur_main_code]\n",
    "                        for word in line_s[1].split():\n",
    "                            verb_dict[word.lower()] = main_codes[cur_main_code]\n",
    "                    else:                   #only single words with sometimes comments, e.g.: CENSURE  # JON 5/17/95\n",
    "                        word = line.split(\"#\")[0].rstrip()    #gets part before \"#\", removes all whitespaces to the right\n",
    "                        verb_dict[word.lower()] = main_codes[cur_main_code]\n",
    "\n",
    "    #read multi word patterns and create a dictionary for their code\n",
    "\n",
    "    #get filler words that occur in multi word patterns\n",
    "    verb_file = open(verb_path, 'r')\n",
    "    verb_lines = verb_file.readlines()\n",
    "\n",
    "    pre_dict = {}\n",
    "    filter_list = []\n",
    "    for line in verb_lines:\n",
    "        if line.startswith(\"&\"):\n",
    "            cur_filter = line.rstrip()\n",
    "        elif line.startswith(\"\\n\") and \"cur_filter\" in locals():\n",
    "            pre_dict[cur_filter.lower()] = filter_list\n",
    "            cur_filter = \"\"\n",
    "            filter_list = []\n",
    "        elif line.startswith(\"+\") and cur_filter != \"\":\n",
    "            filter_list.append(line.rstrip()[1:].replace(\"_\", \"\").lower())\n",
    "    del pre_dict[\"\"]\n",
    "\n",
    "    #generate dictionaries for multi word patterns\n",
    "    verb_file = open(verb_path, 'r')\n",
    "    verb_lines = verb_file.readlines()\n",
    "\n",
    "    spec_dict = {}\n",
    "    spec_code = {}\n",
    "\n",
    "    count = 0\n",
    "    for line in verb_lines:\n",
    "        if line.startswith(\"- \"):\n",
    "            #get main verb as dict key\n",
    "            try: \n",
    "                verb_match = re.search(\"# *\\w+\", line).group()\n",
    "                verb_match = re.search(\"\\w+\", verb_match).group()\n",
    "                verb_match = verb_match.replace(\"_\", \" \").lower()\n",
    "            except: \n",
    "                count += 1\n",
    "\n",
    "            #get code for line\n",
    "            try:\n",
    "                code = re.search(\"\\[.*]\", line).group()[1:3]\n",
    "                if code != \"--\":\n",
    "                    #get all possibility that the line indicates\n",
    "                    poss = gen_poss(line, verb_match, pre_dict)\n",
    "                    for pattern in poss:\n",
    "                        spec_code[pattern] = main_codes[code]\n",
    "                    spec_dict[verb_match] = poss\n",
    "            except:\n",
    "                count += 1\n",
    "\n",
    "    print(f\"{count} patterns could not be loaded\")        \n",
    "\n",
    "    return verb_dict, spec_dict, spec_code\n",
    "\n",
    "\n",
    "def get_triples(sentence, verb_dict, spec_dict, spec_code, nlp):\n",
    "    \"\"\"create triplet structure for training from text input, \n",
    "    verb_dict needs to be loaded before,\n",
    "    spacy model needs to be initialized before \"\"\"\n",
    "    doc = nlp(sentence)\n",
    "    verbs = []\n",
    "    dict = {}\n",
    "    for possible_verb in doc:\n",
    "        if possible_verb.pos == VERB:\n",
    "            if neg in [child.dep for child in possible_verb.children]: continue\n",
    "            else: \n",
    "                for possible_subject in possible_verb.children: \n",
    "                    if possible_subject.dep == xcomp or possible_subject.dep == ccomp:   #subj / obj of composed verb should also be subj / obj of main verb\n",
    "                        main_verb = possible_subject\n",
    "                        main_idx = possible_subject.idx\n",
    "                        \n",
    "                        for chunk in doc.noun_chunks:\n",
    "                            if chunk.root.dep_ == \"poss\" or chunk.root.dep_ == \"prep\":\n",
    "                                if chunk.root.head.head.idx == possible_verb.idx:\n",
    "                                    verbs.append([main_idx, main_verb.lemma_, chunk.text, chunk.root.head.dep_])\n",
    "\n",
    "                            else:\n",
    "                                if chunk.root.head.idx == possible_verb.idx:\n",
    "                                    verbs.append([main_idx, main_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "\n",
    "\n",
    "                for chunk in doc.noun_chunks:       #for normal verbs, check chunks directly\n",
    "                    if chunk.root.head.dep_ == \"poss\" or chunk.root.head.dep_ == \"prep\":\n",
    "                        if chunk.root.head.head.idx == possible_verb.idx:\n",
    "                            verbs.append([possible_verb.idx, possible_verb.lemma_, chunk.text, chunk.root.head.dep_])\n",
    "\n",
    "                    else:\n",
    "                        if chunk.root.head.idx == possible_verb.idx:\n",
    "                            verbs.append([possible_verb.idx, possible_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "\n",
    "    # doc = nlp(sentence)\n",
    "    # verbs = []\n",
    "    # dict = {}\n",
    "\n",
    "    # for possible_verb in doc:           #parses through all words in sentence\n",
    "    #     if possible_verb.pos == VERB:   #we only care about verbs\n",
    "    #         if neg in [child.dep for child in possible_verb.children]: continue #we exclude all negated verbs\n",
    "    #         else: \n",
    "    #             for candidate in possible_verb.children: #for composed verbs of verb (e.g. \"want to join\" -> \"want join\")\n",
    "    #                 if candidate.dep == xcomp:   #subj / obj of composed verb should also be subj / obj of main verb\n",
    "    #                     main_verb = candidate    \n",
    "    #                     main_idx = candidate.idx\n",
    "    #                     for chunk in doc.noun_chunks:   #chunks are noun-groups (e.g.: \"78 out of 100 people\" instead of \"people\")\n",
    "    #                         if chunk.root.head.idx == possible_verb.idx:    #if chunk applies to xcomp (want),\n",
    "    #                                                                         #treat it like it aplles to main verb (\"join\")\n",
    "    #                             verbs.append([main_idx, main_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "    #                             if main_idx in dict.keys(): dict[main_idx] += 1 #count how often verb is used\n",
    "    #                             else: dict[main_idx] = 1\n",
    "\n",
    "    #             for chunk in doc.noun_chunks:       #for normal verbs, check chunks directly\n",
    "    #                 if chunk.root.head.idx == possible_verb.idx:\n",
    "    #                     verbs.append([possible_verb.idx, possible_verb.lemma_, chunk.text, chunk.root.dep_])\n",
    "    #                     if possible_verb.idx in dict.keys(): dict[possible_verb.idx] += 1\n",
    "    #                     else: dict[possible_verb.idx] = 1\n",
    "    \n",
    "    # trip_idx = [key for key in dict if dict[key] > 1]   #if verbs used more than once, its candidate for triplet\n",
    "\n",
    "    #priority for subj-relation-obj triplets\n",
    "    mapper = {\"nsubj\":1,\"dobj\":2, \"pobj\":2, \"iobj\":2}\n",
    "\n",
    "    #create df from verbs extracted \n",
    "    df = pd.DataFrame(verbs, columns = [\"idx\", \"verb\", \"noun\", \"noun_type\"])\n",
    "    df[\"noun_map\"] = df.noun_type.map(mapper)  #turn noun_types into priority \n",
    "    return df\n",
    "\n",
    "    # #create groups that resolve around same word\n",
    "    # gb = df.groupby('idx')    \n",
    "    # #only keep groups if verb idx was identified as potential triplet before, sort by priority for structure\n",
    "    # df_l = [gb.get_group(x).sort_values(\"noun_map\") for x in gb.groups if gb.get_group(x).idx.iloc[0] in dict]\n",
    "    # matches = [merge_trip(group) for group in df_l if not merge_trip(group) == None] #get groups into triplet structure\n",
    "    \n",
    "    # #turn matches into triples by only keeping those with coded verbs, return code instead of verb\n",
    "    # triples = []\n",
    "    # for match in matches:\n",
    "    #     if match[1].lower() in spec_dict:\n",
    "    #         for poss_pattern in spec_dict[match[1].lower()]:\n",
    "    #             if set(poss_pattern.split()).intersection(sentence.split()) == set(poss_pattern.split()):\n",
    "    #                 triples.append(f\"<triplet> {match[0]} <subj> {match[2]} <obj> {spec_code[poss_pattern]}\")\n",
    "                    \n",
    "    #     elif match[1].lower() in verb_dict:\n",
    "    #         triples.append(f\"<triplet> {match[0]} <subj> {match[2]} <obj> {verb_dict[match[1].lower()]}\")\n",
    "    #     else: print(f\"couldn't match {match[1].lower()}\")\n",
    "\n",
    "    # #triples = [f\"<triplet> {match[0]} <subj> {match[2]} <obj> {verb_dict[match[1].lower()]}\" for match in matches if match[1].lower() in verb_dict]\n",
    "\n",
    "    # return triples\n",
    "\n",
    "def merge_trip(df):\n",
    "    \"\"\"helper function to turn two rows of a pandas groupby into subj, verb, obj\"\"\"\n",
    "    if df.shape[0] == 2:\n",
    "        if df.noun_type.iloc[0] != df.noun_type.iloc[1]:\n",
    "            return [df.iloc[0].noun, df.iloc[0].verb, df.iloc[1].noun]\n",
    "    elif df.shape[0] > 2:\n",
    "        for i in range(df.shape[0] - 1):\n",
    "            if df.noun_type.iloc[i] != df.noun_type.iloc[i+1]:\n",
    "                return [df.iloc[0].noun, df.iloc[0].verb, df.iloc[1].noun]\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = [[sentence, get_triples(sentence, verb_dict, spec_dict, spec_code, nlp)] for sentence in read]\n",
    "#very heavy single core - possibly parallelizable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21395"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['“They continue their indiscriminate bombardment of civilians and attacks on civilian infrastructure.',\n",
       "    idx      verb                              noun noun_type  noun_map\n",
       " 0    6  continue                              They     nsubj         1\n",
       " 1    6  continue  their indiscriminate bombardment      dobj         2]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidats = [df for df in dfs if df[1].shape[0] >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18552"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(candidats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = []\n",
    "for can in candidats:\n",
    "    for idx in can[1][\"idx\"]:\n",
    "        if can[1][\"idx\"].to_list().count(idx) > 1:\n",
    "            trips.append([can[0], can[1]])\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17157"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trips)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = []\n",
    "for idx, df in enumerate(trips):\n",
    "    #create groups that resolve around same word\n",
    "    gb = df[1].groupby('idx')    \n",
    "    #only keep groups if verb idx was identified as potential triplet before, sort by priority for structure\n",
    "    for x in gb.groups:\n",
    "        if gb.get_group(x).shape[0] == 2:\n",
    "            if gb.get_group(x).noun_type.iloc[0] != gb.get_group(x).noun_type.iloc[1]:\n",
    "                matches.append([df[0], gb.get_group(x).iloc[0].noun, gb.get_group(x).iloc[0].verb, gb.get_group(x).iloc[1].noun])\n",
    "        elif gb.get_group(x).shape[0] > 2:\n",
    "            for i in range(gb.get_group(x).shape[0] - 1):\n",
    "                if gb.get_group(x).noun_type.iloc[i] != gb.get_group(x).noun_type.iloc[i+1]:\n",
    "                    matches.append([df[0], gb.get_group(x).iloc[0].noun, gb.get_group(x).iloc[0].verb, gb.get_group(x).iloc[1].noun])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46112"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# triples = []\n",
    "# ma_df = pd.DataFrame(matches, columns = [\"text\",\"subj\", \"verb\",\"obj\"])\n",
    "# for row in ma_df.iterrows():\n",
    "#     if row[1][\"verb\"] in spec_dict:\n",
    "#         for poss_pattern in spec_dict[row[1][\"verb\"]]:\n",
    "#             if set(poss_pattern.split()).intersection(row[1][\"text\"].split()) == set(poss_pattern.split()):\n",
    "#                 triples.append([row[1][\"text\"], f\"<triplet> {row[1]['subj']} <subj> {row[1]['obj']} <obj> {spec_code[poss_pattern]}\"])\n",
    "#     elif row[1][\"verb\"] in verb_dict:\n",
    "#             triples.append([row[1][\"text\"], f\"<triplet> {row[1]['subj']} <subj> {row[1]['obj']} <obj> {verb_dict[row[1]['verb']]}\"])\n",
    "\n",
    "triples = []\n",
    "ma_df = pd.DataFrame(matches, columns = [\"text\",\"subj\", \"verb\",\"obj\"])\n",
    "for row in ma_df.iterrows():\n",
    "    if row[1][\"verb\"] in spec_dict:\n",
    "        for poss_pattern in spec_dict[row[1][\"verb\"]]:\n",
    "            if set(poss_pattern.split()).intersection(row[1][\"text\"].split()) == set(poss_pattern.split()):\n",
    "                triples.append([row[1][\"text\"], row[1]['subj'], row[1]['obj'] , spec_code[poss_pattern]])\n",
    "    elif row[1][\"verb\"] in verb_dict:\n",
    "            triples.append([row[1][\"text\"], row[1]['subj'], row[1]['obj'], verb_dict[row[1]['verb']]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "huh = pd.DataFrame(triples, columns = [\"text\", \"subj\", \"obj\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2117"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huh.text.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7156"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/out_data/articles_url_coref3.csv.done.csv\", index_col = 0)\n",
    "df.text.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11:44pm: UN grain coordinator expects loaded s...</td>\n",
       "      <td>&lt;triplet&gt; UN grain coordinator &lt;subj&gt; loaded s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"Russia must clearly understand that Russia wi...</td>\n",
       "      <td>&lt;triplet&gt; Russia &lt;subj&gt; Russia &lt;obj&gt; Consult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8:04pm: Pro-Moscow force renews evacuation of ...</td>\n",
       "      <td>&lt;triplet&gt; the Kherson region &lt;subj&gt; a counter ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Wagner and its alleged boss Yevgeny Prigozhin...</td>\n",
       "      <td>&lt;triplet&gt; terror &lt;subj&gt; Ukraine &lt;obj&gt; Make Pub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12:04pm Kremlin accuses UK of ‘directing and c...</td>\n",
       "      <td>&lt;triplet&gt; Nord Stream blastsThe Kremlin &lt;subj&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8433</th>\n",
       "      <td>then President Donald Trump added then Preside...</td>\n",
       "      <td>&lt;triplet&gt; President Donald Trump &lt;subj&gt; the ap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8434</th>\n",
       "      <td>Taiwan's main opposition party the Kuomintang,...</td>\n",
       "      <td>&lt;triplet&gt; Taiwan's main opposition party &lt;subj...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8435</th>\n",
       "      <td>Kanye West hints at another presidential run  ...</td>\n",
       "      <td>&lt;triplet&gt; Kanye West &lt;subj&gt; another presidenti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8436</th>\n",
       "      <td>Credit:BloombergChina continues to send every ...</td>\n",
       "      <td>&lt;triplet&gt; Credit &lt;subj&gt; BloombergChina &lt;obj&gt; P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8437</th>\n",
       "      <td>A solitary grey wolf (or Canis lupus a species...</td>\n",
       "      <td>&lt;triplet&gt; that &lt;subj&gt; the Eurasian wolf &lt;obj&gt; ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8438 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  \\\n",
       "0     11:44pm: UN grain coordinator expects loaded s...   \n",
       "1     \"Russia must clearly understand that Russia wi...   \n",
       "2     8:04pm: Pro-Moscow force renews evacuation of ...   \n",
       "3     \"Wagner and its alleged boss Yevgeny Prigozhin...   \n",
       "4     12:04pm Kremlin accuses UK of ‘directing and c...   \n",
       "...                                                 ...   \n",
       "8433  then President Donald Trump added then Preside...   \n",
       "8434  Taiwan's main opposition party the Kuomintang,...   \n",
       "8435  Kanye West hints at another presidential run  ...   \n",
       "8436  Credit:BloombergChina continues to send every ...   \n",
       "8437  A solitary grey wolf (or Canis lupus a species...   \n",
       "\n",
       "                                                  label  \n",
       "0     <triplet> UN grain coordinator <subj> loaded s...  \n",
       "1          <triplet> Russia <subj> Russia <obj> Consult  \n",
       "2     <triplet> the Kherson region <subj> a counter ...  \n",
       "3     <triplet> terror <subj> Ukraine <obj> Make Pub...  \n",
       "4     <triplet> Nord Stream blastsThe Kremlin <subj>...  \n",
       "...                                                 ...  \n",
       "8433  <triplet> President Donald Trump <subj> the ap...  \n",
       "8434  <triplet> Taiwan's main opposition party <subj...  \n",
       "8435  <triplet> Kanye West <subj> another presidenti...  \n",
       "8436  <triplet> Credit <subj> BloombergChina <obj> P...  \n",
       "8437  <triplet> that <subj> the Eurasian wolf <obj> ...  \n",
       "\n",
       "[8438 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"len\"] = df.text.apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot: >"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df.boxplot(\"len\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "#split combined triplets into single triplets\n",
    "new = []\n",
    "for row in df.iterrows():\n",
    "    split = re.split(\"<\\w*>\", row[1][\"label\"])[1:] #first one is empty\n",
    "    for i in range(int(len(split)/3)): #always pairs of 3\n",
    "        sub = split[i*3:i*3+3]\n",
    "        new.append([row[1][\"text\"], sub[0].lstrip().rstrip(), sub[1].lstrip().rstrip(), sub[2].lstrip().rstrip()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "huh = pd.DataFrame(new, columns = [\"text\", \"subj\", \"obj\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>subj</th>\n",
       "      <th>obj</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11:44pm: UN grain coordinator expects loaded s...</td>\n",
       "      <td>UN grain coordinator</td>\n",
       "      <td>loaded ships</td>\n",
       "      <td>Consult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11:44pm: UN grain coordinator expects loaded s...</td>\n",
       "      <td>UN grain coordinator</td>\n",
       "      <td>loaded ships</td>\n",
       "      <td>Consult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11:44pm: UN grain coordinator expects loaded s...</td>\n",
       "      <td>UN grain coordinator</td>\n",
       "      <td>loaded ships</td>\n",
       "      <td>Consult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"Russia must clearly understand that Russia wi...</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Russia</td>\n",
       "      <td>Consult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8:04pm: Pro-Moscow force renews evacuation of ...</td>\n",
       "      <td>the Kherson region</td>\n",
       "      <td>a counter</td>\n",
       "      <td>Reduce Relations</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11014</th>\n",
       "      <td>Taiwan's main opposition party the Kuomintang,...</td>\n",
       "      <td>Taiwan's main opposition party</td>\n",
       "      <td>the Kuomintang</td>\n",
       "      <td>Engage In Diplomatic Cooperation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11015</th>\n",
       "      <td>Kanye West hints at another presidential run  ...</td>\n",
       "      <td>Kanye West</td>\n",
       "      <td>another presidential run                      ...</td>\n",
       "      <td>Appeal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11016</th>\n",
       "      <td>Credit:BloombergChina continues to send every ...</td>\n",
       "      <td>Credit</td>\n",
       "      <td>BloombergChina</td>\n",
       "      <td>Provide Aid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11017</th>\n",
       "      <td>Credit:BloombergChina continues to send every ...</td>\n",
       "      <td>Credit</td>\n",
       "      <td>BloombergChina</td>\n",
       "      <td>Provide Aid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11018</th>\n",
       "      <td>A solitary grey wolf (or Canis lupus a species...</td>\n",
       "      <td>that</td>\n",
       "      <td>the Eurasian wolf</td>\n",
       "      <td>Yield</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11019 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  \\\n",
       "0      11:44pm: UN grain coordinator expects loaded s...   \n",
       "1      11:44pm: UN grain coordinator expects loaded s...   \n",
       "2      11:44pm: UN grain coordinator expects loaded s...   \n",
       "3      \"Russia must clearly understand that Russia wi...   \n",
       "4      8:04pm: Pro-Moscow force renews evacuation of ...   \n",
       "...                                                  ...   \n",
       "11014  Taiwan's main opposition party the Kuomintang,...   \n",
       "11015  Kanye West hints at another presidential run  ...   \n",
       "11016  Credit:BloombergChina continues to send every ...   \n",
       "11017  Credit:BloombergChina continues to send every ...   \n",
       "11018  A solitary grey wolf (or Canis lupus a species...   \n",
       "\n",
       "                                 subj  \\\n",
       "0                UN grain coordinator   \n",
       "1                UN grain coordinator   \n",
       "2                UN grain coordinator   \n",
       "3                              Russia   \n",
       "4                  the Kherson region   \n",
       "...                               ...   \n",
       "11014  Taiwan's main opposition party   \n",
       "11015                      Kanye West   \n",
       "11016                          Credit   \n",
       "11017                          Credit   \n",
       "11018                            that   \n",
       "\n",
       "                                                     obj  \\\n",
       "0                                           loaded ships   \n",
       "1                                           loaded ships   \n",
       "2                                           loaded ships   \n",
       "3                                                 Russia   \n",
       "4                                              a counter   \n",
       "...                                                  ...   \n",
       "11014                                     the Kuomintang   \n",
       "11015  another presidential run                      ...   \n",
       "11016                                     BloombergChina   \n",
       "11017                                     BloombergChina   \n",
       "11018                                  the Eurasian wolf   \n",
       "\n",
       "                                  label  \n",
       "0                               Consult  \n",
       "1                               Consult  \n",
       "2                               Consult  \n",
       "3                               Consult  \n",
       "4                      Reduce Relations  \n",
       "...                                 ...  \n",
       "11014  Engage In Diplomatic Cooperation  \n",
       "11015                            Appeal  \n",
       "11016                       Provide Aid  \n",
       "11017                       Provide Aid  \n",
       "11018                             Yield  \n",
       "\n",
       "[11019 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7156"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "huh.text.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "cuda_ = \"cuda\" #all GPUS, specify specific GPUs with \"cuda:0\"\n",
    "device = torch.device(cuda_ if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "nli_model = AutoModelForSequenceClassification.from_pretrained('facebook/bart-large-mnli')\n",
    "nli_model.to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained('facebook/bart-large-mnli')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/werner/thesis_valentin/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2296: FutureWarning: The `truncation_strategy` argument is deprecated and will be removed in a future version, use `truncation=True` to truncate examples to a max length. You can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to truncate to the maximal input size of the model (e.g. 512 for Bert).  If you have pairs of inputs, you can give a specific truncation strategy selected among `truncation='only_first'` (will only truncate the first sentence in the pairs) `truncation='only_second'` (will only truncate the second sentence in the pairs) or `truncation='longest_first'` (will iteratively remove tokens from the longest sentence in the pairs).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "probs_l = []\n",
    "for row in huh.iterrows():\n",
    "    premise = row[1][\"text\"]\n",
    "    subj = row[1][\"subj\"]\n",
    "    rel = row[1][\"label\"]\n",
    "    obj =  row[1][\"obj\"]\n",
    "\n",
    "    hypothesis = f'{subj} does {rel} towards {obj}.'\n",
    "\n",
    "    # run through model pre-trained on MNLI\n",
    "    x = tokenizer.encode(premise, hypothesis, return_tensors='pt',\n",
    "                        truncation_strategy='only_first')\n",
    "    logits = nli_model(x.to(device))[0]\n",
    "\n",
    "    # we throw away \"neutral\" (dim 1) and take the probability of\n",
    "    # \"entailment\" (2) as the probability of the label being true \n",
    "    entail_contradiction_logits = logits[:,[0,2]]\n",
    "    probs = entail_contradiction_logits.softmax(dim=1)\n",
    "    prob_label_is_true = probs[:,1]\n",
    "\n",
    "    probs_l.append([row[0], prob_label_is_true.item()])\n",
    "if device != \"cpu\": torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "huh = pd.merge(huh.reset_index(), pd.DataFrame(probs_l, columns = [\"index\", \"prob\"]), on = \"index\")\n",
    "huh = huh.drop_duplicates([\"text\", \"prob\", \"label\"], keep = \"first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4959, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec = huh[huh.prob < 0.7]\n",
    "dec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2685, 6)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep = huh[huh.prob > 0.7]\n",
    "keep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2685, 6)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2601"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep.text.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>text</th>\n",
       "      <th>subj</th>\n",
       "      <th>obj</th>\n",
       "      <th>label</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>11:44pm: UN grain coordinator expects loaded s...</td>\n",
       "      <td>UN grain coordinator</td>\n",
       "      <td>loaded ships</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.979625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>8:04pm: Pro-Moscow force renews evacuation of ...</td>\n",
       "      <td>which</td>\n",
       "      <td>70,000 people</td>\n",
       "      <td>Yield</td>\n",
       "      <td>0.979495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>\"Wagner and its alleged boss Yevgeny Prigozhin...</td>\n",
       "      <td>terror</td>\n",
       "      <td>Ukraine</td>\n",
       "      <td>Make Public Statement</td>\n",
       "      <td>0.880736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Finland and Sweden asked to join the NATO defe...</td>\n",
       "      <td>Finland</td>\n",
       "      <td>the NATO defence alliance</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.874101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>In a post on Telegram, Vladimir Saldo, the Rus...</td>\n",
       "      <td>civilians</td>\n",
       "      <td>an additional 15 kilometres</td>\n",
       "      <td>Reduce Relations</td>\n",
       "      <td>0.745262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>the country to hold 5th election in 3 years as...</td>\n",
       "      <td>a powerful new player</td>\n",
       "      <td>a vote</td>\n",
       "      <td>Consult</td>\n",
       "      <td>0.875621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>The ultranationalist Religious Zionism party, ...</td>\n",
       "      <td>whose provocative top candidate</td>\n",
       "      <td>Arab legislators</td>\n",
       "      <td>Coerce</td>\n",
       "      <td>0.941591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>“Pakistan can serve as the manufacturing base ...</td>\n",
       "      <td>Pakistan</td>\n",
       "      <td>the manufacturing base</td>\n",
       "      <td>Engage In Material Cooperation</td>\n",
       "      <td>0.769296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>In Bangkok for the latest leg of Foreign Affai...</td>\n",
       "      <td>Foreign Affairs Minister Penny Wong</td>\n",
       "      <td>Bangkok</td>\n",
       "      <td>Engage In Diplomatic Cooperation</td>\n",
       "      <td>0.991021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>Prime Minister Han Duck-soo said South Korea's...</td>\n",
       "      <td>Prime Minister Han Duck-soo</td>\n",
       "      <td>South Korea</td>\n",
       "      <td>Make Public Statement</td>\n",
       "      <td>0.741399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    index                                               text  \\\n",
       "0       0  11:44pm: UN grain coordinator expects loaded s...   \n",
       "5       5  8:04pm: Pro-Moscow force renews evacuation of ...   \n",
       "6       6  \"Wagner and its alleged boss Yevgeny Prigozhin...   \n",
       "11     11  Finland and Sweden asked to join the NATO defe...   \n",
       "13     13  In a post on Telegram, Vladimir Saldo, the Rus...   \n",
       "14     14  the country to hold 5th election in 3 years as...   \n",
       "17     17  The ultranationalist Religious Zionism party, ...   \n",
       "21     21  “Pakistan can serve as the manufacturing base ...   \n",
       "22     22  In Bangkok for the latest leg of Foreign Affai...   \n",
       "24     24  Prime Minister Han Duck-soo said South Korea's...   \n",
       "\n",
       "                                   subj                          obj  \\\n",
       "0                  UN grain coordinator                 loaded ships   \n",
       "5                                 which                70,000 people   \n",
       "6                                terror                      Ukraine   \n",
       "11                              Finland    the NATO defence alliance   \n",
       "13                            civilians  an additional 15 kilometres   \n",
       "14                a powerful new player                       a vote   \n",
       "17      whose provocative top candidate             Arab legislators   \n",
       "21                             Pakistan       the manufacturing base   \n",
       "22  Foreign Affairs Minister Penny Wong                      Bangkok   \n",
       "24          Prime Minister Han Duck-soo                  South Korea   \n",
       "\n",
       "                               label      prob  \n",
       "0                            Consult  0.979625  \n",
       "5                              Yield  0.979495  \n",
       "6              Make Public Statement  0.880736  \n",
       "11                           Consult  0.874101  \n",
       "13                  Reduce Relations  0.745262  \n",
       "14                           Consult  0.875621  \n",
       "17                            Coerce  0.941591  \n",
       "21    Engage In Material Cooperation  0.769296  \n",
       "22  Engage In Diplomatic Cooperation  0.991021  \n",
       "24             Make Public Statement  0.741399  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keep.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "last_txt = \"\"\n",
    "for row in keep.iterrows():\n",
    "    trip = f\"<triplet> {row[1]['subj']} <subj> {row[1]['obj']} <obj> {row[1]['label']}\"\n",
    "    if row[1][\"text\"] == last_txt:\n",
    "        res[-1] = [res[-1][0], res[-1][1] + trip]\n",
    "    else:\n",
    "        res.append([row[1][\"text\"], trip])\n",
    "    last_txt = row[1][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2601, 2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new = pd.DataFrame(res, columns = [\"text\",\"label\"])\n",
    "df_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.to_csv(\"data/out_data/entail_articles_url_coref3.csv.done.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11:44pm: UN grain coordinator expects loaded s...</td>\n",
       "      <td>&lt;triplet&gt; UN grain coordinator &lt;subj&gt; loaded s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8:04pm: Pro-Moscow force renews evacuation of ...</td>\n",
       "      <td>&lt;triplet&gt; which &lt;subj&gt; 70,000 people &lt;obj&gt; Yield</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Wagner and its alleged boss Yevgeny Prigozhin...</td>\n",
       "      <td>&lt;triplet&gt; terror &lt;subj&gt; Ukraine &lt;obj&gt; Make Pub...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Finland and Sweden asked to join the NATO defe...</td>\n",
       "      <td>&lt;triplet&gt; Finland &lt;subj&gt; the NATO defence alli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>In a post on Telegram, Vladimir Saldo, the Rus...</td>\n",
       "      <td>&lt;triplet&gt; civilians &lt;subj&gt; an additional 15 ki...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  11:44pm: UN grain coordinator expects loaded s...   \n",
       "1  8:04pm: Pro-Moscow force renews evacuation of ...   \n",
       "2  \"Wagner and its alleged boss Yevgeny Prigozhin...   \n",
       "3  Finland and Sweden asked to join the NATO defe...   \n",
       "4  In a post on Telegram, Vladimir Saldo, the Rus...   \n",
       "\n",
       "                                               label  \n",
       "0  <triplet> UN grain coordinator <subj> loaded s...  \n",
       "1   <triplet> which <subj> 70,000 people <obj> Yield  \n",
       "2  <triplet> terror <subj> Ukraine <obj> Make Pub...  \n",
       "3  <triplet> Finland <subj> the NATO defence alli...  \n",
       "4  <triplet> civilians <subj> an additional 15 ki...  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8752286434173584"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "premise = \"In early November, Olaf Scholz will be traveling to Beijing for the first time as German chancellor, and despite the ongoing debate about the German economy’s unsustainable dependence on China, Olaf Scholz will likely bring along a significant delegation of German executives\"\n",
    "subj = \"Olaf Scholz\"\n",
    "rel = \"Consult\"\n",
    "obj =  \"Beijing\"\n",
    "\n",
    "hypothesis = f'{subj} does {rel} towards {obj}.'\n",
    "\n",
    "# run through model pre-trained on MNLI\n",
    "x = tokenizer.encode(premise, hypothesis, return_tensors='pt',\n",
    "                    truncation_strategy='only_first')\n",
    "logits = nli_model(x)[0]\n",
    "\n",
    "entail_contradiction_logits = logits[:,[0,2]]\n",
    "probs = entail_contradiction_logits.softmax(dim=1)\n",
    "prob_label_is_true = probs[:,1]\n",
    "\n",
    "prob_label_is_true.item()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('thesis_valentin': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a1350fe81f24607af7015099983099ac829ebf1f4acb969c2cf14b7230b10f2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
