{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/werner/thesis_valentin/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized Pentacode data\n",
      "initialized Pentacode data\n",
      "train shape (2577, 2)\n",
      "val shape (251, 2)\n",
      "test shape (224, 2)\n"
     ]
    }
   ],
   "source": [
    "#Set right ontology up here!!\n",
    "ontology = \"pentacode\" #and make sure its the correct ontology for your checkpoint!\n",
    "stage = \"finetune\"\n",
    "seed = 0\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "import sys\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from typing import Any, Union, List, Optional, Sequence\n",
    "\n",
    "import re\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Callback, LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "\n",
    "import datasets\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "\n",
    "import transformers \n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "import wandb\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "\n",
    "cameo_to_penta = {\n",
    "    \"Make Public Statement\" : \"make a statement\",\n",
    "    \"Appeal\" : \"make a statement\", \n",
    "    \"Express Intent to Cooperate\" : \"verbal cooperation\",\n",
    "    \"Consult\" : \"verbal cooperation\",\n",
    "    \"Engage In Diplomatic Cooperation\" : \"verbal cooperation\",\n",
    "    \"Engage In Material Cooperation\" : \"material cooperation\",\n",
    "    \"Provide Aid\" : \"material cooperation\",\n",
    "    \"Yield\" : \"material cooperation\", \n",
    "    \"Investigate\" : \"verbal conflict\",\n",
    "    \"Demand\" : \"verbal conflict\",\n",
    "    \"Disapprove\" : \"verbal conflict\",\n",
    "    \"Reject\" : \"verbal conflict\",\n",
    "    \"Threaten\" : \"verbal conflict\",\n",
    "    \"Exhibit Military Posture\" : \"material conflict\",\n",
    "    \"Protest\" : \"material conflict\", \n",
    "    \"Reduce Relations\" : \"verbal conflict\",\n",
    "    \"Coerce\" : \"material conflict\",\n",
    "    \"Assault\" : \"material conflict\",\n",
    "    \"Fight\" : \"material conflict\",\n",
    "    \"Engage in unconventional mass violence\" : \"material conflict\"\n",
    "}\n",
    "\n",
    "\n",
    "if stage == \"pretrain\":\n",
    "    #for pre-train data\n",
    "    train = pd.read_csv(f\"data_src/unsupervised/train_{seed}.csv\", index_col = 0)\n",
    "    train = train.rename(columns = {\"label\":\"triplets\"})\n",
    "    val = pd.read_csv(f\"data_src/unsupervised/val_{seed}.csv\", index_col = 0)\n",
    "    val = val.rename(columns = {\"label\":\"triplets\"})\n",
    "    test = pd.read_csv(f\"data_src/unsupervised/test_{seed}.csv\", index_col = 0)\n",
    "    test = test.rename(columns = {\"label\":\"triplets\"})\n",
    "\n",
    "elif stage == \"finetune\":\n",
    "    #for fine-tune data\n",
    "    train = pd.read_csv(f\"data_src/annotated/new_train_aug_0.csv\", index_col = 0)\n",
    "    train = train.rename(columns = {\"label\":\"triplets\"})\n",
    "    val = pd.read_csv(f\"data_src/annotated/new_val_aug_0.csv\", index_col = 0)\n",
    "    val = val.rename(columns = {\"label\":\"triplets\"})\n",
    "    test = pd.read_csv(f\"data_src/annotated/new_test_aug_0.csv\", index_col = 0)\n",
    "    test = test.rename(columns = {\"label\":\"triplets\"})\n",
    "\n",
    "if ontology == \"pentacode\":\n",
    "    #map CAMEO labels to the respective Pentacode labels\n",
    "    penta_map = []\n",
    "    for row in train.iterrows():\n",
    "        trip_text = row[1][\"triplets\"]\n",
    "        for key in cameo_to_penta.keys():\n",
    "            if key in row[1][\"triplets\"]: trip_text = trip_text.replace(key, cameo_to_penta[key])\n",
    "        penta_map.append([row[1][\"text\"], trip_text])\n",
    "    train = pd.DataFrame(penta_map, columns = [\"text\",\"triplets\"])\n",
    "\n",
    "    penta_map = []\n",
    "    for row in val.iterrows():\n",
    "        trip_text = row[1][\"triplets\"]\n",
    "        for key in cameo_to_penta.keys():\n",
    "            if key in row[1][\"triplets\"]: trip_text = trip_text.replace(key, cameo_to_penta[key])\n",
    "        penta_map.append([row[1][\"text\"], trip_text])\n",
    "    val = pd.DataFrame(penta_map, columns = [\"text\",\"triplets\"])\n",
    "\n",
    "    penta_map = []\n",
    "    for row in test.iterrows():\n",
    "        trip_text = row[1][\"triplets\"]\n",
    "        for key in cameo_to_penta.keys():\n",
    "            if key in row[1][\"triplets\"]: trip_text = trip_text.replace(key, cameo_to_penta[key])\n",
    "        penta_map.append([row[1][\"text\"], trip_text])\n",
    "    test = pd.DataFrame(penta_map, columns = [\"text\",\"triplets\"])\n",
    "    print(\"initialized Pentacode data\")\n",
    "\n",
    "if ontology == \"pentacode\":\n",
    "    penta_map = []\n",
    "    for row in train.iterrows():\n",
    "        trip_text = row[1][\"triplets\"]\n",
    "        for key in cameo_to_penta.keys():\n",
    "            if key in row[1][\"triplets\"]: trip_text = trip_text.replace(key, cameo_to_penta[key])\n",
    "        penta_map.append([row[1][\"text\"], trip_text])\n",
    "    train = pd.DataFrame(penta_map, columns = [\"text\",\"triplets\"])\n",
    "\n",
    "    penta_map = []\n",
    "    for row in val.iterrows():\n",
    "        trip_text = row[1][\"triplets\"]\n",
    "        for key in cameo_to_penta.keys():\n",
    "            if key in row[1][\"triplets\"]: trip_text = trip_text.replace(key, cameo_to_penta[key])\n",
    "        penta_map.append([row[1][\"text\"], trip_text])\n",
    "    val = pd.DataFrame(penta_map, columns = [\"text\",\"triplets\"])\n",
    "\n",
    "    penta_map = []\n",
    "    for row in test.iterrows():\n",
    "        trip_text = row[1][\"triplets\"]\n",
    "        for key in cameo_to_penta.keys():\n",
    "            if key in row[1][\"triplets\"]: trip_text = trip_text.replace(key, cameo_to_penta[key])\n",
    "        penta_map.append([row[1][\"text\"], trip_text])\n",
    "    test = pd.DataFrame(penta_map, columns = [\"text\",\"triplets\"])\n",
    "\n",
    "    print(\"initialized Pentacode data\")\n",
    "\n",
    "print(\"train shape\", train.shape)\n",
    "print(\"val shape\", val.shape)\n",
    "print(\"test shape\", test.shape)\n",
    "\n",
    "data_dict = {\n",
    "    \"train\": train, \n",
    "    \"val\": val, \n",
    "    \"test\": test\n",
    "}\n",
    "\n",
    "\n",
    "#In[3]: Define config\n",
    "class conf:\n",
    "    #general\n",
    "    seed = 0\n",
    "    gpus = [1] #1 for any one GPU; [1] for choosing GPU#1\n",
    "    ontology = ontology #cameo or pentacode\n",
    "    \n",
    "    #input\n",
    "    batch_size = 32\n",
    "    max_length = 128\n",
    "    ignore_pad_token_for_loss = True\n",
    "    use_fast_tokenizer = True\n",
    "    gradient_acc_steps = 1\n",
    "    if ontology == \"pentacode\":\n",
    "        gradient_clip_value = 3\n",
    "    else:\n",
    "        gradient_clip_value = 10\n",
    "    load_workers = 50 #50 is 5/8 of plato\n",
    "    masking = 1 #after how many epoch should new masking be applied, 0 = no masking\n",
    "\n",
    "    #optimizer\n",
    "    lr = 0.0001\n",
    "    lr_decay = 0.1\n",
    "    weight_decay = 0.015\n",
    "    eps_loss = 0.15 #for label smoothed loss\n",
    "    warm_up = 1 #num of epochs to warm_up on\n",
    "\n",
    "\n",
    "#In[5]: Pytorch Lightning DataModule\n",
    "\n",
    "class GetData(pl.LightningDataModule):\n",
    "    def __init__(self, conf: conf, tokenizer: AutoTokenizer, model: AutoModelForSeq2SeqLM):\n",
    "        \"\"\"initialize params from config\"\"\"\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.conf = conf\n",
    "        self.datasets = data_dict\n",
    "        \n",
    "        # Data collator\n",
    "        label_pad_token_id = -100                       \n",
    "        self.data_collator = DataCollatorForSeq2Seq(self.tokenizer, self.model, label_pad_token_id=label_pad_token_id, padding = True)\n",
    "\n",
    "    def preprocess_function(self, data):\n",
    "        \"\"\"tokenize, pad, truncate\"\"\"\n",
    "        #split into input and labels\n",
    "        inputs = data[\"text\"]       \n",
    "        outputs = data[\"triplets\"]\n",
    "\n",
    "        #process input\n",
    "        model_inputs = self.tokenizer(inputs, max_length = 128, padding = True, truncation = True)\n",
    "\n",
    "        #process labels\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            labels = self.tokenizer(outputs, max_length = 128, padding = True, truncation = True)\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    def apply_mask(self, data):        \n",
    "        #masking entities in sentence\n",
    "            print(\"applying mask ...\")\n",
    "            new = []\n",
    "            for row in data.iterrows():\n",
    "                split = re.split(\"<\\w*>\", row[1][\"triplets\"])[1:]   #first one is empty\n",
    "                for i in range(int(len(split)/3)):                  #always pairs of 3\n",
    "                    sub = split[i*3:i*3+3]\n",
    "                    subj = sub[0]\n",
    "                    obj = sub[1]             \n",
    "                    if np.random.binomial(1, 0.00, 1) == 1:                  \n",
    "                        tok = np.random.choice(sub).rstrip().lstrip()               \n",
    "                        new.append([row[1][\"text\"].replace(tok, \"<MASK>\"), row[1][\"triplets\"].replace(tok, \"<MASK>\")])                    \n",
    "                        break\n",
    "                    else:\n",
    "                        new.append([row[1][\"text\"], row[1][\"triplets\"]])\n",
    "                    \n",
    "            df = pd.DataFrame(new, columns = [\"text\", \"triplets\"])\n",
    "            df = df.drop_duplicates(\"text\")\n",
    "            \n",
    "            ds = datasets.Dataset.from_pandas(df)\n",
    "            ds = ds.remove_columns([\"__index_level_0__\"])\n",
    "            return ds \n",
    "        \n",
    "    #apply the preprocessing and load data\n",
    "    def train_dataloader(self, *args, **kwargs): \n",
    "        self.train_dataset = self.datasets[\"train\"]\n",
    "        self.train_dataset = self.apply_mask(self.train_dataset)\n",
    "        self.train_dataset = self.train_dataset.map(self.preprocess_function, remove_columns = [\"text\", \"triplets\"], batched = True)\n",
    "        return DataLoader(self.train_dataset, batch_size = self.conf.batch_size, collate_fn = self.data_collator, shuffle = True, num_workers= 50)\n",
    "    \n",
    "    def val_dataloader(self, *args, **kwargs): \n",
    "        self.val_dataset = self.datasets[\"val\"]\n",
    "        self.val_dataset = datasets.Dataset.from_pandas(self.val_dataset)\n",
    "        if conf.ontology != \"pentacode\":\n",
    "            self.val_dataset = self.val_dataset.remove_columns([\"__index_level_0__\"])\n",
    "        self.val_dataset = self.val_dataset.map(self.preprocess_function, remove_columns = [\"text\", \"triplets\"], batched = True)\n",
    "        return DataLoader(self.val_dataset, batch_size = self.conf.batch_size, collate_fn = self.data_collator, num_workers= 50)\n",
    "\n",
    "    def test_dataloader(self, *args, **kwargs): \n",
    "        self.test_dataset = self.datasets[\"test\"]\n",
    "        self.test_dataset = datasets.Dataset.from_pandas(self.test_dataset)\n",
    "        if conf.ontology != \"pentacode\":\n",
    "            self.test_dataset = self.test_dataset.remove_columns([\"__index_level_0__\"])\n",
    "        self.test_dataset = self.test_dataset.map(self.preprocess_function,  remove_columns = [\"text\", \"triplets\"], batched = True)\n",
    "        return DataLoader(self.test_dataset, batch_size = self.conf.batch_size, collate_fn = self.data_collator, num_workers= 50)\n",
    "\n",
    "#In[6]: Pytorch Lightning Base module\n",
    "\n",
    "class BaseModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, conf, config: AutoConfig, tokenizer: AutoTokenizer, model: AutoModelForSeq2SeqLM):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.conf = conf\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ontology = ontology\n",
    "        self.eps_loss = conf.eps_loss\n",
    "        self.loss_fn = label_smoothed_nll_loss \n",
    "        #self.loss_fn = torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        self.num_beams = 3\n",
    "\n",
    "    def forward(self, inputs, labels, *args):\n",
    "        ##### Check later if smooth labeled loss is better \n",
    "        outputs = self.model(**inputs, labels = labels, use_cache = False, return_dict = True, output_hidden_states = True)\n",
    "        output_dict = {'loss': outputs['loss'], 'logits': outputs['logits']}\n",
    "        return output_dict\n",
    "\n",
    "    def training_step(self, batch: dict, batch_idx: int):\n",
    "        labels = batch.pop(\"labels\")\n",
    "        labels_original = labels.clone()\n",
    "        batch[\"decoder_input_ids\"] = torch.where(labels != -100, labels, self.config.pad_token_id)\n",
    "        labels = shift_tokens_left(labels, -100)\n",
    "\n",
    "        forward_output = self.forward(batch, labels)\n",
    "        self.log('loss', forward_output['loss'])\n",
    "\n",
    "        batch[\"labels\"] = labels_original\n",
    "\n",
    "        forward_output['tr_loss'] = forward_output['loss'].mean().detach()\n",
    "        if labels.shape[-1] < 128:\n",
    "            forward_output['labels'] = self._pad_tensors_to_max_len(labels, 128)\n",
    "        else:\n",
    "            forward_output['labels'] = labels\n",
    "        \n",
    "        outputs = {}\n",
    "        outputs['predictions'], outputs['labels'] = self.generate_triples(batch, labels)\n",
    "        outputs[\"loss\"] = forward_output['loss']\n",
    "        return outputs\n",
    "\n",
    "        #return forward_output['loss']\n",
    "\n",
    "    def validation_step(self, batch: dict, batch_idx):\n",
    "        labels = batch.pop(\"labels\")\n",
    "        batch[\"decoder_input_ids\"] = torch.where(labels != -100, labels, self.config.pad_token_id)\n",
    "        labels = shift_tokens_left(labels, -100)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # compute loss on predict data\n",
    "            forward_output = self.forward(batch, labels)\n",
    "\n",
    "        forward_output['loss'] = forward_output['loss'].mean().detach()\n",
    "        forward_output['logits'] = forward_output['logits'].detach()\n",
    "\n",
    "        if labels.shape[-1] < 128:\n",
    "            forward_output['labels'] = self._pad_tensors_to_max_len(labels, 128)\n",
    "        else:\n",
    "            forward_output['labels'] = labels\n",
    "\n",
    "        metrics = {}\n",
    "        metrics['val_loss'] = forward_output['loss']\n",
    "\n",
    "        for key in sorted(metrics.keys()):\n",
    "            self.log(key, metrics[key])\n",
    "\n",
    "        outputs = {}\n",
    "        outputs['predictions'], outputs['labels'] = self.generate_triples(batch, labels)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        labels = batch.pop(\"labels\")\n",
    "        batch[\"decoder_input_ids\"] = torch.where(labels != -100, labels, self.config.pad_token_id)\n",
    "        labels = shift_tokens_left(labels, -100)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # compute loss on predict data\n",
    "            forward_output = self.forward(batch, labels)\n",
    "\n",
    "        forward_output['loss'] = forward_output['loss'].mean().detach()\n",
    "\n",
    "        forward_output['logits'] = forward_output['logits'].detach()\n",
    "\n",
    "        if labels.shape[-1] < 128:\n",
    "            forward_output['labels'] = self._pad_tensors_to_max_len(labels, 128)\n",
    "        else:\n",
    "            forward_output['labels'] = labels\n",
    "\n",
    "\n",
    "        metrics = {}\n",
    "        metrics['test_loss'] = forward_output['loss']\n",
    "\n",
    "        for key in sorted(metrics.keys()):\n",
    "            self.log(key, metrics[key], prog_bar=True)\n",
    "        \n",
    "        #### what does this actually do? how does this change everything\n",
    "        # if self.hparams.finetune:\n",
    "        #     return {'predictions': self.forward_samples(batch, labels)}\n",
    "        # else:\n",
    "\n",
    "        outputs = {}\n",
    "        outputs['predictions'], outputs['labels'] = self.generate_triples(batch, labels)\n",
    "        return outputs\n",
    "\n",
    "    def training_epoch_end(self, output: dict):\n",
    "        \n",
    "        print(\"\\n\\n train eval\\n\")\n",
    "        \n",
    "        scores, class_scores= re_score([item for pred in output for item in pred['predictions']], [item for pred in output for item in pred['labels']])\n",
    "\n",
    "        self.log('train_prec_micro', scores[\"ALL\"][\"p\"]) \n",
    "        self.log('train_recall_micro', scores[\"ALL\"][\"r\"])\n",
    "        self.log('train_F1_micro', scores[\"ALL\"][\"f1\"])\n",
    "\n",
    "        self.log(\"train_prec_macro\", scores[\"ALL\"][\"Macro_p\"])\n",
    "        self.log(\"train_recall_macro\", scores[\"ALL\"][\"Macro_r\"])\n",
    "        self.log(\"train_F1_macro\", scores[\"ALL\"][\"Macro_f1\"])\n",
    "\n",
    "        self.log('train_prec_micro_class', class_scores[\"ALL\"][\"p\"])\n",
    "        self.log('train_recall_micro_class', class_scores[\"ALL\"][\"r\"])\n",
    "        self.log('train_F1_micro_class', class_scores[\"ALL\"][\"f1\"])\n",
    "\n",
    "        self.log(\"train_prec_macro_class\", class_scores[\"ALL\"][\"Macro_p\"])\n",
    "        self.log(\"train_recall_macro_class\", class_scores[\"ALL\"][\"Macro_r\"])\n",
    "        self.log(\"train_F1_macro_class\", class_scores[\"ALL\"][\"Macro_f1\"])\n",
    "\n",
    "    def validation_epoch_end(self, output: dict):\n",
    "\n",
    "        print(\"\\n\\n validation eval\\n\")\n",
    "       \n",
    "        scores, class_scores = re_score([item for pred in output for item in pred['predictions']], [item for pred in output for item in pred['labels']])\n",
    "        \n",
    "        self.log('val_prec_micro', scores[\"ALL\"][\"p\"]) \n",
    "        self.log('val_recall_micro', scores[\"ALL\"][\"r\"])\n",
    "        self.log('val_F1_micro', scores[\"ALL\"][\"f1\"])\n",
    "\n",
    "        self.log(\"val_prec_macro\", scores[\"ALL\"][\"Macro_p\"])\n",
    "        self.log(\"val_recall_macro\", scores[\"ALL\"][\"Macro_r\"])\n",
    "        self.log(\"val_F1_macro\", scores[\"ALL\"][\"Macro_f1\"])\n",
    "\n",
    "        self.log('val_prec_micro_class', class_scores[\"ALL\"][\"p\"])\n",
    "        self.log('val_recall_micro_class', class_scores[\"ALL\"][\"r\"])\n",
    "        self.log('val_F1_micro_class', class_scores[\"ALL\"][\"f1\"])\n",
    "\n",
    "        self.log(\"val_prec_macro_class\", class_scores[\"ALL\"][\"Macro_p\"])\n",
    "        self.log(\"val_recall_macro_class\", class_scores[\"ALL\"][\"Macro_r\"])\n",
    "        self.log(\"val_F1_macro_class\", class_scores[\"ALL\"][\"Macro_f1\"])\n",
    "\n",
    "    def test_epoch_end(self, output: dict):\n",
    "       \n",
    "        scores, class_scores = re_score([item for pred in output for item in pred['predictions']], [item for pred in output for item in pred['labels']])\n",
    "\n",
    "        self.log('test_prec_micro', scores[\"ALL\"][\"p\"]) \n",
    "        self.log('test_recall_micro', scores[\"ALL\"][\"r\"])\n",
    "        self.log('test_F1_micro', scores[\"ALL\"][\"f1\"])\n",
    "\n",
    "        self.log(\"test_prec_macro\", scores[\"ALL\"][\"Macro_p\"])\n",
    "        self.log(\"test_recall_macro\", scores[\"ALL\"][\"Macro_r\"])\n",
    "        self.log(\"test_F1_macro\", scores[\"ALL\"][\"Macro_f1\"])\n",
    "\n",
    "        self.log('test_prec_micro_class', class_scores[\"ALL\"][\"p\"])\n",
    "        self.log('test_recall_micro_class', class_scores[\"ALL\"][\"r\"])\n",
    "        self.log('test_F1_micro_class', class_scores[\"ALL\"][\"f1\"])\n",
    "\n",
    "        self.log(\"test_prec_macro_class\", class_scores[\"ALL\"][\"Macro_p\"])\n",
    "        self.log(\"test_recall_macro_class\", class_scores[\"ALL\"][\"Macro_r\"])\n",
    "        self.log(\"test_F1_macro_class\", class_scores[\"ALL\"][\"Macro_f1\"])\n",
    "\n",
    "\n",
    "    # additional functions called in main functions\n",
    "\n",
    "    def generate_triples(self, batch, labels) -> None:\n",
    "\n",
    "        generated_tokens = self.model.generate(\n",
    "            batch[\"input_ids\"].to(self.model.device),\n",
    "            attention_mask=batch[\"attention_mask\"].to(self.model.device),\n",
    "            use_cache = True, max_length = 128, early_stopping = False, length_penalty = 0, \n",
    "            no_repeat_ngram_size = 0, num_beams = 3)\n",
    "\n",
    "        decoded_preds = self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
    "        decoded_labels = self.tokenizer.batch_decode(torch.where(labels != -100, labels, self.config.pad_token_id), skip_special_tokens=False)\n",
    "\n",
    "        return [extract_triplets(rel) for rel in decoded_preds], [extract_triplets(rel) for rel in decoded_labels]\n",
    "\n",
    "    def _pad_tensors_to_max_len(self, tensor, max_length):\n",
    "        # If PAD token is not defined at least EOS token has to be defined\n",
    "        pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else self.config.eos_token_id\n",
    "\n",
    "        if pad_token_id is None:\n",
    "            raise ValueError(\n",
    "                f\"Make sure that either `config.pad_token_id` or `config.eos_token_id` is defined if tensor has to be padded to `max_length`={max_length}\"\n",
    "            )\n",
    "\n",
    "        padded_tensor = pad_token_id * torch.ones(\n",
    "            (tensor.shape[0], max_length), dtype=tensor.dtype, device=tensor.device\n",
    "        )\n",
    "        padded_tensor[:, : tensor.shape[-1]] = tensor\n",
    "        return padded_tensor\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        ##### HUH\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.conf.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr = self.conf.lr, betas = (0.9, 0.999), eps = 0.00000001, weight_decay = self.conf.weight_decay)\n",
    "\n",
    "        def lr_schedule(epoch):\n",
    "            k = self.conf.lr_decay\n",
    "            if epoch < 1: lr_scale =  0.1\n",
    "            else: lr_scale = 1 * math.exp(-k*epoch)\n",
    "            return lr_scale\n",
    "\n",
    "        scheduler = lr_scheduler.LambdaLR(\n",
    "            optimizer,\n",
    "            lr_lambda=lr_schedule\n",
    "        )\n",
    "        #scheduler = inverse_square_root(optimizer, num_warmup_steps= conf.warmup_steps)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "#In[7]: Helper functions\n",
    "\n",
    "#FROM https://github.com/facebookresearch/fairseq/blob/main/fairseq/criterions/label_smoothed_cross_entropy.py\n",
    "def label_smoothed_nll_loss(lprobs, target, ignore_index=-100):\n",
    "    \"\"\"From fairseq\"\"\"\n",
    "    eps_loss = self.conf.eps_loss\n",
    "    if target.dim() == lprobs.dim() - 1:\n",
    "        target = target.unsqueeze(-1)\n",
    "    nll_loss = -lprobs.gather(dim=-1, index=target)\n",
    "    smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
    "    if ignore_index is not None:\n",
    "        pad_mask = target.eq(ignore_index)\n",
    "        nll_loss.masked_fill_(pad_mask, 0.0)\n",
    "        smooth_loss.masked_fill_(pad_mask, 0.0)\n",
    "    else:\n",
    "        nll_loss = nll_loss.squeeze(-1)\n",
    "        smooth_loss = smooth_loss.squeeze(-1)\n",
    "\n",
    "    nll_loss = nll_loss.sum()  \n",
    "    smooth_loss = smooth_loss.sum()\n",
    "    eps_i = eps_loss / lprobs.size(-1)\n",
    "    loss = (1.0 - eps_loss) * nll_loss + eps_i * smooth_loss\n",
    "    return loss, nll_loss\n",
    "\n",
    "#from REBEL\n",
    "class GenerateTextSamplesCallback(Callback):\n",
    "    \"\"\"\n",
    "    PL Callback to generate triplets along training\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, logging_batch_interval):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logging_batch_interval: How frequently to inspect/potentially plot something\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.logging_batch_interval = logging_batch_interval\n",
    "\n",
    "    def on_train_batch_end(self,trainer: Trainer,pl_module: LightningModule, outputs: Sequence, batch: Sequence, batch_idx: int) -> None:\n",
    "        wandb_table = wandb.Table(columns=[\"Source\", \"Pred\", \"Gold\"])\n",
    "        # pl_module.logger.info(\"Executing translation callback\")\n",
    "        labels = batch.pop(\"labels\")\n",
    "        gen_kwargs = {\n",
    "            \"max_length\": 128,\n",
    "            \"early_stopping\": False,\n",
    "            \"no_repeat_ngram_size\": 0,\n",
    "            \"num_beams\": 3\n",
    "        }\n",
    "        pl_module.eval()\n",
    "\n",
    "        decoder_inputs = torch.roll(labels, 1, 1)[:,0:2]\n",
    "        decoder_inputs[:, 0] = 0\n",
    "        generated_tokens = pl_module.model.generate(\n",
    "            batch[\"input_ids\"].to(pl_module.model.device),\n",
    "            attention_mask=batch[\"attention_mask\"].to(pl_module.model.device),\n",
    "            decoder_input_ids=decoder_inputs.to(pl_module.model.device),\n",
    "            **gen_kwargs,\n",
    "        )\n",
    "        # in case the batch is shorter than max length, the output should be padded\n",
    "        if generated_tokens.shape[-1] < gen_kwargs[\"max_length\"]:\n",
    "            generated_tokens = pl_module._pad_tensors_to_max_len(generated_tokens, gen_kwargs[\"max_length\"])\n",
    "        pl_module.train()\n",
    "        decoded_preds = pl_module.tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
    "\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = torch.where(labels != -100, labels, pl_module.tokenizer.pad_token_id)\n",
    "\n",
    "        decoded_labels = pl_module.tokenizer.batch_decode(labels, skip_special_tokens=False)\n",
    "        decoded_inputs = pl_module.tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=False)\n",
    "\n",
    "        for source, translation, gold_output in zip(decoded_inputs, decoded_preds, decoded_labels):\n",
    "            wandb_table.add_data(\n",
    "                source.replace('[PAD]', '').replace(\"[SEP]\",\"\"), \n",
    "                translation.replace('[PAD]', '').replace(\"[SEP]\",\"\"), \n",
    "                gold_output.replace('[PAD]', '').replace(\"[SEP]\",\"\")\n",
    "            )\n",
    "        pl_module.logger.experiment.log({\"Triplets\": wandb_table})\n",
    "\n",
    "#from REBEL\n",
    "def shift_tokens_left(input_ids: torch.Tensor, pad_token_id: int):\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right.\n",
    "    \"\"\"\n",
    "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "    shifted_input_ids[:, :-1] = input_ids[:, 1:].clone()\n",
    "    shifted_input_ids[:, -1] = pad_token_id\n",
    "\n",
    "    assert pad_token_id is not None, \"self.model.config.pad_token_id has to be defined.\"\n",
    "\n",
    "    return shifted_input_ids\n",
    "\n",
    "#from REBEL\n",
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"[CLS]\", \"\").replace(\"[PAD]\", \"\").replace(\"[SEP]\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "    return triplets\n",
    "\n",
    "#from REBEL\n",
    "'''Adapted from: https://github.com/btaille/sincere/blob/6f5472c5aeaf7ef7765edf597ede48fdf1071712/code/utils/evaluation.py'''\n",
    "def re_score(pred_relations, gt_relations):\n",
    "    \"\"\"Evaluate RE predictions\n",
    "    Args:\n",
    "        pred_relations (list) :  list of list of predicted relations (several relations in each sentence)\n",
    "        gt_relations (list) :    list of list of ground truth relations\n",
    "            rel = { \"head\": (start_idx (inclusive), end_idx (exclusive)),\n",
    "                    \"tail\": (start_idx (inclusive), end_idx (exclusive)),\n",
    "                    \"head_type\": ent_type,\n",
    "                    \"tail_type\": ent_type,\n",
    "                    \"type\": rel_type}\n",
    "        vocab (Vocab) :         dataset vocabulary\n",
    "        mode (str) :            in 'strict' or 'boundaries' \"\"\"\n",
    "\n",
    "    if ontology == \"pentacode\":\n",
    "        relation_types = [\"make a statement\", \"verbal cooperation\", \"material cooperation\", \"verbal conflict\", \"material conflict\"]\n",
    "    else:\n",
    "        if stage == \"finetune\":\n",
    "            relation_types = [\"make public statement\",\"appeal\",\"express intend to cooperate\",\"consult\",\"engage in diplomatic cooperation\",\n",
    "            \"engage in material cooperation\",\"provide aid\",\"yield\",\"investigate\",\"demand\",\"disapprove\",\"threaten\",\n",
    "            \"exhibit military posture\",\"reject\",\n",
    "            \"protest\",\"reduce relations\",\"coerce\",\"assault\",\"fight\"]#\"engage in unconvential mass violence\"]\n",
    "        elif stage == \"pretrain\":\n",
    "            relation_types = [\"make public statement\",\"appeal\",\"express intend to cooperate\",\"consult\",\"engage in diplomatic cooperation\",\n",
    "            \"engage in material cooperation\",\"provide aid\",\"yield\",\"investigate\",\"demand\",\"disapprove\",\"threaten\",\n",
    "            #\"exhibit military posture\",\"reject\",\n",
    "            \"protest\",\"reduce relations\",\"coerce\",\"assault\",\"fight\"]#\"engage in unconvential mass violence\"]\n",
    "      \n",
    "    scores = {rel: {\"tp\": 0, \"fp\": 0, \"fn\": 0} for rel in relation_types + [\"ALL\"]}\n",
    "    class_scores = {rel: {\"tp\": 0, \"fp\": 0, \"fn\": 0} for rel in relation_types + [\"ALL\"]}\n",
    "\n",
    "    # Count GT relations and Predicted relations\n",
    "    n_sents = len(gt_relations)\n",
    "    n_rels = sum([len([rel for rel in sent]) for sent in gt_relations])\n",
    "    n_found = sum([len([rel for rel in sent]) for sent in pred_relations])\n",
    "\n",
    "    # Count TP, FP and FN per type\n",
    "    for pred_sent, gt_sent in zip(pred_relations, gt_relations):\n",
    "        for rel_type in relation_types:\n",
    "            pred_rels = {(rel[\"head\"], rel[\"tail\"]) for rel in pred_sent if rel[\"type\"] == rel_type}\n",
    "            gt_rels = {(rel[\"head\"], rel[\"tail\"]) for rel in gt_sent if rel[\"type\"] == rel_type}\n",
    "\n",
    "            scores[rel_type][\"tp\"] += len(pred_rels & gt_rels)\n",
    "            scores[rel_type][\"fp\"] += len(pred_rels - gt_rels)\n",
    "            scores[rel_type][\"fn\"] += len(gt_rels - pred_rels)\n",
    "\n",
    "            class_pred = [rel[\"type\"] for rel in pred_sent if rel[\"type\"] == rel_type]\n",
    "            class_label = [rel[\"type\"] for rel in gt_sent if rel[\"type\"] == rel_type]\n",
    "\n",
    "            class_scores[rel_type][\"tp\"] += min(len(class_pred), len(class_label))\n",
    "            class_scores[rel_type][\"fp\"] += max(len(class_pred)-len(class_label),0)\n",
    "            class_scores[rel_type][\"fn\"] += max(len(class_label)-len(class_pred),0)\n",
    "\n",
    "\n",
    "    # Compute per triplet Precision / Recall / F1\n",
    "    for rel_type in scores.keys():\n",
    "        if scores[rel_type][\"tp\"]:\n",
    "            scores[rel_type][\"p\"] = 100 * scores[rel_type][\"tp\"] / (scores[rel_type][\"fp\"] + scores[rel_type][\"tp\"])\n",
    "            scores[rel_type][\"r\"] = 100 * scores[rel_type][\"tp\"] / (scores[rel_type][\"fn\"] + scores[rel_type][\"tp\"])\n",
    "        else:\n",
    "            scores[rel_type][\"p\"], scores[rel_type][\"r\"] = 0, 0\n",
    "\n",
    "        if not scores[rel_type][\"p\"] + scores[rel_type][\"r\"] == 0:\n",
    "            scores[rel_type][\"f1\"] = 2 * scores[rel_type][\"p\"] * scores[rel_type][\"r\"] / (scores[rel_type][\"p\"] + scores[rel_type][\"r\"])\n",
    "        else:\n",
    "            scores[rel_type][\"f1\"] = 0\n",
    "\n",
    "    # Compute per class Precision / Recall / F1\n",
    "    for rel_type in scores.keys():\n",
    "        if class_scores[rel_type][\"tp\"]:\n",
    "            class_scores[rel_type][\"p\"] = 100 * class_scores[rel_type][\"tp\"] / (class_scores[rel_type][\"fp\"] + class_scores[rel_type][\"tp\"])\n",
    "            class_scores[rel_type][\"r\"] = 100 * class_scores[rel_type][\"tp\"] / (class_scores[rel_type][\"fn\"] + class_scores[rel_type][\"tp\"])\n",
    "        else:\n",
    "            class_scores[rel_type][\"p\"], class_scores[rel_type][\"r\"] = 0, 0\n",
    "\n",
    "        if not class_scores[rel_type][\"p\"] + class_scores[rel_type][\"r\"] == 0:\n",
    "            class_scores[rel_type][\"f1\"] = 2 * class_scores[rel_type][\"p\"] * class_scores[rel_type][\"r\"] / (class_scores[rel_type][\"p\"] + class_scores[rel_type][\"r\"])\n",
    "        else:\n",
    "            class_scores[rel_type][\"f1\"] = 0\n",
    "\n",
    "    # Compute micro F1 Scores, relations\n",
    "    tp = sum([scores[rel_type][\"tp\"] for rel_type in relation_types])\n",
    "    fp = sum([scores[rel_type][\"fp\"] for rel_type in relation_types])\n",
    "    fn = sum([scores[rel_type][\"fn\"] for rel_type in relation_types])\n",
    "\n",
    "    if tp:\n",
    "        precision = 100 * tp / (tp + fp)\n",
    "        recall = 100 * tp / (tp + fn)\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    else:\n",
    "        precision, recall, f1 = 0, 0, 0\n",
    "\n",
    "    scores[\"ALL\"][\"p\"] = precision\n",
    "    scores[\"ALL\"][\"r\"] = recall\n",
    "    scores[\"ALL\"][\"f1\"] = f1\n",
    "    scores[\"ALL\"][\"tp\"] = tp\n",
    "    scores[\"ALL\"][\"fp\"] = fp\n",
    "    scores[\"ALL\"][\"fn\"] = fn\n",
    "\n",
    "    # Compute micro F1 Scores, classes\n",
    "    class_tp = sum([class_scores[rel_type][\"tp\"] for rel_type in relation_types])\n",
    "    class_fp = sum([class_scores[rel_type][\"fp\"] for rel_type in relation_types])\n",
    "    class_fn = sum([class_scores[rel_type][\"fn\"] for rel_type in relation_types])\n",
    "\n",
    "    if class_tp:\n",
    "        class_precision = 100 * class_tp / (class_tp + class_fp)\n",
    "        class_recall = 100 * class_tp / (class_tp + class_fn)\n",
    "        class_f1 = 2 * class_precision * class_recall / (class_precision + class_recall)\n",
    "\n",
    "    else:\n",
    "        class_precision, class_recall, class_f1 = 0, 0, 0\n",
    "\n",
    "    class_scores[\"ALL\"][\"p\"] = class_precision\n",
    "    class_scores[\"ALL\"][\"r\"] = class_recall\n",
    "    class_scores[\"ALL\"][\"f1\"] = class_f1\n",
    "    class_scores[\"ALL\"][\"tp\"] = class_tp\n",
    "    class_scores[\"ALL\"][\"fp\"] = class_fp\n",
    "    class_scores[\"ALL\"][\"fn\"] = class_fn\n",
    "\n",
    "    # Compute Macro F1 Scores, relations\n",
    "    scores[\"ALL\"][\"Macro_f1\"] = np.mean([scores[rel_type][\"f1\"] for rel_type in relation_types])\n",
    "    scores[\"ALL\"][\"Macro_p\"] = np.mean([scores[rel_type][\"p\"] for rel_type in relation_types])\n",
    "    scores[\"ALL\"][\"Macro_r\"] = np.mean([scores[rel_type][\"r\"] for rel_type in relation_types])\n",
    "\n",
    "    print(f\"Full Triplet Evaluation\")\n",
    "    print(\n",
    "        \"processed {} sentences with {} relations; found: {} relations; correct: {}.\".format(n_sents, n_rels, n_found,tp))\n",
    "    print(\n",
    "        \"\\tALL\\t TP: {};\\tFP: {};\\tFN: {}\".format(\n",
    "            scores[\"ALL\"][\"tp\"],\n",
    "            scores[\"ALL\"][\"fp\"],\n",
    "            scores[\"ALL\"][\"fn\"]))\n",
    "    print(\n",
    "        \"\\t\\t(m avg): precision: {:.2f};\\trecall: {:.2f};\\tf1: {:.2f} (micro)\".format(\n",
    "            precision,\n",
    "            recall,\n",
    "            f1))\n",
    "    print(\n",
    "        \"\\t\\t(M avg): precision: {:.2f};\\trecall: {:.2f};\\tf1: {:.2f} (Macro)\\n\".format(\n",
    "            scores[\"ALL\"][\"Macro_p\"],\n",
    "            scores[\"ALL\"][\"Macro_r\"],\n",
    "            scores[\"ALL\"][\"Macro_f1\"]))\n",
    "\n",
    "    for rel_type in relation_types:\n",
    "        print(\"\\t{}: \\tTP: {};\\tFP: {};\\tFN: {};\\tprecision: {:.2f};\\trecall: {:.2f};\\tf1: {:.2f};\\t{}\".format(\n",
    "            rel_type,\n",
    "            scores[rel_type][\"tp\"],\n",
    "            scores[rel_type][\"fp\"],\n",
    "            scores[rel_type][\"fn\"],\n",
    "            scores[rel_type][\"p\"],\n",
    "            scores[rel_type][\"r\"],\n",
    "            scores[rel_type][\"f1\"],\n",
    "            scores[rel_type][\"tp\"] +\n",
    "            scores[rel_type][\"fp\"]))\n",
    "\n",
    "    # Compute Macro F1 Scores, relations\n",
    "    class_scores[\"ALL\"][\"Macro_f1\"] = np.mean([class_scores[rel_type][\"f1\"] for rel_type in relation_types])\n",
    "    class_scores[\"ALL\"][\"Macro_p\"] = np.mean([class_scores[rel_type][\"p\"] for rel_type in relation_types])\n",
    "    class_scores[\"ALL\"][\"Macro_r\"] = np.mean([class_scores[rel_type][\"r\"] for rel_type in relation_types])\n",
    "\n",
    "    print(f\"Relation Classification Evaluation\")\n",
    "\n",
    "    print(\n",
    "        \"processed {} sentences with {} relations; found: {} relations; correct: {}.\".format(n_sents, n_rels, n_found,class_tp))\n",
    "    print(\n",
    "        \"\\tALL\\t TP: {};\\tFP: {};\\tFN: {}\".format(\n",
    "            class_scores[\"ALL\"][\"tp\"],\n",
    "            class_scores[\"ALL\"][\"fp\"],\n",
    "            class_scores[\"ALL\"][\"fn\"]))\n",
    "    print(\n",
    "        \"\\t\\t(m avg): precision: {:.2f};\\trecall: {:.2f};\\tf1: {:.2f} (micro)\".format(\n",
    "            class_precision,\n",
    "            class_recall,\n",
    "            class_f1))\n",
    "    print(\n",
    "        \"\\t\\t(M avg): precision: {:.2f};\\trecall: {:.2f};\\tf1: {:.2f} (Macro)\\n\".format(\n",
    "            class_scores[\"ALL\"][\"Macro_p\"],\n",
    "            class_scores[\"ALL\"][\"Macro_r\"],\n",
    "            class_scores[\"ALL\"][\"Macro_f1\"]))\n",
    "\n",
    "    for rel_type in relation_types:\n",
    "        print(\"\\t{}: \\tTP: {};\\tFP: {};\\tFN: {};\\tprecision: {:.2f};\\trecall: {:.2f};\\tf1: {:.2f};\\t{}\".format(\n",
    "            rel_type,\n",
    "            class_scores[rel_type][\"tp\"],\n",
    "            class_scores[rel_type][\"fp\"],\n",
    "            class_scores[rel_type][\"fn\"],\n",
    "            class_scores[rel_type][\"p\"],\n",
    "            class_scores[rel_type][\"r\"],\n",
    "            class_scores[rel_type][\"f1\"],\n",
    "            class_scores[rel_type][\"tp\"] +\n",
    "            class_scores[rel_type][\"fp\"]))\n",
    "\n",
    "    return scores, class_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_size: 32 \n",
      " learning rate: 0.0001 \n",
      " learning rate decay: 0.1 \n",
      " weight decay: 0.015 \n",
      " epsilon loss: 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at snowood1/ConfliBERT-scr-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at snowood1/ConfliBERT-scr-uncased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at snowood1/ConfliBERT-scr-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.key.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The following encoder weights were not tied to the decoder ['bert/pooler']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(30008, 768)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(0)\n",
    "\n",
    "print(f\" batch_size: {conf.batch_size} \\n learning rate: {conf.lr} \\n learning rate decay: {conf.lr_decay} \\n weight decay: {conf.weight_decay} \\n epsilon loss: {conf.eps_loss}\")\n",
    "\n",
    "add_tokens = [\"<obj>\", \"<subj>\", \"<triplet>\", \"<head>\", \"</head>\", \"<tail>\", \"</tail>\", \"<MASK>\"]\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"snowood1/ConfliBERT-scr-uncased\", use_fast = True,\n",
    "        additional_special_tokens = add_tokens)\n",
    "model = transformers.EncoderDecoderModel.from_encoder_decoder_pretrained(\"snowood1/ConfliBERT-scr-uncased\",\"snowood1/ConfliBERT-scr-uncased\", tie_encoder_decoder = True)\n",
    "\n",
    "\n",
    "config = model.config\n",
    "config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "config.eos_token_id = tokenizer.sep_token_id\n",
    "config.pad_token_id = tokenizer.pad_token_id\n",
    "config.vocab_size = config.encoder.vocab_size\n",
    "\n",
    "model.encoder.resize_token_embeddings(len(tokenizer))\n",
    "model.decoder.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30008"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ckpt = torch.load(\"models/ConfliBERT_pretrain_Pentacode/epoch=29-step=14970.ckpt\")\n",
    "\n",
    "state_dict = {}\n",
    "for key in ckpt[\"state_dict\"].keys():\n",
    "    state_dict[key[6:]] = ckpt[\"state_dict\"][key]\n",
    "\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "100%|| 1/1 [00:00<00:00, 14.60ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/7 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/werner/thesis_valentin/lib/python3.8/site-packages/transformers/models/encoder_decoder/modeling_encoder_decoder.py:530: FutureWarning: Version v4.12.0 introduces a better way to train encoder-decoder models by computing the loss inside the encoder-decoder framework rather than in the decoder itself. You may observe training discrepancies if fine-tuning a model trained with versions anterior to 4.12.0. The decoder_input_ids are now created based on the labels, no need to pass them yourself anymore.\n",
      "  warnings.warn(DEPRECATION_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|| 7/7 [00:07<00:00,  1.12s/it]Full Triplet Evaluation\n",
      "processed 224 sentences with 498 relations; found: 221 relations; correct: 32.\n",
      "\tALL\t TP: 32;\tFP: 187;\tFN: 448\n",
      "\t\t(m avg): precision: 14.61;\trecall: 6.67;\tf1: 9.16 (micro)\n",
      "\t\t(M avg): precision: 14.91;\trecall: 6.80;\tf1: 8.88 (Macro)\n",
      "\n",
      "\tmake a statement: \tTP: 9;\tFP: 37;\tFN: 147;\tprecision: 19.57;\trecall: 5.77;\tf1: 8.91;\t46\n",
      "\tverbal cooperation: \tTP: 2;\tFP: 44;\tFN: 33;\tprecision: 4.35;\trecall: 5.71;\tf1: 4.94;\t46\n",
      "\tmaterial cooperation: \tTP: 5;\tFP: 31;\tFN: 49;\tprecision: 13.89;\trecall: 9.26;\tf1: 11.11;\t36\n",
      "\tverbal conflict: \tTP: 12;\tFP: 30;\tFN: 112;\tprecision: 28.57;\trecall: 9.68;\tf1: 14.46;\t42\n",
      "\tmaterial conflict: \tTP: 4;\tFP: 45;\tFN: 107;\tprecision: 8.16;\trecall: 3.60;\tf1: 5.00;\t49\n",
      "Relation Classification Evaluation\n",
      "processed 224 sentences with 498 relations; found: 221 relations; correct: 124.\n",
      "\tALL\t TP: 124;\tFP: 97;\tFN: 356\n",
      "\t\t(m avg): precision: 56.11;\trecall: 25.83;\tf1: 35.38 (micro)\n",
      "\t\t(M avg): precision: 56.16;\trecall: 26.70;\tf1: 34.65 (Macro)\n",
      "\n",
      "\tmake a statement: \tTP: 26;\tFP: 20;\tFN: 130;\tprecision: 56.52;\trecall: 16.67;\tf1: 25.74;\t46\n",
      "\tverbal cooperation: \tTP: 7;\tFP: 40;\tFN: 28;\tprecision: 14.89;\trecall: 20.00;\tf1: 17.07;\t47\n",
      "\tmaterial cooperation: \tTP: 19;\tFP: 17;\tFN: 35;\tprecision: 52.78;\trecall: 35.19;\tf1: 42.22;\t36\n",
      "\tverbal conflict: \tTP: 34;\tFP: 9;\tFN: 90;\tprecision: 79.07;\trecall: 27.42;\tf1: 40.72;\t43\n",
      "\tmaterial conflict: \tTP: 38;\tFP: 11;\tFN: 73;\tprecision: 77.55;\trecall: 34.23;\tf1: 47.50;\t49\n",
      "Testing DataLoader 0: 100%|| 7/7 [00:07<00:00,  1.13s/it]\n",
      "\n",
      "       Test metric             DataLoader 0\n",
      "\n",
      "      test_F1_macro           8.8836210260919\n",
      "   test_F1_macro_class       34.65130601712135\n",
      "      test_F1_micro          9.155937194824219\n",
      "   test_F1_micro_class       35.37803268432617\n",
      "        test_loss           10.673196792602539\n",
      "     test_prec_macro        14.907325248940158\n",
      "  test_prec_macro_class      56.16278435590258\n",
      "     test_prec_micro        14.611871719360352\n",
      "  test_prec_micro_class      56.10859680175781\n",
      "    test_recall_macro        6.804759740243611\n",
      " test_recall_macro_class    26.701088184959154\n",
      "    test_recall_micro        6.666666507720947\n",
      " test_recall_micro_class     25.83333396911621\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 10.673196792602539,\n",
       "  'test_prec_micro': 14.611871719360352,\n",
       "  'test_recall_micro': 6.666666507720947,\n",
       "  'test_F1_micro': 9.155937194824219,\n",
       "  'test_prec_macro': 14.907325248940158,\n",
       "  'test_recall_macro': 6.804759740243611,\n",
       "  'test_F1_macro': 8.8836210260919,\n",
       "  'test_prec_micro_class': 56.10859680175781,\n",
       "  'test_recall_micro_class': 25.83333396911621,\n",
       "  'test_F1_micro_class': 35.37803268432617,\n",
       "  'test_prec_macro_class': 56.16278435590258,\n",
       "  'test_recall_macro_class': 26.701088184959154,\n",
       "  'test_F1_macro_class': 34.65130601712135}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_data_module = GetData(conf, tokenizer, model)\n",
    "pl_module = BaseModule(conf, config, tokenizer, model)\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator = \"gpu\",\n",
    "    devices = [1],\n",
    "    accumulate_grad_batches=1,\n",
    "    gradient_clip_val= conf.gradient_clip_value,\n",
    "    max_epochs = 30,\n",
    "    min_epochs = 5,\n",
    "    reload_dataloaders_every_n_epochs = 1, \n",
    "    precision=16,\n",
    "    amp_level=None\n",
    ")\n",
    "\n",
    "trainer.test(pl_module, datamodule=pl_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_valentin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Jun 22 2022, 20:18:18) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a1350fe81f24607af7015099983099ac829ebf1f4acb969c2cf14b7230b10f2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
