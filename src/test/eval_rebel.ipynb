{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/werner/thesis_valentin/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import json\n",
    "import os\n",
    "os.chdir(\"..\")\n",
    "os.chdir(\"..\")\n",
    "import sys\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from typing import Any, Union, List, Optional, Sequence\n",
    "\n",
    "import re\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW, lr_scheduler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning import Callback, LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor\n",
    "\n",
    "import datasets\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "\n",
    "import transformers \n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    default_data_collator,\n",
    "    set_seed,\n",
    ")\n",
    "\n",
    "import wandb\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "\n",
    "cameo_to_penta = {\n",
    "    \"Make Public Statement\" : \"Make a statement\",\n",
    "    \"Appeal\" : \"Make a statement\", \n",
    "    \"Express Intent to Cooperate\" : \"Verbal Cooperation\",\n",
    "    \"Consult\" : \"Verbal Cooperation\",\n",
    "    \"Engage In Diplomatic Cooperation\" : \"Verbal Cooperation\",\n",
    "    \"Engage In Material Cooperation\" : \"Material Cooperation\",\n",
    "    \"Provide Aid\" : \"Material Cooperation\",\n",
    "    \"Yield\" : \"Material Cooperation\", \n",
    "    \"Investigate\" : \"Verbal Conflict\",\n",
    "    \"Demand\" : \"Verbal Conflict\",\n",
    "    \"Disapprove\" : \"Verbal Conflict\",\n",
    "    \"Reject\" : \"Verbal Conflict\",\n",
    "    \"Threaten\" : \"Verbal Conflict\",\n",
    "    \"Exhibit Military Posture\" : \"Material Conflict\",\n",
    "    \"Protest\" : \"Material Conflict\", \n",
    "    \"Reduce Relations\" : \"Verbal Conflict\",\n",
    "    \"Coerce\" : \"Material Conflict\",\n",
    "    \"Assault\" : \"Material Conflict\",\n",
    "    \"Fight\" : \"Material Conflict\",\n",
    "    \"Engage in unconventional mass violence\" : \"Material Conflict\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read data without augmentation\n",
      "train shape (1659, 2)\n",
      "val shape (254, 2)\n",
      "test shape (224, 2)\n"
     ]
    }
   ],
   "source": [
    "#Set right ontology up here!!\n",
    "ontology = \"cameo\" #and make sure its the correct ontology for your checkpoint!\n",
    "stage = \"finetune\"\n",
    "seed = 1\n",
    "\n",
    "if stage == \"pretrain\":\n",
    "    #for pre-train data\n",
    "    train = pd.read_csv(f\"data_src/unsupervised/train_{seed}.csv\", index_col = 0)\n",
    "    train = train.rename(columns = {\"label\":\"triplets\"})\n",
    "    val = pd.read_csv(f\"data_src/unsupervised/val_{seed}.csv\", index_col = 0)\n",
    "    val = val.rename(columns = {\"label\":\"triplets\"})\n",
    "    test = pd.read_csv(f\"data_src/unsupervised/test_{seed}.csv\", index_col = 0)\n",
    "    test = test.rename(columns = {\"label\":\"triplets\"})\n",
    "\n",
    "elif stage == \"finetune\":\n",
    "    #for fine-tune data\n",
    "    # train = pd.read_csv(f\"data_src/annotated/new_train_aug_{seed}.csv\", index_col = 0)\n",
    "    # train = train.rename(columns = {\"label\":\"triplets\"})\n",
    "    # val = pd.read_csv(f\"data_src/annotated/new_val_aug_{seed}.csv\", index_col = 0)\n",
    "    # val = val.rename(columns = {\"label\":\"triplets\"})\n",
    "    # test = pd.read_csv(f\"data_src/annotated/new_test_aug_{seed}.csv\", index_col = 0)\n",
    "    # test = test.rename(columns = {\"label\":\"triplets\"})\n",
    "\n",
    "    #data without aug\n",
    "    train = pd.read_csv(f\"data_src/annotated_noaug/train_{seed}.csv\", index_col = 0)[[\"text\",\"label\"]]\n",
    "    train = train.rename(columns = {\"label\":\"triplets\"})\n",
    "    val = pd.read_csv(f\"data_src/annotated_noaug/val_{seed}.csv\", index_col = 0)[[\"text\",\"label\"]]\n",
    "    val = val.rename(columns = {\"label\":\"triplets\"})\n",
    "    test = pd.read_csv(f\"data_src/annotated_noaug/test_{seed}.csv\", index_col = 0)[[\"text\",\"label\"]]\n",
    "    test = test.rename(columns = {\"label\":\"triplets\"})\n",
    "    print(\"read data without augmentation\")\n",
    "\n",
    "if ontology == \"pentacode\":\n",
    "    #map CAMEO labels to the respective Pentacode labels\n",
    "    penta_map = []\n",
    "    for row in train.iterrows():\n",
    "        trip_text = row[1][\"triplets\"]\n",
    "        for key in cameo_to_penta.keys():\n",
    "            if key in row[1][\"triplets\"]: trip_text = trip_text.replace(key, cameo_to_penta[key])\n",
    "        penta_map.append([row[1][\"text\"], trip_text])\n",
    "    train = pd.DataFrame(penta_map, columns = [\"text\",\"triplets\"])\n",
    "\n",
    "    penta_map = []\n",
    "    for row in val.iterrows():\n",
    "        trip_text = row[1][\"triplets\"]\n",
    "        for key in cameo_to_penta.keys():\n",
    "            if key in row[1][\"triplets\"]: trip_text = trip_text.replace(key, cameo_to_penta[key])\n",
    "        penta_map.append([row[1][\"text\"], trip_text])\n",
    "    val = pd.DataFrame(penta_map, columns = [\"text\",\"triplets\"])\n",
    "\n",
    "    penta_map = []\n",
    "    for row in test.iterrows():\n",
    "        trip_text = row[1][\"triplets\"]\n",
    "        for key in cameo_to_penta.keys():\n",
    "            if key in row[1][\"triplets\"]: trip_text = trip_text.replace(key, cameo_to_penta[key])\n",
    "        penta_map.append([row[1][\"text\"], trip_text])\n",
    "    test = pd.DataFrame(penta_map, columns = [\"text\",\"triplets\"])\n",
    "    print(\"initialized Pentacode data\")\n",
    "\n",
    "if ontology == \"pentacode\":\n",
    "    penta_map = []\n",
    "    for row in train.iterrows():\n",
    "        trip_text = row[1][\"triplets\"]\n",
    "        for key in cameo_to_penta.keys():\n",
    "            if key in row[1][\"triplets\"]: trip_text = trip_text.replace(key, cameo_to_penta[key])\n",
    "        penta_map.append([row[1][\"text\"], trip_text])\n",
    "    train = pd.DataFrame(penta_map, columns = [\"text\",\"triplets\"])\n",
    "\n",
    "    penta_map = []\n",
    "    for row in val.iterrows():\n",
    "        trip_text = row[1][\"triplets\"]\n",
    "        for key in cameo_to_penta.keys():\n",
    "            if key in row[1][\"triplets\"]: trip_text = trip_text.replace(key, cameo_to_penta[key])\n",
    "        penta_map.append([row[1][\"text\"], trip_text])\n",
    "    val = pd.DataFrame(penta_map, columns = [\"text\",\"triplets\"])\n",
    "\n",
    "    penta_map = []\n",
    "    for row in test.iterrows():\n",
    "        trip_text = row[1][\"triplets\"]\n",
    "        for key in cameo_to_penta.keys():\n",
    "            if key in row[1][\"triplets\"]: trip_text = trip_text.replace(key, cameo_to_penta[key])\n",
    "        penta_map.append([row[1][\"text\"], trip_text])\n",
    "    test = pd.DataFrame(penta_map, columns = [\"text\",\"triplets\"])\n",
    "\n",
    "    print(\"initialized Pentacode data\")\n",
    "\n",
    "print(\"train shape\", train.shape)\n",
    "print(\"val shape\", val.shape)\n",
    "print(\"test shape\", test.shape)\n",
    "\n",
    "data_dict = {\n",
    "    \"train\": train, \n",
    "    \"val\": val, \n",
    "    \"test\": test\n",
    "}\n",
    "\n",
    "#In[3]: Define config\n",
    "\n",
    "class conf:\n",
    "    #general\n",
    "    seed = 0\n",
    "    ontology = ontology #cameo or pentacode\n",
    "    \n",
    "    #input\n",
    "    batch_size = 32\n",
    "    max_length = 128\n",
    "    ignore_pad_token_for_loss = True\n",
    "    use_fast_tokenizer = True\n",
    "    gradient_acc_steps = 1\n",
    "    if ontology == \"pentacode\":\n",
    "        gradient_clip_value = 3\n",
    "    else:\n",
    "        gradient_clip_value = 10.0\n",
    "    load_workers = 50 #50 is 5/8 of plato\n",
    "    masking = 1 #after how many epoch should new masking be applied, 0 = no masking\n",
    "\n",
    "    #optimizer\n",
    "    lr = 0.000125\n",
    "    lr_decay = 0.2\n",
    "    weight_decay = 0.01\n",
    "    eps_loss = 0.1 #for label smoothed loss\n",
    "    warm_up = 1 #num of epochs to warm_up on\n",
    "\n",
    "\n",
    "\n",
    "#In[5]: Pytorch Lightning DataModule\n",
    "\n",
    "class GetData(pl.LightningDataModule):\n",
    "    def __init__(self, conf: conf, tokenizer: AutoTokenizer, model: AutoModelForSeq2SeqLM):\n",
    "        \"\"\"initialize params from config\"\"\"\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.conf = conf\n",
    "        self.datasets = data_dict\n",
    "        \n",
    "        # Data collator\n",
    "        label_pad_token_id = -100                       \n",
    "        self.data_collator = DataCollatorForSeq2Seq(self.tokenizer, self.model, label_pad_token_id=label_pad_token_id, padding = True)\n",
    "\n",
    "    def preprocess_function(self, data):\n",
    "        \"\"\"tokenize, pad, truncate\"\"\"\n",
    "        #split into input and labels\n",
    "        inputs = data[\"text\"]       \n",
    "        outputs = data[\"triplets\"]\n",
    "\n",
    "        #process input\n",
    "        model_inputs = self.tokenizer(inputs, max_length = 128, padding = True, truncation = True)\n",
    "\n",
    "        #process labels\n",
    "        with self.tokenizer.as_target_tokenizer():\n",
    "            labels = self.tokenizer(outputs, max_length = 128, padding = True, truncation = True)\n",
    "\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    def apply_mask(self, data):        \n",
    "        #masking entities in sentence\n",
    "            print(\"applying mask ...\")\n",
    "            new = []\n",
    "            for row in data.iterrows():\n",
    "                split = re.split(\"<\\w*>\", row[1][\"triplets\"])[1:]   #first one is empty\n",
    "                for i in range(int(len(split)/3)):                  #always pairs of 3\n",
    "                    sub = split[i*3:i*3+3]\n",
    "                    subj = sub[0]\n",
    "                    obj = sub[1]             \n",
    "                    if np.random.binomial(1, 0.00, 1) == 1:                  \n",
    "                        tok = np.random.choice(sub).rstrip().lstrip()               \n",
    "                        new.append([row[1][\"text\"].replace(tok, \"<MASK>\"), row[1][\"triplets\"].replace(tok, \"<MASK>\")])                    \n",
    "                        break\n",
    "                    else:\n",
    "                        new.append([row[1][\"text\"], row[1][\"triplets\"]])\n",
    "                    \n",
    "            df = pd.DataFrame(new, columns = [\"text\", \"triplets\"])\n",
    "            df = df.drop_duplicates(\"text\")\n",
    "            \n",
    "            ds = datasets.Dataset.from_pandas(df)\n",
    "            ds = ds.remove_columns([\"__index_level_0__\"])\n",
    "            return ds \n",
    "        \n",
    "    #apply the preprocessing and load data\n",
    "    def train_dataloader(self, *args, **kwargs): \n",
    "        self.train_dataset = self.datasets[\"train\"]\n",
    "        self.train_dataset = self.apply_mask(self.train_dataset)\n",
    "        self.train_dataset = self.train_dataset.map(self.preprocess_function, remove_columns = [\"text\", \"triplets\"], batched = True)\n",
    "        return DataLoader(self.train_dataset, batch_size = self.conf.batch_size, collate_fn = self.data_collator, shuffle = True, num_workers= 50)\n",
    "    \n",
    "    def val_dataloader(self, *args, **kwargs): \n",
    "        self.val_dataset = self.datasets[\"val\"]\n",
    "        self.val_dataset = datasets.Dataset.from_pandas(self.val_dataset)\n",
    "        if ontology != \"pentacode\":\n",
    "            self.val_dataset = self.val_dataset.remove_columns([\"__index_level_0__\"])\n",
    "        self.val_dataset = self.val_dataset.map(self.preprocess_function, remove_columns = [\"text\", \"triplets\"], batched = True)\n",
    "        return DataLoader(self.val_dataset, batch_size = self.conf.batch_size, collate_fn = self.data_collator, num_workers= 50)\n",
    "\n",
    "    def test_dataloader(self, *args, **kwargs): \n",
    "        self.test_dataset = self.datasets[\"test\"]\n",
    "        self.test_dataset = datasets.Dataset.from_pandas(self.test_dataset)\n",
    "        if ontology != \"pentacode\":\n",
    "            self.test_dataset = self.test_dataset.remove_columns([\"__index_level_0__\"])\n",
    "        self.test_dataset = self.test_dataset.map(self.preprocess_function,  remove_columns = [\"text\", \"triplets\"], batched = True)\n",
    "        return DataLoader(self.test_dataset, batch_size = self.conf.batch_size, collate_fn = self.data_collator, num_workers= 50)\n",
    "\n",
    "#In[6]: Pytorch Lightning Base module\n",
    "\n",
    "class BaseModule(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, conf, config: AutoConfig, tokenizer: AutoTokenizer, model: AutoModelForSeq2SeqLM):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.conf = conf\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.ontology = conf.ontology\n",
    "        self.eps_loss = conf.eps_loss\n",
    "        self.loss_fn = label_smoothed_nll_loss #torch.nn.CrossEntropyLoss(ignore_index=-100)\n",
    "        self.num_beams = 3\n",
    "\n",
    "    def forward(self, inputs, labels, *args):\n",
    "        ##### Check later if smooth labeled loss is better \n",
    "        outputs = self.model(**inputs, labels = labels, use_cache = False, return_dict = True, output_hidden_states = True)\n",
    "        output_dict = {'loss': outputs['loss'], 'logits': outputs['logits']}\n",
    "        return output_dict\n",
    "\n",
    "    def training_step(self, batch: dict, batch_idx: int):\n",
    "        labels = batch.pop(\"labels\")\n",
    "        labels_original = labels.clone()\n",
    "        batch[\"decoder_input_ids\"] = torch.where(labels != -100, labels, self.config.pad_token_id)\n",
    "        labels = shift_tokens_left(labels, -100)\n",
    "\n",
    "        forward_output = self.forward(batch, labels)\n",
    "        self.log('loss', forward_output['loss'])\n",
    "\n",
    "        batch[\"labels\"] = labels_original\n",
    "\n",
    "        forward_output['tr_loss'] = forward_output['loss'].mean().detach()\n",
    "        if labels.shape[-1] < 128:\n",
    "            forward_output['labels'] = self._pad_tensors_to_max_len(labels, 128)\n",
    "        else:\n",
    "            forward_output['labels'] = labels\n",
    "        \n",
    "        outputs = {}\n",
    "        outputs['predictions'], outputs['labels'] = self.generate_triples(batch, labels)\n",
    "        outputs[\"loss\"] = forward_output['loss']\n",
    "        return outputs\n",
    "\n",
    "    def validation_step(self, batch: dict, batch_idx):\n",
    "        labels = batch.pop(\"labels\")\n",
    "        batch[\"decoder_input_ids\"] = torch.where(labels != -100, labels, self.config.pad_token_id)\n",
    "        labels = shift_tokens_left(labels, -100)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # compute loss on predict data\n",
    "            forward_output = self.forward(batch, labels)\n",
    "\n",
    "        forward_output['loss'] = forward_output['loss'].mean().detach()\n",
    "        forward_output['logits'] = forward_output['logits'].detach()\n",
    "\n",
    "        if labels.shape[-1] < 128:\n",
    "            forward_output['labels'] = self._pad_tensors_to_max_len(labels, 128)\n",
    "        else:\n",
    "            forward_output['labels'] = labels\n",
    "\n",
    "        metrics = {}\n",
    "        metrics['val_loss'] = forward_output['loss']\n",
    "\n",
    "        for key in sorted(metrics.keys()):\n",
    "            self.log(key, metrics[key])\n",
    "\n",
    "        outputs = {}\n",
    "        outputs['predictions'], outputs['labels'] = self.generate_triples(batch, labels)\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        labels = batch.pop(\"labels\")\n",
    "        batch[\"decoder_input_ids\"] = torch.where(labels != -100, labels, self.config.pad_token_id)\n",
    "        labels = shift_tokens_left(labels, -100)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # compute loss on predict data\n",
    "            forward_output = self.forward(batch, labels)\n",
    "\n",
    "        forward_output['loss'] = forward_output['loss'].mean().detach()\n",
    "\n",
    "        forward_output['logits'] = forward_output['logits'].detach()\n",
    "\n",
    "        if labels.shape[-1] < 128:\n",
    "            forward_output['labels'] = self._pad_tensors_to_max_len(labels, 128)\n",
    "        else:\n",
    "            forward_output['labels'] = labels\n",
    "\n",
    "\n",
    "        metrics = {}\n",
    "        metrics['test_loss'] = forward_output['loss']\n",
    "\n",
    "        for key in sorted(metrics.keys()):\n",
    "            self.log(key, metrics[key], prog_bar=True)\n",
    "\n",
    "        outputs = {}\n",
    "        outputs['predictions'], outputs['labels'] = self.generate_triples(batch, labels)\n",
    "        \n",
    "        return outputs\n",
    "\n",
    "    def training_epoch_end(self, output: dict):\n",
    "        \n",
    "        print(\"\\n\\n train eval\\n\")\n",
    "        \n",
    "        scores, class_scores= re_score([item for pred in output for item in pred['predictions']], [item for pred in output for item in pred['labels']])\n",
    "\n",
    "        self.log('train_prec_micro', scores[\"ALL\"][\"p\"]) \n",
    "        self.log('train_recall_micro', scores[\"ALL\"][\"r\"])\n",
    "        self.log('train_F1_micro', scores[\"ALL\"][\"f1\"])\n",
    "\n",
    "        self.log(\"train_prec_macro\", scores[\"ALL\"][\"Macro_p\"])\n",
    "        self.log(\"train_recall_macro\", scores[\"ALL\"][\"Macro_r\"])\n",
    "        self.log(\"train_F1_macro\", scores[\"ALL\"][\"Macro_f1\"])\n",
    "\n",
    "        self.log('train_prec_micro_class', class_scores[\"ALL\"][\"p\"])\n",
    "        self.log('train_recall_micro_class', class_scores[\"ALL\"][\"r\"])\n",
    "        self.log('train_F1_micro_class', class_scores[\"ALL\"][\"f1\"])\n",
    "\n",
    "        self.log(\"train_prec_macro_class\", class_scores[\"ALL\"][\"Macro_p\"])\n",
    "        self.log(\"train_recall_macro_class\", class_scores[\"ALL\"][\"Macro_r\"])\n",
    "        self.log(\"train_F1_macro_class\", class_scores[\"ALL\"][\"Macro_f1\"])\n",
    "\n",
    "    def validation_epoch_end(self, output: dict):\n",
    "\n",
    "        print(\"\\n\\n validation eval\\n\")\n",
    "       \n",
    "        scores, class_scores = re_score([item for pred in output for item in pred['predictions']], [item for pred in output for item in pred['labels']])\n",
    "        \n",
    "        self.log('val_prec_micro', scores[\"ALL\"][\"p\"]) \n",
    "        self.log('val_recall_micro', scores[\"ALL\"][\"r\"])\n",
    "        self.log('val_F1_micro', scores[\"ALL\"][\"f1\"])\n",
    "\n",
    "        self.log(\"val_prec_macro\", scores[\"ALL\"][\"Macro_p\"])\n",
    "        self.log(\"val_recall_macro\", scores[\"ALL\"][\"Macro_r\"])\n",
    "        self.log(\"val_F1_macro\", scores[\"ALL\"][\"Macro_f1\"])\n",
    "\n",
    "        self.log('val_prec_micro_class', class_scores[\"ALL\"][\"p\"])\n",
    "        self.log('val_recall_micro_class', class_scores[\"ALL\"][\"r\"])\n",
    "        self.log('val_F1_micro_class', class_scores[\"ALL\"][\"f1\"])\n",
    "\n",
    "        self.log(\"val_prec_macro_class\", class_scores[\"ALL\"][\"Macro_p\"])\n",
    "        self.log(\"val_recall_macro_class\", class_scores[\"ALL\"][\"Macro_r\"])\n",
    "        self.log(\"val_F1_macro_class\", class_scores[\"ALL\"][\"Macro_f1\"])\n",
    "\n",
    "    def test_epoch_end(self, output: dict):\n",
    "       \n",
    "        scores, class_scores = re_score([item for pred in output for item in pred['predictions']], [item for pred in output for item in pred['labels']])\n",
    "\n",
    "        self.log('test_prec_micro', scores[\"ALL\"][\"p\"]) \n",
    "        self.log('test_recall_micro', scores[\"ALL\"][\"r\"])\n",
    "        self.log('test_F1_micro', scores[\"ALL\"][\"f1\"])\n",
    "\n",
    "        self.log(\"test_prec_macro\", scores[\"ALL\"][\"Macro_p\"])\n",
    "        self.log(\"test_recall_macro\", scores[\"ALL\"][\"Macro_r\"])\n",
    "        self.log(\"test_F1_macro\", scores[\"ALL\"][\"Macro_f1\"])\n",
    "\n",
    "        self.log('test_prec_micro_class', class_scores[\"ALL\"][\"p\"])\n",
    "        self.log('test_recall_micro_class', class_scores[\"ALL\"][\"r\"])\n",
    "        self.log('test_F1_micro_class', class_scores[\"ALL\"][\"f1\"])\n",
    "\n",
    "        self.log(\"test_prec_macro_class\", class_scores[\"ALL\"][\"Macro_p\"])\n",
    "        self.log(\"test_recall_macro_class\", class_scores[\"ALL\"][\"Macro_r\"])\n",
    "        self.log(\"test_F1_macro_class\", class_scores[\"ALL\"][\"Macro_f1\"])\n",
    "\n",
    "    # additional functions called in main functions\n",
    "\n",
    "    def generate_triples(self, batch, labels) -> None:\n",
    "\n",
    "        generated_tokens = self.model.generate(\n",
    "            batch[\"input_ids\"].to(self.model.device),\n",
    "            attention_mask=batch[\"attention_mask\"].to(self.model.device),\n",
    "            use_cache = True, max_length = 128, early_stopping = False, length_penalty = 0, \n",
    "            no_repeat_ngram_size = 0, num_beams = 3)\n",
    "\n",
    "        decoded_preds = self.tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
    "        decoded_labels = self.tokenizer.batch_decode(torch.where(labels != -100, labels, self.config.pad_token_id), skip_special_tokens=False)\n",
    "\n",
    "        return [extract_triplets(rel) for rel in decoded_preds], [extract_triplets(rel) for rel in decoded_labels]\n",
    "\n",
    "    def _pad_tensors_to_max_len(self, tensor, max_length):\n",
    "        # If PAD token is not defined at least EOS token has to be defined\n",
    "        pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else self.config.eos_token_id\n",
    "\n",
    "        if pad_token_id is None:\n",
    "            raise ValueError(\n",
    "                f\"Make sure that either `config.pad_token_id` or `config.eos_token_id` is defined if tensor has to be padded to `max_length`={max_length}\"\n",
    "            )\n",
    "\n",
    "        padded_tensor = pad_token_id * torch.ones(\n",
    "            (tensor.shape[0], max_length), dtype=tensor.dtype, device=tensor.device\n",
    "        )\n",
    "        padded_tensor[:, : tensor.shape[-1]] = tensor\n",
    "        return padded_tensor\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "\n",
    "        ##### HUH\n",
    "        no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": self.conf.weight_decay,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [p for n, p in self.model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "                \"weight_decay\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "        \n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr = self.conf.lr, betas = (0.9, 0.999), eps = 0.00000001, weight_decay = self.conf.weight_decay)\n",
    "\n",
    "        def lr_schedule(epoch):\n",
    "            k = self.conf.lr_decay\n",
    "            if epoch < 1: lr_scale =  0.1\n",
    "            else: lr_scale = 1 * math.exp(-k*epoch)\n",
    "            return lr_scale\n",
    "\n",
    "        scheduler = lr_scheduler.LambdaLR(\n",
    "            optimizer,\n",
    "            lr_lambda=lr_schedule\n",
    "        )\n",
    "        #scheduler = inverse_square_root(optimizer, num_warmup_steps= conf.warmup_steps)\n",
    "\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "\n",
    "#In[7]: Helper functions\n",
    "\n",
    "#from REBEL\n",
    "def label_smoothed_nll_loss(lprobs, target, ignore_index=-100):\n",
    "    \"\"\"From fairseq\"\"\"\n",
    "    eps_loss = conf.eps_loss\n",
    "    if target.dim() == lprobs.dim() - 1:\n",
    "        target = target.unsqueeze(-1)\n",
    "    nll_loss = -lprobs.gather(dim=-1, index=target)\n",
    "    smooth_loss = -lprobs.sum(dim=-1, keepdim=True)\n",
    "    if ignore_index is not None:\n",
    "        pad_mask = target.eq(ignore_index)\n",
    "        nll_loss.masked_fill_(pad_mask, 0.0)\n",
    "        smooth_loss.masked_fill_(pad_mask, 0.0)\n",
    "    else:\n",
    "        nll_loss = nll_loss.squeeze(-1)\n",
    "        smooth_loss = smooth_loss.squeeze(-1)\n",
    "\n",
    "    nll_loss = nll_loss.sum()  \n",
    "    smooth_loss = smooth_loss.sum()\n",
    "    eps_i = eps_loss / lprobs.size(-1)\n",
    "    loss = (1.0 - eps_loss) * nll_loss + eps_i * smooth_loss\n",
    "    return loss, nll_loss\n",
    "\n",
    "#from REBEL\n",
    "class GenerateTextSamplesCallback(Callback):\n",
    "    \"\"\"\n",
    "    PL Callback to generate triplets along training\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, logging_batch_interval):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            logging_batch_interval: How frequently to inspect/potentially plot something\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.logging_batch_interval = logging_batch_interval\n",
    "\n",
    "    def on_train_batch_end(self,trainer: Trainer,pl_module: LightningModule, outputs: Sequence, batch: Sequence, batch_idx: int) -> None:\n",
    "        wandb_table = wandb.Table(columns=[\"Source\", \"Pred\", \"Gold\"])\n",
    "        # pl_module.logger.info(\"Executing translation callback\")\n",
    "        labels = batch.pop(\"labels\")\n",
    "        gen_kwargs = {\n",
    "            \"max_length\": 128,\n",
    "            \"early_stopping\": False,\n",
    "            \"no_repeat_ngram_size\": 0,\n",
    "            \"num_beams\": 3\n",
    "        }\n",
    "        pl_module.eval()\n",
    "\n",
    "        decoder_inputs = torch.roll(labels, 1, 1)[:,0:2]\n",
    "        decoder_inputs[:, 0] = 0\n",
    "        generated_tokens = pl_module.model.generate(\n",
    "            batch[\"input_ids\"].to(pl_module.model.device),\n",
    "            attention_mask=batch[\"attention_mask\"].to(pl_module.model.device),\n",
    "            decoder_input_ids=decoder_inputs.to(pl_module.model.device),\n",
    "            **gen_kwargs,\n",
    "        )\n",
    "        # in case the batch is shorter than max length, the output should be padded\n",
    "        if generated_tokens.shape[-1] < gen_kwargs[\"max_length\"]:\n",
    "            generated_tokens = pl_module._pad_tensors_to_max_len(generated_tokens, gen_kwargs[\"max_length\"])\n",
    "        pl_module.train()\n",
    "        decoded_preds = pl_module.tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
    "\n",
    "        # Replace -100 in the labels as we can't decode them.\n",
    "        labels = torch.where(labels != -100, labels, pl_module.tokenizer.pad_token_id)\n",
    "\n",
    "        decoded_labels = pl_module.tokenizer.batch_decode(labels, skip_special_tokens=False)\n",
    "        decoded_inputs = pl_module.tokenizer.batch_decode(batch[\"input_ids\"], skip_special_tokens=False)\n",
    "\n",
    "        # pl_module.logger.experiment.log_text('generated samples', '\\n'.join(decoded_preds).replace('<pad>', ''))\n",
    "        # pl_module.logger.experiment.log_text('original samples', '\\n'.join(decoded_labels).replace('<pad>', ''))\n",
    "        for source, translation, gold_output in zip(decoded_inputs, decoded_preds, decoded_labels):\n",
    "            wandb_table.add_data(\n",
    "                source.replace('<pad>', ''), translation.replace('<pad>', ''), gold_output.replace('<pad>', '')\n",
    "            )\n",
    "        pl_module.logger.experiment.log({\"Triplets\": wandb_table})\n",
    "\n",
    "#from REBEL\n",
    "def shift_tokens_left(input_ids: torch.Tensor, pad_token_id: int):\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right.\n",
    "    \"\"\"\n",
    "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "    shifted_input_ids[:, :-1] = input_ids[:, 1:].clone()\n",
    "    shifted_input_ids[:, -1] = pad_token_id\n",
    "\n",
    "    assert pad_token_id is not None, \"self.model.config.pad_token_id has to be defined.\"\n",
    "\n",
    "    return shifted_input_ids\n",
    "\n",
    "#from REBEL\n",
    "def extract_triplets(text):\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append({'head': subject.strip(), 'type': relation.strip(),'tail': object_.strip()})\n",
    "    return triplets\n",
    "\n",
    "#from REBEL\n",
    "'''Adapted from: https://github.com/btaille/sincere/blob/6f5472c5aeaf7ef7765edf597ede48fdf1071712/code/utils/evaluation.py'''\n",
    "def re_score(pred_relations, gt_relations):\n",
    "    \"\"\"Evaluate RE predictions\n",
    "    Args:\n",
    "        pred_relations (list) :  list of list of predicted relations (several relations in each sentence)\n",
    "        gt_relations (list) :    list of list of ground truth relations\n",
    "            rel = { \"head\": (start_idx (inclusive), end_idx (exclusive)),\n",
    "                    \"tail\": (start_idx (inclusive), end_idx (exclusive)),\n",
    "                    \"head_type\": ent_type,\n",
    "                    \"tail_type\": ent_type,\n",
    "                    \"type\": rel_type}\n",
    "        vocab (Vocab) :         dataset vocabulary\n",
    "        mode (str) :            in 'strict' or 'boundaries' \"\"\"\n",
    "\n",
    "    if conf.ontology == \"pentacode\":\n",
    "        relation_types = [\"Make a statement\", \"Verbal Cooperation\", \"Material Cooperation\", \"Verbal Conflict\", \"Material Conflict\"]\n",
    "    else:\n",
    "        if stage == \"finetune\":\n",
    "            relation_types = [\"Make Public Statement\",\"Appeal\",\"Express Intend to Cooperate\",\"Consult\",\"Engage In Diplomatic Cooperation\",\n",
    "            \"Engage In Material Cooperation\",\"Provide Aid\",\"Yield\",\"Investigate\",\"Demand\",\"Disapprove\",\n",
    "            \"Reject\",\"Exhibit Military Posture\",    \n",
    "            \"Threaten\",\"Protest\",\"Reduce Relations\",\"Coerce\",\"Assault\",\"Fight\"]#,\"Engage In Unconvential Mass Violence\"]\n",
    "        elif stage == \"pretrain\":\n",
    "            relation_types = [\"Make Public Statement\",\"Appeal\",\"Express Intend to Cooperate\",\"Consult\",\"Engage In Diplomatic Cooperation\",\n",
    "            \"Engage In Material Cooperation\",\"Provide Aid\",\"Yield\",\"Investigate\",\"Demand\",\"Disapprove\",\n",
    "            #\"Reject\",\"Exhibit Military Posture\",    \n",
    "            \"Threaten\",\"Protest\",\"Reduce Relations\",\"Coerce\",\"Assault\",\"Fight\"]#,\"Engage In Unconvential Mass Violence\"]    \n",
    "\n",
    "    scores = {rel: {\"tp\": 0, \"fp\": 0, \"fn\": 0} for rel in relation_types + [\"ALL\"]}\n",
    "    class_scores = {rel: {\"tp\": 0, \"fp\": 0, \"fn\": 0} for rel in relation_types + [\"ALL\"]}\n",
    "\n",
    "    # Count GT relations and Predicted relations\n",
    "    n_sents = len(gt_relations)\n",
    "    n_rels = sum([len([rel for rel in sent]) for sent in gt_relations])\n",
    "    n_found = sum([len([rel for rel in sent]) for sent in pred_relations])\n",
    "\n",
    "    # Count TP, FP and FN per type\n",
    "    for pred_sent, gt_sent in zip(pred_relations, gt_relations):\n",
    "        for rel_type in relation_types:\n",
    "            pred_rels = {(rel[\"head\"], rel[\"tail\"]) for rel in pred_sent if rel[\"type\"] == rel_type}\n",
    "            gt_rels = {(rel[\"head\"], rel[\"tail\"]) for rel in gt_sent if rel[\"type\"] == rel_type}\n",
    "\n",
    "            scores[rel_type][\"tp\"] += len(pred_rels & gt_rels)\n",
    "            scores[rel_type][\"fp\"] += len(pred_rels - gt_rels)\n",
    "            scores[rel_type][\"fn\"] += len(gt_rels - pred_rels)\n",
    "\n",
    "            class_pred = [rel[\"type\"] for rel in pred_sent if rel[\"type\"] == rel_type]\n",
    "            class_label = [rel[\"type\"] for rel in gt_sent if rel[\"type\"] == rel_type]\n",
    "\n",
    "            class_scores[rel_type][\"tp\"] += min(len(class_pred), len(class_label))\n",
    "            class_scores[rel_type][\"fp\"] += max(len(class_pred)-len(class_label),0)\n",
    "            class_scores[rel_type][\"fn\"] += max(len(class_label)-len(class_pred),0)\n",
    "\n",
    "\n",
    "    # Compute per triplet Precision / Recall / F1\n",
    "    for rel_type in scores.keys():\n",
    "        if scores[rel_type][\"tp\"]:\n",
    "            scores[rel_type][\"p\"] = 100 * scores[rel_type][\"tp\"] / (scores[rel_type][\"fp\"] + scores[rel_type][\"tp\"])\n",
    "            scores[rel_type][\"r\"] = 100 * scores[rel_type][\"tp\"] / (scores[rel_type][\"fn\"] + scores[rel_type][\"tp\"])\n",
    "        else:\n",
    "            scores[rel_type][\"p\"], scores[rel_type][\"r\"] = 0, 0\n",
    "\n",
    "        if not scores[rel_type][\"p\"] + scores[rel_type][\"r\"] == 0:\n",
    "            scores[rel_type][\"f1\"] = 2 * scores[rel_type][\"p\"] * scores[rel_type][\"r\"] / (scores[rel_type][\"p\"] + scores[rel_type][\"r\"])\n",
    "        else:\n",
    "            scores[rel_type][\"f1\"] = 0\n",
    "\n",
    "    # Compute per class Precision / Recall / F1\n",
    "    for rel_type in scores.keys():\n",
    "        if class_scores[rel_type][\"tp\"]:\n",
    "            class_scores[rel_type][\"p\"] = 100 * class_scores[rel_type][\"tp\"] / (class_scores[rel_type][\"fp\"] + class_scores[rel_type][\"tp\"])\n",
    "            class_scores[rel_type][\"r\"] = 100 * class_scores[rel_type][\"tp\"] / (class_scores[rel_type][\"fn\"] + class_scores[rel_type][\"tp\"])\n",
    "        else:\n",
    "            class_scores[rel_type][\"p\"], class_scores[rel_type][\"r\"] = 0, 0\n",
    "\n",
    "        if not class_scores[rel_type][\"p\"] + class_scores[rel_type][\"r\"] == 0:\n",
    "            class_scores[rel_type][\"f1\"] = 2 * class_scores[rel_type][\"p\"] * class_scores[rel_type][\"r\"] / (class_scores[rel_type][\"p\"] + class_scores[rel_type][\"r\"])\n",
    "        else:\n",
    "            class_scores[rel_type][\"f1\"] = 0\n",
    "\n",
    "    # Compute micro F1 Scores, relations\n",
    "    tp = sum([scores[rel_type][\"tp\"] for rel_type in relation_types])\n",
    "    fp = sum([scores[rel_type][\"fp\"] for rel_type in relation_types])\n",
    "    fn = sum([scores[rel_type][\"fn\"] for rel_type in relation_types])\n",
    "\n",
    "    if tp:\n",
    "        precision = 100 * tp / (tp + fp)\n",
    "        recall = 100 * tp / (tp + fn)\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    else:\n",
    "        precision, recall, f1 = 0, 0, 0\n",
    "\n",
    "    scores[\"ALL\"][\"p\"] = precision\n",
    "    scores[\"ALL\"][\"r\"] = recall\n",
    "    scores[\"ALL\"][\"f1\"] = f1\n",
    "    scores[\"ALL\"][\"tp\"] = tp\n",
    "    scores[\"ALL\"][\"fp\"] = fp\n",
    "    scores[\"ALL\"][\"fn\"] = fn\n",
    "\n",
    "    # Compute micro F1 Scores, classes\n",
    "    class_tp = sum([class_scores[rel_type][\"tp\"] for rel_type in relation_types])\n",
    "    class_fp = sum([class_scores[rel_type][\"fp\"] for rel_type in relation_types])\n",
    "    class_fn = sum([class_scores[rel_type][\"fn\"] for rel_type in relation_types])\n",
    "\n",
    "    if class_tp:\n",
    "        class_precision = 100 * class_tp / (class_tp + class_fp)\n",
    "        class_recall = 100 * class_tp / (class_tp + class_fn)\n",
    "        class_f1 = 2 * class_precision * class_recall / (class_precision + class_recall)\n",
    "\n",
    "    else:\n",
    "        class_precision, class_recall, class_f1 = 0, 0, 0\n",
    "\n",
    "    class_scores[\"ALL\"][\"p\"] = class_precision\n",
    "    class_scores[\"ALL\"][\"r\"] = class_recall\n",
    "    class_scores[\"ALL\"][\"f1\"] = class_f1\n",
    "    class_scores[\"ALL\"][\"tp\"] = class_tp\n",
    "    class_scores[\"ALL\"][\"fp\"] = class_fp\n",
    "    class_scores[\"ALL\"][\"fn\"] = class_fn\n",
    "\n",
    "    # Compute Macro F1 Scores, relations\n",
    "    scores[\"ALL\"][\"Macro_f1\"] = np.mean([scores[rel_type][\"f1\"] for rel_type in relation_types])\n",
    "    scores[\"ALL\"][\"Macro_p\"] = np.mean([scores[rel_type][\"p\"] for rel_type in relation_types])\n",
    "    scores[\"ALL\"][\"Macro_r\"] = np.mean([scores[rel_type][\"r\"] for rel_type in relation_types])\n",
    "\n",
    "    print(f\"Full Triplet Evaluation\")\n",
    "    print(\n",
    "        \"processed {} sentences with {} relations; found: {} relations; correct: {}.\".format(n_sents, n_rels, n_found,tp))\n",
    "    print(\n",
    "        \"\\tALL\\t TP: {};\\tFP: {};\\tFN: {}\".format(\n",
    "            scores[\"ALL\"][\"tp\"],\n",
    "            scores[\"ALL\"][\"fp\"],\n",
    "            scores[\"ALL\"][\"fn\"]))\n",
    "    print(\n",
    "        \"\\t\\t(m avg): precision: {:.2f};\\trecall: {:.2f};\\tf1: {:.2f} (micro)\".format(\n",
    "            precision,\n",
    "            recall,\n",
    "            f1))\n",
    "    print(\n",
    "        \"\\t\\t(M avg): precision: {:.2f};\\trecall: {:.2f};\\tf1: {:.2f} (Macro)\\n\".format(\n",
    "            scores[\"ALL\"][\"Macro_p\"],\n",
    "            scores[\"ALL\"][\"Macro_r\"],\n",
    "            scores[\"ALL\"][\"Macro_f1\"]))\n",
    "\n",
    "    for rel_type in relation_types:\n",
    "        print(\"\\t{}: \\tTP: {};\\tFP: {};\\tFN: {};\\tprecision: {:.2f};\\trecall: {:.2f};\\tf1: {:.2f};\\t{}\".format(\n",
    "            rel_type,\n",
    "            scores[rel_type][\"tp\"],\n",
    "            scores[rel_type][\"fp\"],\n",
    "            scores[rel_type][\"fn\"],\n",
    "            scores[rel_type][\"p\"],\n",
    "            scores[rel_type][\"r\"],\n",
    "            scores[rel_type][\"f1\"],\n",
    "            scores[rel_type][\"tp\"] +\n",
    "            scores[rel_type][\"fp\"]))\n",
    "\n",
    "    # Compute Macro F1 Scores, relations\n",
    "    class_scores[\"ALL\"][\"Macro_f1\"] = np.mean([class_scores[rel_type][\"f1\"] for rel_type in relation_types])\n",
    "    class_scores[\"ALL\"][\"Macro_p\"] = np.mean([class_scores[rel_type][\"p\"] for rel_type in relation_types])\n",
    "    class_scores[\"ALL\"][\"Macro_r\"] = np.mean([class_scores[rel_type][\"r\"] for rel_type in relation_types])\n",
    "\n",
    "    print(f\"Relation Classification Evaluation\")\n",
    "\n",
    "    print(\n",
    "        \"processed {} sentences with {} relations; found: {} relations; correct: {}.\".format(n_sents, n_rels, n_found,class_tp))\n",
    "    print(\n",
    "        \"\\tALL\\t TP: {};\\tFP: {};\\tFN: {}\".format(\n",
    "            class_scores[\"ALL\"][\"tp\"],\n",
    "            class_scores[\"ALL\"][\"fp\"],\n",
    "            class_scores[\"ALL\"][\"fn\"]))\n",
    "    print(\n",
    "        \"\\t\\t(m avg): precision: {:.2f};\\trecall: {:.2f};\\tf1: {:.2f} (micro)\".format(\n",
    "            class_precision,\n",
    "            class_recall,\n",
    "            class_f1))\n",
    "    print(\n",
    "        \"\\t\\t(M avg): precision: {:.2f};\\trecall: {:.2f};\\tf1: {:.2f} (Macro)\\n\".format(\n",
    "            class_scores[\"ALL\"][\"Macro_p\"],\n",
    "            class_scores[\"ALL\"][\"Macro_r\"],\n",
    "            class_scores[\"ALL\"][\"Macro_f1\"]))\n",
    "\n",
    "    for rel_type in relation_types:\n",
    "        print(\"\\t{}: \\tTP: {};\\tFP: {};\\tFN: {};\\tprecision: {:.2f};\\trecall: {:.2f};\\tf1: {:.2f};\\t{}\".format(\n",
    "            rel_type,\n",
    "            class_scores[rel_type][\"tp\"],\n",
    "            class_scores[rel_type][\"fp\"],\n",
    "            class_scores[rel_type][\"fn\"],\n",
    "            class_scores[rel_type][\"p\"],\n",
    "            class_scores[rel_type][\"r\"],\n",
    "            class_scores[rel_type][\"f1\"],\n",
    "            class_scores[rel_type][\"tp\"] +\n",
    "            class_scores[rel_type][\"fp\"]))\n",
    "\n",
    "    return scores, class_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " batch_size: 32 \n",
      " learning rate: 0.000125 \n",
      " learning rate decay: 0.2 \n",
      " weight decay: 0.01 \n",
      " epsilon loss: 0.1 \n",
      " gradient clipping: 10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50273, 1024)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(conf.seed)\n",
    "\n",
    "#load model\n",
    "print(f\" batch_size: {conf.batch_size} \\n learning rate: {conf.lr} \\n learning rate decay: {conf.lr_decay} \\n weight decay: {conf.weight_decay} \\n epsilon loss: {conf.eps_loss} \\n gradient clipping: {conf.gradient_clip_value}\")\n",
    "\n",
    "#add_tokens = [\"<obj>\", \"<subj>\", \"<triplet>\", \"<head>\", \"</head>\", \"<tail>\", \"</tail>\", \"<ents>\", \"</ents>\", \"<MASK>\"]\n",
    "add_tokens = [\"<obj>\", \"<subj>\", \"<triplet>\", \"<head>\", \"</head>\", \"<tail>\", \"</tail>\", \"<MASK>\"]\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\", use_fast = True,\n",
    "    additional_special_tokens = add_tokens)\n",
    "config = transformers.AutoConfig.from_pretrained(\"Babelscape/rebel-large\")\n",
    "model = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\", config = config)\n",
    "\n",
    "#add embeddings for new tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load checkpoints\n",
    "ckpt = torch.load(f\"test_models/no_aug0/epoch=11-step=1294.ckpt\", map_location={'cuda:0':'cuda:0'})   \n",
    "#the map possibly needs to be adjusted or removed :) if all GPUs are free, just remove it \n",
    "\n",
    "#the checkpoint adds the prefix \"model.\" to layers, which we need to remove\n",
    "state_dict = {}\n",
    "for key in ckpt[\"state_dict\"].keys():\n",
    "    state_dict[key[6:]] = ckpt[\"state_dict\"][key]\n",
    "\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit native Automatic Mixed Precision (AMP)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: /home/werner/Thesis_RelationExtraction_PoliticsNews/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "100%|██████████| 1/1 [00:00<00:00, 14.32ba/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 7/7 [00:23<00:00,  3.42s/it]Full Triplet Evaluation\n",
      "processed 224 sentences with 493 relations; found: 363 relations; correct: 184.\n",
      "\tALL\t TP: 184;\tFP: 178;\tFN: 309\n",
      "\t\t(m avg): precision: 50.83;\trecall: 37.32;\tf1: 43.04 (micro)\n",
      "\t\t(M avg): precision: 49.19;\trecall: 37.03;\tf1: 41.35 (Macro)\n",
      "\n",
      "\tMake Public Statement: \tTP: 46;\tFP: 32;\tFN: 76;\tprecision: 58.97;\trecall: 37.70;\tf1: 46.00;\t78\n",
      "\tAppeal: \tTP: 12;\tFP: 8;\tFN: 20;\tprecision: 60.00;\trecall: 37.50;\tf1: 46.15;\t20\n",
      "\tExpress Intend to Cooperate: \tTP: 3;\tFP: 4;\tFN: 15;\tprecision: 42.86;\trecall: 16.67;\tf1: 24.00;\t7\n",
      "\tConsult: \tTP: 6;\tFP: 9;\tFN: 10;\tprecision: 40.00;\trecall: 37.50;\tf1: 38.71;\t15\n",
      "\tEngage In Diplomatic Cooperation: \tTP: 6;\tFP: 14;\tFN: 12;\tprecision: 30.00;\trecall: 33.33;\tf1: 31.58;\t20\n",
      "\tEngage In Material Cooperation: \tTP: 2;\tFP: 5;\tFN: 13;\tprecision: 28.57;\trecall: 13.33;\tf1: 18.18;\t7\n",
      "\tProvide Aid: \tTP: 6;\tFP: 4;\tFN: 5;\tprecision: 60.00;\trecall: 54.55;\tf1: 57.14;\t10\n",
      "\tYield: \tTP: 15;\tFP: 9;\tFN: 13;\tprecision: 62.50;\trecall: 53.57;\tf1: 57.69;\t24\n",
      "\tInvestigate: \tTP: 7;\tFP: 4;\tFN: 5;\tprecision: 63.64;\trecall: 58.33;\tf1: 60.87;\t11\n",
      "\tDemand: \tTP: 18;\tFP: 15;\tFN: 13;\tprecision: 54.55;\trecall: 58.06;\tf1: 56.25;\t33\n",
      "\tDisapprove: \tTP: 11;\tFP: 8;\tFN: 13;\tprecision: 57.89;\trecall: 45.83;\tf1: 51.16;\t19\n",
      "\tReject: \tTP: 14;\tFP: 3;\tFN: 6;\tprecision: 82.35;\trecall: 70.00;\tf1: 75.68;\t17\n",
      "\tExhibit Military Posture: \tTP: 2;\tFP: 3;\tFN: 5;\tprecision: 40.00;\trecall: 28.57;\tf1: 33.33;\t5\n",
      "\tThreaten: \tTP: 8;\tFP: 2;\tFN: 9;\tprecision: 80.00;\trecall: 47.06;\tf1: 59.26;\t10\n",
      "\tProtest: \tTP: 7;\tFP: 17;\tFN: 16;\tprecision: 29.17;\trecall: 30.43;\tf1: 29.79;\t24\n",
      "\tReduce Relations: \tTP: 4;\tFP: 8;\tFN: 16;\tprecision: 33.33;\trecall: 20.00;\tf1: 25.00;\t12\n",
      "\tCoerce: \tTP: 2;\tFP: 9;\tFN: 18;\tprecision: 18.18;\trecall: 10.00;\tf1: 12.90;\t11\n",
      "\tAssault: \tTP: 8;\tFP: 4;\tFN: 25;\tprecision: 66.67;\trecall: 24.24;\tf1: 35.56;\t12\n",
      "\tFight: \tTP: 7;\tFP: 20;\tFN: 19;\tprecision: 25.93;\trecall: 26.92;\tf1: 26.42;\t27\n",
      "Relation Classification Evaluation\n",
      "processed 224 sentences with 493 relations; found: 363 relations; correct: 266.\n",
      "\tALL\t TP: 266;\tFP: 97;\tFN: 227\n",
      "\t\t(m avg): precision: 73.28;\trecall: 53.96;\tf1: 62.15 (micro)\n",
      "\t\t(M avg): precision: 71.87;\trecall: 54.41;\tf1: 60.53 (Macro)\n",
      "\n",
      "\tMake Public Statement: \tTP: 61;\tFP: 17;\tFN: 61;\tprecision: 78.21;\trecall: 50.00;\tf1: 61.00;\t78\n",
      "\tAppeal: \tTP: 18;\tFP: 2;\tFN: 14;\tprecision: 90.00;\trecall: 56.25;\tf1: 69.23;\t20\n",
      "\tExpress Intend to Cooperate: \tTP: 4;\tFP: 3;\tFN: 14;\tprecision: 57.14;\trecall: 22.22;\tf1: 32.00;\t7\n",
      "\tConsult: \tTP: 6;\tFP: 9;\tFN: 10;\tprecision: 40.00;\trecall: 37.50;\tf1: 38.71;\t15\n",
      "\tEngage In Diplomatic Cooperation: \tTP: 9;\tFP: 11;\tFN: 9;\tprecision: 45.00;\trecall: 50.00;\tf1: 47.37;\t20\n",
      "\tEngage In Material Cooperation: \tTP: 4;\tFP: 3;\tFN: 11;\tprecision: 57.14;\trecall: 26.67;\tf1: 36.36;\t7\n",
      "\tProvide Aid: \tTP: 7;\tFP: 3;\tFN: 4;\tprecision: 70.00;\trecall: 63.64;\tf1: 66.67;\t10\n",
      "\tYield: \tTP: 20;\tFP: 4;\tFN: 8;\tprecision: 83.33;\trecall: 71.43;\tf1: 76.92;\t24\n",
      "\tInvestigate: \tTP: 10;\tFP: 1;\tFN: 2;\tprecision: 90.91;\trecall: 83.33;\tf1: 86.96;\t11\n",
      "\tDemand: \tTP: 27;\tFP: 6;\tFN: 4;\tprecision: 81.82;\trecall: 87.10;\tf1: 84.37;\t33\n",
      "\tDisapprove: \tTP: 14;\tFP: 5;\tFN: 10;\tprecision: 73.68;\trecall: 58.33;\tf1: 65.12;\t19\n",
      "\tReject: \tTP: 16;\tFP: 2;\tFN: 4;\tprecision: 88.89;\trecall: 80.00;\tf1: 84.21;\t18\n",
      "\tExhibit Military Posture: \tTP: 4;\tFP: 1;\tFN: 3;\tprecision: 80.00;\trecall: 57.14;\tf1: 66.67;\t5\n",
      "\tThreaten: \tTP: 10;\tFP: 0;\tFN: 7;\tprecision: 100.00;\trecall: 58.82;\tf1: 74.07;\t10\n",
      "\tProtest: \tTP: 22;\tFP: 2;\tFN: 1;\tprecision: 91.67;\trecall: 95.65;\tf1: 93.62;\t24\n",
      "\tReduce Relations: \tTP: 5;\tFP: 7;\tFN: 15;\tprecision: 41.67;\trecall: 25.00;\tf1: 31.25;\t12\n",
      "\tCoerce: \tTP: 7;\tFP: 4;\tFN: 13;\tprecision: 63.64;\trecall: 35.00;\tf1: 45.16;\t11\n",
      "\tAssault: \tTP: 11;\tFP: 1;\tFN: 22;\tprecision: 91.67;\trecall: 33.33;\tf1: 48.89;\t12\n",
      "\tFight: \tTP: 11;\tFP: 16;\tFN: 15;\tprecision: 40.74;\trecall: 42.31;\tf1: 41.51;\t27\n",
      "Testing DataLoader 0: 100%|██████████| 7/7 [00:23<00:00,  3.43s/it]\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "       Test metric             DataLoader 0\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n",
      "      test_F1_macro         41.351115151901155\n",
      "   test_F1_macro_class       60.5309447353628\n",
      "      test_F1_micro          43.04093551635742\n",
      "   test_F1_micro_class      62.149532318115234\n",
      "        test_loss            8.647708892822266\n",
      "     test_prec_macro         49.18983354619659\n",
      "  test_prec_macro_class      71.86850801809251\n",
      "     test_prec_micro         50.82872772216797\n",
      "  test_prec_micro_class      73.27823638916016\n",
      "    test_recall_macro        37.03246595545997\n",
      " test_recall_macro_class    54.406676364354205\n",
      "    test_recall_micro       37.322513580322266\n",
      " test_recall_micro_class     53.95537567138672\n",
      "────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'test_loss': 8.647708892822266,\n",
       "  'test_prec_micro': 50.82872772216797,\n",
       "  'test_recall_micro': 37.322513580322266,\n",
       "  'test_F1_micro': 43.04093551635742,\n",
       "  'test_prec_macro': 49.18983354619659,\n",
       "  'test_recall_macro': 37.03246595545997,\n",
       "  'test_F1_macro': 41.351115151901155,\n",
       "  'test_prec_micro_class': 73.27823638916016,\n",
       "  'test_recall_micro_class': 53.95537567138672,\n",
       "  'test_F1_micro_class': 62.149532318115234,\n",
       "  'test_prec_macro_class': 71.86850801809251,\n",
       "  'test_recall_macro_class': 54.406676364354205,\n",
       "  'test_F1_macro_class': 60.5309447353628}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl_data_module = GetData(conf, tokenizer, model)\n",
    "pl_module = BaseModule(conf, config, tokenizer, model)\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    accelerator = \"gpu\",\n",
    "    devices = [0],\n",
    "    accumulate_grad_batches=1,\n",
    "    gradient_clip_val=conf.gradient_clip_value,\n",
    "    max_epochs = 30,\n",
    "    min_epochs = 5,\n",
    "    reload_dataloaders_every_n_epochs = 1, \n",
    "    precision=16,\n",
    "    amp_level=None,\n",
    ")\n",
    "\n",
    "trainer.test(pl_module, datamodule=pl_data_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis_valentin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a1350fe81f24607af7015099983099ac829ebf1f4acb969c2cf14b7230b10f2d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
